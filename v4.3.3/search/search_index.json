{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Blackbox Optimization The blackboxopt Python package contains blackbox optimization algorithms with a common interface, along with useful helpers like parallel optimization loops, analysis and visualization tools. Key Features Common Interface The blackboxopt base classes along with the EvaluationSpecification and Evaluation data classes specify a unified interface for different blackbox optimization method implementations. In addition to these interfaces, a standard pytest compatible testsuite is available to ensure functional compatibility of an optimizer implementation with the blackboxopt framework. Optimizers Aside from random search, the two main optimizers part of this package are Hyperband and BOHB. Where BOHB is intended as a cleaner replacement of the former implementation in HpBandSter . Optimization Loops As part of the blackboxopt.optimization_loops module compatible implementations for optimization loops are avilable bot for local, serial execution as well as for distributed optimization via dask.distributed . Visualizations Interactive visualizations like objective value over time or duration for single objective optimization, as well as an objectives pair plot with a highlighted pareto front for multi objective optimization is available as part of the blackboxopt.visualizations module. Getting Started The following example outlines how a quadratic function can be optimized with random search in a distributed manner. # Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import parameterspace as ps try : import dask.distributed as dd except ImportError : raise ImportError ( \"Unable to import Dask Distributed specific dependencies. \" + \"Make sure to install blackboxopt[dask]\" ) from blackboxopt import Evaluation , EvaluationSpecification , Objective from blackboxopt.optimization_loops.dask_distributed import ( run_optimization_loop , ) from blackboxopt.optimizers.random_search import RandomSearch def evaluation_function ( eval_spec : EvaluationSpecification ) -> Evaluation : return eval_spec . create_evaluation ( objectives = { \"loss\" : eval_spec . configuration [ \"p1\" ] ** 2 }, user_info = { \"weather\" : \"sunny\" }, ) if __name__ == \"__main__\" : space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( - 1.0 , 1.0 ))) optimizer = RandomSearch ( space , [ Objective ( \"loss\" , greater_is_better = False )], max_steps = 1000 , ) evaluations = run_optimization_loop ( optimizer , evaluation_function , dd . Client (), max_evaluations = 100 ) n_successes = len ([ e for e in evaluations if not e . all_objectives_none ]) print ( f \"Successfully evaluated { n_successes } / { len ( evaluations ) } \" ) License blackboxopt is open-sourced under the Apache-2.0 license. See the LICENSE file for details. For a list of other open source components included in blackboxopt , see the file 3rd-party-licenses.txt .","title":"Overview"},{"location":"#blackbox-optimization","text":"The blackboxopt Python package contains blackbox optimization algorithms with a common interface, along with useful helpers like parallel optimization loops, analysis and visualization tools.","title":"Blackbox Optimization"},{"location":"#key-features","text":"","title":"Key Features"},{"location":"#common-interface","text":"The blackboxopt base classes along with the EvaluationSpecification and Evaluation data classes specify a unified interface for different blackbox optimization method implementations. In addition to these interfaces, a standard pytest compatible testsuite is available to ensure functional compatibility of an optimizer implementation with the blackboxopt framework.","title":"Common Interface"},{"location":"#optimizers","text":"Aside from random search, the two main optimizers part of this package are Hyperband and BOHB. Where BOHB is intended as a cleaner replacement of the former implementation in HpBandSter .","title":"Optimizers"},{"location":"#optimization-loops","text":"As part of the blackboxopt.optimization_loops module compatible implementations for optimization loops are avilable bot for local, serial execution as well as for distributed optimization via dask.distributed .","title":"Optimization Loops"},{"location":"#visualizations","text":"Interactive visualizations like objective value over time or duration for single objective optimization, as well as an objectives pair plot with a highlighted pareto front for multi objective optimization is available as part of the blackboxopt.visualizations module.","title":"Visualizations"},{"location":"#getting-started","text":"The following example outlines how a quadratic function can be optimized with random search in a distributed manner. # Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import parameterspace as ps try : import dask.distributed as dd except ImportError : raise ImportError ( \"Unable to import Dask Distributed specific dependencies. \" + \"Make sure to install blackboxopt[dask]\" ) from blackboxopt import Evaluation , EvaluationSpecification , Objective from blackboxopt.optimization_loops.dask_distributed import ( run_optimization_loop , ) from blackboxopt.optimizers.random_search import RandomSearch def evaluation_function ( eval_spec : EvaluationSpecification ) -> Evaluation : return eval_spec . create_evaluation ( objectives = { \"loss\" : eval_spec . configuration [ \"p1\" ] ** 2 }, user_info = { \"weather\" : \"sunny\" }, ) if __name__ == \"__main__\" : space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( - 1.0 , 1.0 ))) optimizer = RandomSearch ( space , [ Objective ( \"loss\" , greater_is_better = False )], max_steps = 1000 , ) evaluations = run_optimization_loop ( optimizer , evaluation_function , dd . Client (), max_evaluations = 100 ) n_successes = len ([ e for e in evaluations if not e . all_objectives_none ]) print ( f \"Successfully evaluated { n_successes } / { len ( evaluations ) } \" )","title":"Getting Started"},{"location":"#license","text":"blackboxopt is open-sourced under the Apache-2.0 license. See the LICENSE file for details. For a list of other open source components included in blackboxopt , see the file 3rd-party-licenses.txt .","title":"License"},{"location":"examples/dask-distributed/","text":"Dask Distributed # Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import parameterspace as ps try : import dask.distributed as dd except ImportError : raise ImportError ( \"Unable to import Dask Distributed specific dependencies. \" + \"Make sure to install blackboxopt[dask]\" ) from blackboxopt import Evaluation , EvaluationSpecification , Objective from blackboxopt.optimization_loops.dask_distributed import ( run_optimization_loop , ) from blackboxopt.optimizers.random_search import RandomSearch def evaluation_function ( eval_spec : EvaluationSpecification ) -> Evaluation : return eval_spec . create_evaluation ( objectives = { \"loss\" : eval_spec . configuration [ \"p1\" ] ** 2 }, user_info = { \"weather\" : \"sunny\" }, ) if __name__ == \"__main__\" : space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( - 1.0 , 1.0 ))) optimizer = RandomSearch ( space , [ Objective ( \"loss\" , greater_is_better = False )], max_steps = 1000 , ) evaluations = run_optimization_loop ( optimizer , evaluation_function , dd . Client (), max_evaluations = 100 ) n_successes = len ([ e for e in evaluations if not e . all_objectives_none ]) print ( f \"Successfully evaluated { n_successes } / { len ( evaluations ) } \" )","title":"Dask Distributed"},{"location":"examples/dask-distributed/#dask-distributed","text":"# Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import parameterspace as ps try : import dask.distributed as dd except ImportError : raise ImportError ( \"Unable to import Dask Distributed specific dependencies. \" + \"Make sure to install blackboxopt[dask]\" ) from blackboxopt import Evaluation , EvaluationSpecification , Objective from blackboxopt.optimization_loops.dask_distributed import ( run_optimization_loop , ) from blackboxopt.optimizers.random_search import RandomSearch def evaluation_function ( eval_spec : EvaluationSpecification ) -> Evaluation : return eval_spec . create_evaluation ( objectives = { \"loss\" : eval_spec . configuration [ \"p1\" ] ** 2 }, user_info = { \"weather\" : \"sunny\" }, ) if __name__ == \"__main__\" : space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( - 1.0 , 1.0 ))) optimizer = RandomSearch ( space , [ Objective ( \"loss\" , greater_is_better = False )], max_steps = 1000 , ) evaluations = run_optimization_loop ( optimizer , evaluation_function , dd . Client (), max_evaluations = 100 ) n_successes = len ([ e for e in evaluations if not e . all_objectives_none ]) print ( f \"Successfully evaluated { n_successes } / { len ( evaluations ) } \" )","title":"Dask Distributed"},{"location":"examples/overview/","text":"Examples - Overview To get all dependencies for the examples, run: pip install blackboxopt [ examples ] List of available examples Dask Distributed","title":"Overview"},{"location":"examples/overview/#examples-overview","text":"To get all dependencies for the examples, run: pip install blackboxopt [ examples ]","title":"Examples - Overview"},{"location":"examples/overview/#list-of-available-examples","text":"Dask Distributed","title":"List of available examples"},{"location":"reference/base/","text":"blackboxopt.base call_functions_with_evaluations_and_collect_errors ( functions , evaluations ) The given evaluations are passed to all given functions in order and the first Exception that occurrs for an evaluation is recorded alongside the evaluation and raised together with all erroneous evaluations as part of an EvaluationsError . NOTE: Even if reporting some evaluations fails, all that can be are successfully reported. Also, if an evaluation passes through some of the functions before causing issues, the effect the evaluation had on the previous functions can't be reverted. Exceptions: Type Description EvaluationsError In case exceptions occurred when calling the functions with the evaluations. Source code in blackboxopt/base.py def call_functions_with_evaluations_and_collect_errors ( functions : Iterable [ Callable [[ Evaluation ], None ]], evaluations : Iterable [ Evaluation ], ) -> None : \"\"\"The given evaluations are passed to all given functions in order and the first Exception that occurrs for an evaluation is recorded alongside the evaluation and raised together with all erroneous evaluations as part of an `EvaluationsError`. NOTE: Even if reporting some evaluations fails, all that can be are successfully reported. Also, if an evaluation passes through some of the functions before causing issues, the effect the evaluation had on the previous functions can't be reverted. Raises: EvaluationsError: In case exceptions occurred when calling the functions with the evaluations. \"\"\" evaluations_with_errors = [] for evaluation in evaluations : for func in functions : try : func ( evaluation ) except EvaluationsError as e : evaluations_with_errors . extend ( e . evaluations_with_errors ) break except Exception as e : evaluations_with_errors . append (( evaluation , e )) break if evaluations_with_errors : raise EvaluationsError ( evaluations_with_errors ) ConstraintsError Raised on incomplete or missing constraints. ContextError Raised on incomplete or missing context information. EvaluationsError Raised on invalid evaluations. The problematic evaluations and their respective exceptions are passed in the evaluations_with_errors attribute. MultiObjectiveOptimizer generate_evaluation_specification ( self ) inherited Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" report ( self , evaluations ) Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Source code in blackboxopt/base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , ) Objective dataclass Objective(name: str, greater_is_better: bool) ObjectivesError Raised on incomplete or missing objectives. OptimizationComplete Exception that is raised when the optimization run is finished, e.g. when the budget has been exhausted. Optimizer Abstract base class for blackbox optimizer implementations. generate_evaluation_specification ( self ) Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" report ( self , evaluations ) Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Source code in blackboxopt/base.py @abc . abstractmethod def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. \"\"\" OptimizerNotReady Exception that is raised when the optimizer is not ready to propose a new evaluation specification. raise_on_unknown_or_incomplete ( exception , known , reported ) Raise the given exception if not all known strings are contained in reported or the other way around. Source code in blackboxopt/base.py def raise_on_unknown_or_incomplete ( exception : Type [ ValueError ], known : Iterable [ str ], reported : Iterable [ str ] ) -> None : \"\"\"Raise the given exception if not all known strings are contained in reported or the other way around. \"\"\" known_set = set ( known ) reported_set = set ( reported ) unknown = reported_set - known_set if unknown : raise exception ( f \"Unknown reported: { list ( unknown ) } . Valid are only: { list ( known_set ) } \" ) missing = known_set - reported_set if missing : raise exception ( f \"Missing: { list ( missing ) } \" ) SingleObjectiveOptimizer generate_evaluation_specification ( self ) inherited Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" report ( self , evaluations ) Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an `EvaluationsError` is raised, which includes the problematic evaluations with their respective Exceptions in the `evaluations_with_errors` attribute. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. Raises: EvaluationsError: Raised when an evaluation could not be processed. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = [ self . objective ])], _evals , )","title":"Base"},{"location":"reference/base/#blackboxopt.base","text":"","title":"base"},{"location":"reference/base/#blackboxopt.base.call_functions_with_evaluations_and_collect_errors","text":"The given evaluations are passed to all given functions in order and the first Exception that occurrs for an evaluation is recorded alongside the evaluation and raised together with all erroneous evaluations as part of an EvaluationsError . NOTE: Even if reporting some evaluations fails, all that can be are successfully reported. Also, if an evaluation passes through some of the functions before causing issues, the effect the evaluation had on the previous functions can't be reverted. Exceptions: Type Description EvaluationsError In case exceptions occurred when calling the functions with the evaluations. Source code in blackboxopt/base.py def call_functions_with_evaluations_and_collect_errors ( functions : Iterable [ Callable [[ Evaluation ], None ]], evaluations : Iterable [ Evaluation ], ) -> None : \"\"\"The given evaluations are passed to all given functions in order and the first Exception that occurrs for an evaluation is recorded alongside the evaluation and raised together with all erroneous evaluations as part of an `EvaluationsError`. NOTE: Even if reporting some evaluations fails, all that can be are successfully reported. Also, if an evaluation passes through some of the functions before causing issues, the effect the evaluation had on the previous functions can't be reverted. Raises: EvaluationsError: In case exceptions occurred when calling the functions with the evaluations. \"\"\" evaluations_with_errors = [] for evaluation in evaluations : for func in functions : try : func ( evaluation ) except EvaluationsError as e : evaluations_with_errors . extend ( e . evaluations_with_errors ) break except Exception as e : evaluations_with_errors . append (( evaluation , e )) break if evaluations_with_errors : raise EvaluationsError ( evaluations_with_errors )","title":"call_functions_with_evaluations_and_collect_errors()"},{"location":"reference/base/#blackboxopt.base.ConstraintsError","text":"Raised on incomplete or missing constraints.","title":"ConstraintsError"},{"location":"reference/base/#blackboxopt.base.ContextError","text":"Raised on incomplete or missing context information.","title":"ContextError"},{"location":"reference/base/#blackboxopt.base.EvaluationsError","text":"Raised on invalid evaluations. The problematic evaluations and their respective exceptions are passed in the evaluations_with_errors attribute.","title":"EvaluationsError"},{"location":"reference/base/#blackboxopt.base.MultiObjectiveOptimizer","text":"","title":"MultiObjectiveOptimizer"},{"location":"reference/base/#blackboxopt.base.MultiObjectiveOptimizer.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\"","title":"generate_evaluation_specification()"},{"location":"reference/base/#blackboxopt.base.MultiObjectiveOptimizer.report","text":"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Source code in blackboxopt/base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , )","title":"report()"},{"location":"reference/base/#blackboxopt.base.Objective","text":"Objective(name: str, greater_is_better: bool)","title":"Objective"},{"location":"reference/base/#blackboxopt.base.ObjectivesError","text":"Raised on incomplete or missing objectives.","title":"ObjectivesError"},{"location":"reference/base/#blackboxopt.base.OptimizationComplete","text":"Exception that is raised when the optimization run is finished, e.g. when the budget has been exhausted.","title":"OptimizationComplete"},{"location":"reference/base/#blackboxopt.base.Optimizer","text":"Abstract base class for blackbox optimizer implementations.","title":"Optimizer"},{"location":"reference/base/#blackboxopt.base.Optimizer.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\"","title":"generate_evaluation_specification()"},{"location":"reference/base/#blackboxopt.base.Optimizer.report","text":"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Source code in blackboxopt/base.py @abc . abstractmethod def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. \"\"\"","title":"report()"},{"location":"reference/base/#blackboxopt.base.OptimizerNotReady","text":"Exception that is raised when the optimizer is not ready to propose a new evaluation specification.","title":"OptimizerNotReady"},{"location":"reference/base/#blackboxopt.base.raise_on_unknown_or_incomplete","text":"Raise the given exception if not all known strings are contained in reported or the other way around. Source code in blackboxopt/base.py def raise_on_unknown_or_incomplete ( exception : Type [ ValueError ], known : Iterable [ str ], reported : Iterable [ str ] ) -> None : \"\"\"Raise the given exception if not all known strings are contained in reported or the other way around. \"\"\" known_set = set ( known ) reported_set = set ( reported ) unknown = reported_set - known_set if unknown : raise exception ( f \"Unknown reported: { list ( unknown ) } . Valid are only: { list ( known_set ) } \" ) missing = known_set - reported_set if missing : raise exception ( f \"Missing: { list ( missing ) } \" )","title":"raise_on_unknown_or_incomplete()"},{"location":"reference/base/#blackboxopt.base.SingleObjectiveOptimizer","text":"","title":"SingleObjectiveOptimizer"},{"location":"reference/base/#blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\"","title":"generate_evaluation_specification()"},{"location":"reference/base/#blackboxopt.base.SingleObjectiveOptimizer.report","text":"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an `EvaluationsError` is raised, which includes the problematic evaluations with their respective Exceptions in the `evaluations_with_errors` attribute. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. Raises: EvaluationsError: Raised when an evaluation could not be processed. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = [ self . objective ])], _evals , )","title":"report()"},{"location":"reference/evaluation/","text":"blackboxopt.evaluation Evaluation dataclass An evaluated specification with a timestamp indicating the time of the evaluation, and a result dictionary for all objective values. NOTE: NaN is not allowed as an objective value, use None instead. create_evaluation ( self , objectives , constraints = None , user_info = None , stacktrace = None , finished_unixtime = None ) inherited Create a blackboxopt.Evaluation based on this evaluation specification. Parameters: Name Type Description Default objectives Dict[str, Optional[float]] For each objective name the respective value. required constraints Optional[Dict[str, Union[float, NoneType]]] For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. None user_info Optional[dict] Miscellaneous information provided by the user. None stacktrace Optional[str] The stacktrace in case an unhandled exception occurred inside the evaluation function. None finished_unixtime Optional[float] Timestamp at completion of this evaluation. If none is provided, the current time is used. None Source code in blackboxopt/evaluation.py def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation get ( self , key , default = None ) inherited D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in blackboxopt/evaluation.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default get_specification ( self , reset_created_unixtime = False ) Get the evaluation specifiation for which this result was evaluated. Source code in blackboxopt/evaluation.py def get_specification ( self , reset_created_unixtime : bool = False ) -> EvaluationSpecification : \"\"\"Get the evaluation specifiation for which this result was evaluated.\"\"\" eval_spec_kwargs = deepcopy ( dict ( configuration = self . configuration , settings = self . settings , optimizer_info = self . optimizer_info , context = self . context , ) ) if reset_created_unixtime : return EvaluationSpecification ( created_unixtime = _datetime_now_timestamp (), ** eval_spec_kwargs ) return EvaluationSpecification ( created_unixtime = self . created_unixtime , ** eval_spec_kwargs ) items ( self ) inherited D.items() -> a set-like object providing a view on D's items Source code in blackboxopt/evaluation.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self ) keys ( self ) inherited D.keys() -> a set-like object providing a view on D's keys Source code in blackboxopt/evaluation.py def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member values ( self ) inherited D.values() -> an object providing a view on D's values Source code in blackboxopt/evaluation.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self ) EvaluationSpecification dataclass EvaluationSpecification( args, *kwds) create_evaluation ( self , objectives , constraints = None , user_info = None , stacktrace = None , finished_unixtime = None ) Create a blackboxopt.Evaluation based on this evaluation specification. Parameters: Name Type Description Default objectives Dict[str, Optional[float]] For each objective name the respective value. required constraints Optional[Dict[str, Union[float, NoneType]]] For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. None user_info Optional[dict] Miscellaneous information provided by the user. None stacktrace Optional[str] The stacktrace in case an unhandled exception occurred inside the evaluation function. None finished_unixtime Optional[float] Timestamp at completion of this evaluation. If none is provided, the current time is used. None Source code in blackboxopt/evaluation.py def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation get ( self , key , default = None ) inherited D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in blackboxopt/evaluation.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default items ( self ) inherited D.items() -> a set-like object providing a view on D's items Source code in blackboxopt/evaluation.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self ) keys ( self ) D.keys() -> a set-like object providing a view on D's keys Source code in blackboxopt/evaluation.py def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member values ( self ) inherited D.values() -> an object providing a view on D's values Source code in blackboxopt/evaluation.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self )","title":"Evaluation"},{"location":"reference/evaluation/#blackboxopt.evaluation","text":"","title":"evaluation"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation","text":"An evaluated specification with a timestamp indicating the time of the evaluation, and a result dictionary for all objective values. NOTE: NaN is not allowed as an objective value, use None instead.","title":"Evaluation"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.create_evaluation","text":"Create a blackboxopt.Evaluation based on this evaluation specification. Parameters: Name Type Description Default objectives Dict[str, Optional[float]] For each objective name the respective value. required constraints Optional[Dict[str, Union[float, NoneType]]] For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. None user_info Optional[dict] Miscellaneous information provided by the user. None stacktrace Optional[str] The stacktrace in case an unhandled exception occurred inside the evaluation function. None finished_unixtime Optional[float] Timestamp at completion of this evaluation. If none is provided, the current time is used. None Source code in blackboxopt/evaluation.py def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation","title":"create_evaluation()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.get","text":"D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in blackboxopt/evaluation.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default","title":"get()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.get_specification","text":"Get the evaluation specifiation for which this result was evaluated. Source code in blackboxopt/evaluation.py def get_specification ( self , reset_created_unixtime : bool = False ) -> EvaluationSpecification : \"\"\"Get the evaluation specifiation for which this result was evaluated.\"\"\" eval_spec_kwargs = deepcopy ( dict ( configuration = self . configuration , settings = self . settings , optimizer_info = self . optimizer_info , context = self . context , ) ) if reset_created_unixtime : return EvaluationSpecification ( created_unixtime = _datetime_now_timestamp (), ** eval_spec_kwargs ) return EvaluationSpecification ( created_unixtime = self . created_unixtime , ** eval_spec_kwargs )","title":"get_specification()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.items","text":"D.items() -> a set-like object providing a view on D's items Source code in blackboxopt/evaluation.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self )","title":"items()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.keys","text":"D.keys() -> a set-like object providing a view on D's keys Source code in blackboxopt/evaluation.py def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member","title":"keys()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.values","text":"D.values() -> an object providing a view on D's values Source code in blackboxopt/evaluation.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self )","title":"values()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification","text":"EvaluationSpecification( args, *kwds)","title":"EvaluationSpecification"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.create_evaluation","text":"Create a blackboxopt.Evaluation based on this evaluation specification. Parameters: Name Type Description Default objectives Dict[str, Optional[float]] For each objective name the respective value. required constraints Optional[Dict[str, Union[float, NoneType]]] For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. None user_info Optional[dict] Miscellaneous information provided by the user. None stacktrace Optional[str] The stacktrace in case an unhandled exception occurred inside the evaluation function. None finished_unixtime Optional[float] Timestamp at completion of this evaluation. If none is provided, the current time is used. None Source code in blackboxopt/evaluation.py def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation","title":"create_evaluation()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.get","text":"D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in blackboxopt/evaluation.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default","title":"get()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.items","text":"D.items() -> a set-like object providing a view on D's items Source code in blackboxopt/evaluation.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self )","title":"items()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.keys","text":"D.keys() -> a set-like object providing a view on D's keys Source code in blackboxopt/evaluation.py def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member","title":"keys()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.values","text":"D.values() -> an object providing a view on D's values Source code in blackboxopt/evaluation.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self )","title":"values()"},{"location":"reference/utils/","text":"blackboxopt.utils get_loss_vector ( known_objectives , reported_objectives , none_replacement = nan ) Convert reported objectives into a vector of known objectives. Parameters: Name Type Description Default known_objectives Sequence[blackboxopt.base.Objective] A sequence of objectives with names and directions (whether greate is better). The order of the objectives dictates the order of the returned loss values. required reported_objectives Dict[str, Optional[float]] A dictionary with the objective value for each of the known objectives' names. required none_replacement float The value to use for missing objective values that are None nan Returns: Type Description List[float] A list of loss values. Source code in blackboxopt/utils.py def get_loss_vector ( known_objectives : Sequence [ Objective ], reported_objectives : Dict [ str , Optional [ float ]], none_replacement : float = float ( \"NaN\" ), ) -> List [ float ]: \"\"\"Convert reported objectives into a vector of known objectives. Args: known_objectives: A sequence of objectives with names and directions (whether greate is better). The order of the objectives dictates the order of the returned loss values. reported_objectives: A dictionary with the objective value for each of the known objectives' names. none_replacement: The value to use for missing objective values that are `None` Returns: A list of loss values. \"\"\" losses = [] for objective in known_objectives : objective_value = reported_objectives [ objective . name ] if objective_value is None : losses . append ( none_replacement ) elif objective . greater_is_better : losses . append ( - 1.0 * objective_value ) else : losses . append ( objective_value ) return losses","title":"Utils"},{"location":"reference/utils/#blackboxopt.utils","text":"","title":"utils"},{"location":"reference/utils/#blackboxopt.utils.get_loss_vector","text":"Convert reported objectives into a vector of known objectives. Parameters: Name Type Description Default known_objectives Sequence[blackboxopt.base.Objective] A sequence of objectives with names and directions (whether greate is better). The order of the objectives dictates the order of the returned loss values. required reported_objectives Dict[str, Optional[float]] A dictionary with the objective value for each of the known objectives' names. required none_replacement float The value to use for missing objective values that are None nan Returns: Type Description List[float] A list of loss values. Source code in blackboxopt/utils.py def get_loss_vector ( known_objectives : Sequence [ Objective ], reported_objectives : Dict [ str , Optional [ float ]], none_replacement : float = float ( \"NaN\" ), ) -> List [ float ]: \"\"\"Convert reported objectives into a vector of known objectives. Args: known_objectives: A sequence of objectives with names and directions (whether greate is better). The order of the objectives dictates the order of the returned loss values. reported_objectives: A dictionary with the objective value for each of the known objectives' names. none_replacement: The value to use for missing objective values that are `None` Returns: A list of loss values. \"\"\" losses = [] for objective in known_objectives : objective_value = reported_objectives [ objective . name ] if objective_value is None : losses . append ( none_replacement ) elif objective . greater_is_better : losses . append ( - 1.0 * objective_value ) else : losses . append ( objective_value ) return losses","title":"get_loss_vector()"},{"location":"reference/optimization_loops/dask_distributed/","text":"blackboxopt.optimization_loops.dask_distributed run_optimization_loop ( optimizer , evaluation_function , dask_client , timeout_s = inf , max_evaluations = None , logger = None ) Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Parameters: Name Type Description Default optimizer Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer] The blackboxopt optimizer to run. required dask_client Client A Dask Distributed client that is configured with workers. required evaluation_function Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation] The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a blackboxopt.Evaluation as a result. required timeout_s float If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. inf max_evaluations int If given, the optimization loop will terminate after the given number of steps. Defaults to None. None logger Logger The logger to use for logging progress. Defaults to None. None Returns: Type Description List[blackboxopt.evaluation.Evaluation] List of evluation specification and result for all evaluations. Source code in blackboxopt/optimization_loops/dask_distributed.py def run_optimization_loop ( optimizer : Union [ SingleObjectiveOptimizer , MultiObjectiveOptimizer ], evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], dask_client : dd . Client , timeout_s : float = float ( \"inf\" ), max_evaluations : int = None , logger : logging . Logger = None , ) -> List [ Evaluation ]: \"\"\"Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Args: optimizer: The blackboxopt optimizer to run. dask_client: A Dask Distributed client that is configured with workers. evaluation_function: The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a `blackboxopt.Evaluation` as a result. timeout_s: If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. max_evaluations: If given, the optimization loop will terminate after the given number of steps. Defaults to None. logger: The logger to use for logging progress. Defaults to None. Returns: List of evluation specification and result for all evaluations. \"\"\" logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger objectives = ( optimizer . objectives if isinstance ( optimizer , MultiObjectiveOptimizer ) else [ optimizer . objective ] ) evaluations : List [ Evaluation ] = [] dask_scheduler = MinimalDaskScheduler ( dask_client = dask_client , objectives = objectives , logger = logger ) _max_evaluations = init_max_evaluations_with_limit_logging ( max_evaluations = max_evaluations , timeout_s = timeout_s , logger = logger ) n_eval_specs = 0 start = time . time () while time . time () - start < timeout_s and n_eval_specs < _max_evaluations : if dask_scheduler . has_capacity (): try : eval_spec = optimizer . generate_evaluation_specification () dask_scheduler . submit ( evaluation_function , eval_spec ) n_eval_specs += 1 continue except OptimizerNotReady : logger . info ( \"Optimizer is not ready yet; will retry after short pause.\" ) except OptimizationComplete : logger . info ( \"Optimization is complete\" ) break new_evaluations = dask_scheduler . check_for_results ( timeout_s = 20 ) optimizer . report ( new_evaluations ) evaluations . extend ( new_evaluations ) while dask_scheduler . has_running_jobs (): new_evaluations = dask_scheduler . check_for_results ( timeout_s = 20 ) optimizer . report ( new_evaluations ) evaluations . extend ( new_evaluations ) return evaluations","title":"Dask distributed"},{"location":"reference/optimization_loops/dask_distributed/#blackboxopt.optimization_loops.dask_distributed","text":"","title":"dask_distributed"},{"location":"reference/optimization_loops/dask_distributed/#blackboxopt.optimization_loops.dask_distributed.run_optimization_loop","text":"Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Parameters: Name Type Description Default optimizer Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer] The blackboxopt optimizer to run. required dask_client Client A Dask Distributed client that is configured with workers. required evaluation_function Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation] The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a blackboxopt.Evaluation as a result. required timeout_s float If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. inf max_evaluations int If given, the optimization loop will terminate after the given number of steps. Defaults to None. None logger Logger The logger to use for logging progress. Defaults to None. None Returns: Type Description List[blackboxopt.evaluation.Evaluation] List of evluation specification and result for all evaluations. Source code in blackboxopt/optimization_loops/dask_distributed.py def run_optimization_loop ( optimizer : Union [ SingleObjectiveOptimizer , MultiObjectiveOptimizer ], evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], dask_client : dd . Client , timeout_s : float = float ( \"inf\" ), max_evaluations : int = None , logger : logging . Logger = None , ) -> List [ Evaluation ]: \"\"\"Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Args: optimizer: The blackboxopt optimizer to run. dask_client: A Dask Distributed client that is configured with workers. evaluation_function: The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a `blackboxopt.Evaluation` as a result. timeout_s: If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. max_evaluations: If given, the optimization loop will terminate after the given number of steps. Defaults to None. logger: The logger to use for logging progress. Defaults to None. Returns: List of evluation specification and result for all evaluations. \"\"\" logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger objectives = ( optimizer . objectives if isinstance ( optimizer , MultiObjectiveOptimizer ) else [ optimizer . objective ] ) evaluations : List [ Evaluation ] = [] dask_scheduler = MinimalDaskScheduler ( dask_client = dask_client , objectives = objectives , logger = logger ) _max_evaluations = init_max_evaluations_with_limit_logging ( max_evaluations = max_evaluations , timeout_s = timeout_s , logger = logger ) n_eval_specs = 0 start = time . time () while time . time () - start < timeout_s and n_eval_specs < _max_evaluations : if dask_scheduler . has_capacity (): try : eval_spec = optimizer . generate_evaluation_specification () dask_scheduler . submit ( evaluation_function , eval_spec ) n_eval_specs += 1 continue except OptimizerNotReady : logger . info ( \"Optimizer is not ready yet; will retry after short pause.\" ) except OptimizationComplete : logger . info ( \"Optimization is complete\" ) break new_evaluations = dask_scheduler . check_for_results ( timeout_s = 20 ) optimizer . report ( new_evaluations ) evaluations . extend ( new_evaluations ) while dask_scheduler . has_running_jobs (): new_evaluations = dask_scheduler . check_for_results ( timeout_s = 20 ) optimizer . report ( new_evaluations ) evaluations . extend ( new_evaluations ) return evaluations","title":"run_optimization_loop()"},{"location":"reference/optimization_loops/sequential/","text":"blackboxopt.optimization_loops.sequential run_optimization_loop ( optimizer , evaluation_function , timeout_s = inf , max_evaluations = None , catch_exceptions_from_evaluation_function = False , post_evaluation_callback = None , logger = None ) Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Parameters: Name Type Description Default optimizer Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer] The blackboxopt optimizer to run. required evaluation_function Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation] The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a blackboxopt.Evaluation as a result. required timeout_s float If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. inf max_evaluations int If given, the optimization loop will terminate after the given number of steps. None catch_exceptions_from_evaluation_function bool Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's stacktrace attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. False post_evaluation_callback Optional[Callable[[blackboxopt.evaluation.Evaluation], Any]] Reference to a callable that is invoked after each evaluation and takes a blackboxopt.Evaluation as its argument. None logger Logger The logger to use for logging progress. None Returns: Type Description List[blackboxopt.evaluation.Evaluation] List of evluation specification and result for all evaluations. Source code in blackboxopt/optimization_loops/sequential.py def run_optimization_loop ( optimizer : Union [ SingleObjectiveOptimizer , MultiObjectiveOptimizer ], evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], timeout_s : float = float ( \"inf\" ), max_evaluations : int = None , catch_exceptions_from_evaluation_function : bool = False , post_evaluation_callback : Optional [ Callable [[ Evaluation ], Any ]] = None , logger : logging . Logger = None , ) -> List [ Evaluation ]: \"\"\"Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Args: optimizer: The blackboxopt optimizer to run. evaluation_function: The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a `blackboxopt.Evaluation` as a result. timeout_s: If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. max_evaluations: If given, the optimization loop will terminate after the given number of steps. catch_exceptions_from_evaluation_function: Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's `stacktrace` attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. post_evaluation_callback: Reference to a callable that is invoked after each evaluation and takes a `blackboxopt.Evaluation` as its argument. logger: The logger to use for logging progress. Returns: List of evluation specification and result for all evaluations. \"\"\" logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger objectives = ( optimizer . objectives if isinstance ( optimizer , MultiObjectiveOptimizer ) else [ optimizer . objective ] ) evaluations : List [ Evaluation ] = [] _max_evaluations = init_max_evaluations_with_limit_logging ( max_evaluations = max_evaluations , timeout_s = timeout_s , logger = logger ) start = time . time () num_evaluations = 0 while time . time () - start < timeout_s and num_evaluations < _max_evaluations : num_evaluations += 1 try : evaluation_specification = optimizer . generate_evaluation_specification () logger . info ( \"The optimizer proposed a specification for evaluation: \\n \" + f \" { json . dumps ( evaluation_specification . to_dict (), indent = 2 ) } \" ) evaluation = evaluation_function_wrapper ( evaluation_function = evaluation_function , evaluation_specification = evaluation_specification , logger = logger , objectives = objectives , catch_exceptions_from_evaluation_function = catch_exceptions_from_evaluation_function , ) logger . info ( \"Reporting the result from the evaluation function to the optimizer: \\n \" + f \" { json . dumps ( evaluation . to_dict (), indent = 2 ) } \" ) optimizer . report ( evaluation ) evaluations . append ( evaluation ) if post_evaluation_callback is not None : post_evaluation_callback ( evaluation ) except OptimizerNotReady : logger . info ( \"Optimizer is not ready yet, retrying in two seconds\" ) time . sleep ( 2 ) continue except OptimizationComplete : logger . info ( \"Optimization is complete\" ) return evaluations logger . info ( \"Aborting optimization due to specified maximum evaluations or timeout\" ) return evaluations","title":"Sequential"},{"location":"reference/optimization_loops/sequential/#blackboxopt.optimization_loops.sequential","text":"","title":"sequential"},{"location":"reference/optimization_loops/sequential/#blackboxopt.optimization_loops.sequential.run_optimization_loop","text":"Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Parameters: Name Type Description Default optimizer Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer] The blackboxopt optimizer to run. required evaluation_function Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation] The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a blackboxopt.Evaluation as a result. required timeout_s float If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. inf max_evaluations int If given, the optimization loop will terminate after the given number of steps. None catch_exceptions_from_evaluation_function bool Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's stacktrace attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. False post_evaluation_callback Optional[Callable[[blackboxopt.evaluation.Evaluation], Any]] Reference to a callable that is invoked after each evaluation and takes a blackboxopt.Evaluation as its argument. None logger Logger The logger to use for logging progress. None Returns: Type Description List[blackboxopt.evaluation.Evaluation] List of evluation specification and result for all evaluations. Source code in blackboxopt/optimization_loops/sequential.py def run_optimization_loop ( optimizer : Union [ SingleObjectiveOptimizer , MultiObjectiveOptimizer ], evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], timeout_s : float = float ( \"inf\" ), max_evaluations : int = None , catch_exceptions_from_evaluation_function : bool = False , post_evaluation_callback : Optional [ Callable [[ Evaluation ], Any ]] = None , logger : logging . Logger = None , ) -> List [ Evaluation ]: \"\"\"Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Args: optimizer: The blackboxopt optimizer to run. evaluation_function: The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a `blackboxopt.Evaluation` as a result. timeout_s: If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. max_evaluations: If given, the optimization loop will terminate after the given number of steps. catch_exceptions_from_evaluation_function: Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's `stacktrace` attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. post_evaluation_callback: Reference to a callable that is invoked after each evaluation and takes a `blackboxopt.Evaluation` as its argument. logger: The logger to use for logging progress. Returns: List of evluation specification and result for all evaluations. \"\"\" logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger objectives = ( optimizer . objectives if isinstance ( optimizer , MultiObjectiveOptimizer ) else [ optimizer . objective ] ) evaluations : List [ Evaluation ] = [] _max_evaluations = init_max_evaluations_with_limit_logging ( max_evaluations = max_evaluations , timeout_s = timeout_s , logger = logger ) start = time . time () num_evaluations = 0 while time . time () - start < timeout_s and num_evaluations < _max_evaluations : num_evaluations += 1 try : evaluation_specification = optimizer . generate_evaluation_specification () logger . info ( \"The optimizer proposed a specification for evaluation: \\n \" + f \" { json . dumps ( evaluation_specification . to_dict (), indent = 2 ) } \" ) evaluation = evaluation_function_wrapper ( evaluation_function = evaluation_function , evaluation_specification = evaluation_specification , logger = logger , objectives = objectives , catch_exceptions_from_evaluation_function = catch_exceptions_from_evaluation_function , ) logger . info ( \"Reporting the result from the evaluation function to the optimizer: \\n \" + f \" { json . dumps ( evaluation . to_dict (), indent = 2 ) } \" ) optimizer . report ( evaluation ) evaluations . append ( evaluation ) if post_evaluation_callback is not None : post_evaluation_callback ( evaluation ) except OptimizerNotReady : logger . info ( \"Optimizer is not ready yet, retrying in two seconds\" ) time . sleep ( 2 ) continue except OptimizationComplete : logger . info ( \"Optimization is complete\" ) return evaluations logger . info ( \"Aborting optimization due to specified maximum evaluations or timeout\" ) return evaluations","title":"run_optimization_loop()"},{"location":"reference/optimization_loops/testing/","text":"blackboxopt.optimization_loops.testing","title":"Testing"},{"location":"reference/optimization_loops/testing/#blackboxopt.optimization_loops.testing","text":"","title":"testing"},{"location":"reference/optimization_loops/utils/","text":"blackboxopt.optimization_loops.utils evaluation_function_wrapper ( evaluation_function , evaluation_specification , objectives , catch_exceptions_from_evaluation_function , logger ) Wrapper for evaluation functions. The evaluation result returned by the evaluation function is checked to contain all relevant objectives. An empty evaluation with a stacktrace is reported to the optimizer in case an unhandled Exception occurrs during the evaluation function call when catch_exceptions_from_evaluation_function is set to True , otherwise an EvaluationFunctionError is raised based on the original exception. Source code in blackboxopt/optimization_loops/utils.py def evaluation_function_wrapper ( evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], evaluation_specification : EvaluationSpecification , objectives : List [ Objective ], catch_exceptions_from_evaluation_function : bool , logger : logging . Logger , ) -> Evaluation : \"\"\"Wrapper for evaluation functions. The evaluation result returned by the evaluation function is checked to contain all relevant objectives. An empty evaluation with a stacktrace is reported to the optimizer in case an unhandled Exception occurrs during the evaluation function call when `catch_exceptions_from_evaluation_function` is set to `True`, otherwise an `EvaluationFunctionError` is raised based on the original exception. \"\"\" try : evaluation = evaluation_function ( evaluation_specification ) except Exception as e : if not catch_exceptions_from_evaluation_function : raise EvaluationFunctionError ( evaluation_specification ) from e stacktrace = traceback . format_exc () logger . warning ( \"Reporting FAILURE due to unhandled error in evaluation function. See \" + \"DEBUG log level output or evaluation.stacktrace for details. \" + \"Alternatively, disable automated exception handling by setting \" + \"catch_exceptions_from_evaluation_function=False to exit on errors.\" ) logger . debug ( stacktrace ) evaluation = evaluation_specification . create_evaluation ( stacktrace = stacktrace , objectives = { o . name : None for o in objectives } ) raise_on_unknown_or_incomplete ( exception = ObjectivesError , known = [ o . name for o in objectives ], reported = evaluation . objectives . keys (), ) return evaluation EvaluationFunctionError Raised on errors originating from the user defined evaluation function. init_max_evaluations_with_limit_logging ( timeout_s , logger , max_evaluations = None ) [summary] Parameters: Name Type Description Default timeout_s float [description] required logger Logger [description] required max_evaluations int [description] None Returns: Type Description float [description] Source code in blackboxopt/optimization_loops/utils.py def init_max_evaluations_with_limit_logging ( timeout_s : float , logger : logging . Logger , max_evaluations : int = None ) -> float : \"\"\"[summary] Args: timeout_s: [description] logger: [description] max_evaluations: [description] Returns: [description] \"\"\" if max_evaluations : logger . info ( \"Starting optimization run. Stops when complete or \" + f \" { max_evaluations } evaluations reached.\" ) return float ( max_evaluations ) if timeout_s == float ( \"inf\" ): logger . info ( \"Starting optimization run. Stops when complete.\" ) else : timeout_pretty = datetime . timedelta ( seconds = timeout_s ) logger . info ( \"Starting optimization run. Stops when complete or \" + f \" { timeout_pretty } passed.\" ) return float ( \"inf\" )","title":"Utils"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils","text":"","title":"utils"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils.evaluation_function_wrapper","text":"Wrapper for evaluation functions. The evaluation result returned by the evaluation function is checked to contain all relevant objectives. An empty evaluation with a stacktrace is reported to the optimizer in case an unhandled Exception occurrs during the evaluation function call when catch_exceptions_from_evaluation_function is set to True , otherwise an EvaluationFunctionError is raised based on the original exception. Source code in blackboxopt/optimization_loops/utils.py def evaluation_function_wrapper ( evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], evaluation_specification : EvaluationSpecification , objectives : List [ Objective ], catch_exceptions_from_evaluation_function : bool , logger : logging . Logger , ) -> Evaluation : \"\"\"Wrapper for evaluation functions. The evaluation result returned by the evaluation function is checked to contain all relevant objectives. An empty evaluation with a stacktrace is reported to the optimizer in case an unhandled Exception occurrs during the evaluation function call when `catch_exceptions_from_evaluation_function` is set to `True`, otherwise an `EvaluationFunctionError` is raised based on the original exception. \"\"\" try : evaluation = evaluation_function ( evaluation_specification ) except Exception as e : if not catch_exceptions_from_evaluation_function : raise EvaluationFunctionError ( evaluation_specification ) from e stacktrace = traceback . format_exc () logger . warning ( \"Reporting FAILURE due to unhandled error in evaluation function. See \" + \"DEBUG log level output or evaluation.stacktrace for details. \" + \"Alternatively, disable automated exception handling by setting \" + \"catch_exceptions_from_evaluation_function=False to exit on errors.\" ) logger . debug ( stacktrace ) evaluation = evaluation_specification . create_evaluation ( stacktrace = stacktrace , objectives = { o . name : None for o in objectives } ) raise_on_unknown_or_incomplete ( exception = ObjectivesError , known = [ o . name for o in objectives ], reported = evaluation . objectives . keys (), ) return evaluation","title":"evaluation_function_wrapper()"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils.EvaluationFunctionError","text":"Raised on errors originating from the user defined evaluation function.","title":"EvaluationFunctionError"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils.init_max_evaluations_with_limit_logging","text":"[summary] Parameters: Name Type Description Default timeout_s float [description] required logger Logger [description] required max_evaluations int [description] None Returns: Type Description float [description] Source code in blackboxopt/optimization_loops/utils.py def init_max_evaluations_with_limit_logging ( timeout_s : float , logger : logging . Logger , max_evaluations : int = None ) -> float : \"\"\"[summary] Args: timeout_s: [description] logger: [description] max_evaluations: [description] Returns: [description] \"\"\" if max_evaluations : logger . info ( \"Starting optimization run. Stops when complete or \" + f \" { max_evaluations } evaluations reached.\" ) return float ( max_evaluations ) if timeout_s == float ( \"inf\" ): logger . info ( \"Starting optimization run. Stops when complete.\" ) else : timeout_pretty = datetime . timedelta ( seconds = timeout_s ) logger . info ( \"Starting optimization run. Stops when complete or \" + f \" { timeout_pretty } passed.\" ) return float ( \"inf\" )","title":"init_max_evaluations_with_limit_logging()"},{"location":"reference/optimizers/bohb/","text":"blackboxopt.optimizers.bohb BOHB generate_evaluation_specification ( self ) inherited Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/bohb.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady report ( self , evaluations ) inherited Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/bohb.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"Bohb"},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb","text":"","title":"bohb"},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB","text":"","title":"BOHB"},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/bohb.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB.report","text":"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/bohb.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"report()"},{"location":"reference/optimizers/hyperband/","text":"blackboxopt.optimizers.hyperband Hyperband generate_evaluation_specification ( self ) inherited Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/hyperband.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady report ( self , evaluations ) inherited Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/hyperband.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"Hyperband"},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband","text":"","title":"hyperband"},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband.Hyperband","text":"","title":"Hyperband"},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband.Hyperband.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/hyperband.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband.Hyperband.report","text":"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/hyperband.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"report()"},{"location":"reference/optimizers/random_search/","text":"blackboxopt.optimizers.random_search RandomSearch generate_evaluation_specification ( self ) [summary] Exceptions: Type Description OptimizationComplete Raised if the optimizer's max_steps are reached. Returns: Type Description EvaluationSpecification [description] Source code in blackboxopt/optimizers/random_search.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"[summary] Raises: OptimizationComplete: Raised if the optimizer's `max_steps` are reached. Returns: [description] \"\"\" if self . n_steps >= self . max_steps : raise OptimizationComplete () eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), settings = {}, optimizer_info = { \"step\" : self . n_steps }, ) self . n_steps += 1 return eval_spec report ( self , evaluations ) inherited Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Source code in blackboxopt/optimizers/random_search.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , )","title":"Random search"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search","text":"","title":"random_search"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search.RandomSearch","text":"","title":"RandomSearch"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search.RandomSearch.generate_evaluation_specification","text":"[summary] Exceptions: Type Description OptimizationComplete Raised if the optimizer's max_steps are reached. Returns: Type Description EvaluationSpecification [description] Source code in blackboxopt/optimizers/random_search.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"[summary] Raises: OptimizationComplete: Raised if the optimizer's `max_steps` are reached. Returns: [description] \"\"\" if self . n_steps >= self . max_steps : raise OptimizationComplete () eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), settings = {}, optimizer_info = { \"step\" : self . n_steps }, ) self . n_steps += 1 return eval_spec","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search.RandomSearch.report","text":"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Source code in blackboxopt/optimizers/random_search.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , )","title":"report()"},{"location":"reference/optimizers/testing/","text":"blackboxopt.optimizers.testing Tests that can be imported and used to test optimizer implementations against this packages blackbox optimizer interface. handles_reporting_evaluations_list ( optimizer_class , optimizer_kwargs ) Check if optimizer's report method can process an iterable of evalutions. All optimizers should be able to allow reporting batches of evalutions. It's up to the optimizer's implementation, if evaluations in a batch are processed one by one like if they were reported individually, or if a batch is handled differently. Parameters: Name Type Description Default optimizer_class Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializating the optimizer. ( search_space and objective(s) are set automatically by the test.) required Returns: Type Description bool True if the test is passed. Source code in blackboxopt/optimizers/testing.py def handles_reporting_evaluations_list ( optimizer_class , optimizer_kwargs : dict ) -> bool : \"\"\"Check if optimizer's report method can process an iterable of evalutions. All optimizers should be able to allow reporting batches of evalutions. It's up to the optimizer's implementation, if evaluations in a batch are processed one by one like if they were reported individually, or if a batch is handled differently. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializating the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) Returns: `True` if the test is passed. \"\"\" opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , objective = Objective ( \"loss\" , False ), objectives = [ Objective ( \"loss\" , False )], ) evaluations = [] for _ in range ( 3 ): es = opt . generate_evaluation_specification () evaluation = es . create_evaluation ( objectives = { \"loss\" : 0.42 }) evaluations . append ( evaluation ) opt . report ( evaluations ) return True is_deterministic_with_fixed_seed ( optimizer_class , optimizer_kwargs ) Check if optimizer is deterministic. Repeatedly initialize the optimizer with the same parameter space and a fixed seed, get an evaluation specification, report a placeholder result and get another evaluation specification. The configuration of all final evaluation specifications should be equal. Parameters: Name Type Description Default optimizer_class Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializating the optimizer. ( search_space and objective(s) are set automatically by the test.) required Returns: Type Description bool True if the test is passed. Source code in blackboxopt/optimizers/testing.py def is_deterministic_with_fixed_seed ( optimizer_class , optimizer_kwargs : dict ) -> bool : \"\"\"Check if optimizer is deterministic. Repeatedly initialize the optimizer with the same parameter space and a fixed seed, get an evaluation specification, report a placeholder result and get another evaluation specification. The configuration of all final evaluation specifications should be equal. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializating the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) Returns: `True` if the test is passed. \"\"\" final_configurations = [] for _ in range ( 2 ): opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , objective = Objective ( \"loss\" , False ), objectives = [ Objective ( \"loss\" , False )], ) es1 = opt . generate_evaluation_specification () evaluation1 = es1 . create_evaluation ( objectives = { \"loss\" : 0.42 }) opt . report ( evaluation1 ) es2 = opt . generate_evaluation_specification () final_configurations . append ( es2 . configuration . copy ()) assert final_configurations [ 0 ] == final_configurations [ 1 ] return True optimize_single_parameter_sequentially_for_n_max_evaluations ( optimizer_class , optimizer_kwargs , n_max_evaluations = 20 ) [summary] Parameters: Name Type Description Default optimizer_class [description] required optimizer_kwargs dict [description] required n_max_evaluations int [description] 20 Returns: Type Description bool [description] Source code in blackboxopt/optimizers/testing.py def optimize_single_parameter_sequentially_for_n_max_evaluations ( optimizer_class , optimizer_kwargs : dict , n_max_evaluations : int = 20 ) -> bool : \"\"\"[summary] Args: optimizer_class: [description] optimizer_kwargs: [description] n_max_evaluations: [description] Returns: [description] \"\"\" def quadratic_function ( p1 ): return p1 ** 2 assert issubclass ( optimizer_class , Optimizer ), ( \"The default test suite is only applicable for implementations of \" \"blackboxopt.base.Optimizer\" ) optimizer = _initialize_optimizer ( optimizer_class , optimizer_kwargs , objective = Objective ( \"loss\" , False ), objectives = [ Objective ( \"loss\" , False ), Objective ( \"score\" , True )], ) eval_spec = optimizer . generate_evaluation_specification () if issubclass ( optimizer_class , MultiObjectiveOptimizer ): evaluation = eval_spec . create_evaluation ( objectives = { \"loss\" : None , \"score\" : None } ) else : evaluation = eval_spec . create_evaluation ( objectives = { \"loss\" : None }) optimizer . report ( evaluation ) for _ in range ( n_max_evaluations ): try : eval_spec = optimizer . generate_evaluation_specification () except OptimizationComplete : break loss = quadratic_function ( p1 = eval_spec . configuration [ \"p1\" ]) if issubclass ( optimizer_class , MultiObjectiveOptimizer ): evaluation_result = { \"loss\" : loss , \"score\" : - loss } else : evaluation_result = { \"loss\" : loss } evaluation = eval_spec . create_evaluation ( objectives = evaluation_result ) optimizer . report ( evaluation ) return True raises_evaluation_error_when_reporting_unknown_objective ( optimizer_class , optimizer_kwargs ) Check if optimizer's report method raises exception in case objective is unknown. Also make sure that the faulty evaluations (and only those) are included in the exception. Parameters: Name Type Description Default optimizer_class Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializating the optimizer. ( search_space and objective(s) are set automatically by the test.) required Returns: Type Description bool True if the test is passed. Source code in blackboxopt/optimizers/testing.py def raises_evaluation_error_when_reporting_unknown_objective ( optimizer_class , optimizer_kwargs : dict ) -> bool : \"\"\"Check if optimizer's report method raises exception in case objective is unknown. Also make sure that the faulty evaluations (and only those) are included in the exception. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializating the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) Returns: `True` if the test is passed. \"\"\" opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , objective = Objective ( \"loss\" , False ), objectives = [ Objective ( \"loss\" , False )], ) es_1 = opt . generate_evaluation_specification () es_2 = opt . generate_evaluation_specification () es_3 = opt . generate_evaluation_specification () # NOTE: The following is not using pytest.raises because this would add pytest as # a regular dependency to blackboxopt. try : evaluation_1 = es_1 . create_evaluation ( objectives = { \"loss\" : 1 }) evaluation_2 = es_2 . create_evaluation ( objectives = { \"unknown_objective\" : 2 }) evaluation_3 = es_3 . create_evaluation ( objectives = { \"loss\" : 4 }) opt . report ([ evaluation_1 , evaluation_2 , evaluation_3 ]) raise AssertionError ( f \"Optimizer { optimizer_class } did not raise an ObjectivesError when a \" + \"result including an unknown objective name was reported.\" ) except EvaluationsError as exception : invalid_evaluations = [ e for e , _ in exception . evaluations_with_errors ] assert len ( invalid_evaluations ) == 1 assert evaluation_2 in invalid_evaluations return True","title":"Testing"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing","text":"Tests that can be imported and used to test optimizer implementations against this packages blackbox optimizer interface.","title":"testing"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.handles_reporting_evaluations_list","text":"Check if optimizer's report method can process an iterable of evalutions. All optimizers should be able to allow reporting batches of evalutions. It's up to the optimizer's implementation, if evaluations in a batch are processed one by one like if they were reported individually, or if a batch is handled differently. Parameters: Name Type Description Default optimizer_class Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializating the optimizer. ( search_space and objective(s) are set automatically by the test.) required Returns: Type Description bool True if the test is passed. Source code in blackboxopt/optimizers/testing.py def handles_reporting_evaluations_list ( optimizer_class , optimizer_kwargs : dict ) -> bool : \"\"\"Check if optimizer's report method can process an iterable of evalutions. All optimizers should be able to allow reporting batches of evalutions. It's up to the optimizer's implementation, if evaluations in a batch are processed one by one like if they were reported individually, or if a batch is handled differently. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializating the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) Returns: `True` if the test is passed. \"\"\" opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , objective = Objective ( \"loss\" , False ), objectives = [ Objective ( \"loss\" , False )], ) evaluations = [] for _ in range ( 3 ): es = opt . generate_evaluation_specification () evaluation = es . create_evaluation ( objectives = { \"loss\" : 0.42 }) evaluations . append ( evaluation ) opt . report ( evaluations ) return True","title":"handles_reporting_evaluations_list()"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.is_deterministic_with_fixed_seed","text":"Check if optimizer is deterministic. Repeatedly initialize the optimizer with the same parameter space and a fixed seed, get an evaluation specification, report a placeholder result and get another evaluation specification. The configuration of all final evaluation specifications should be equal. Parameters: Name Type Description Default optimizer_class Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializating the optimizer. ( search_space and objective(s) are set automatically by the test.) required Returns: Type Description bool True if the test is passed. Source code in blackboxopt/optimizers/testing.py def is_deterministic_with_fixed_seed ( optimizer_class , optimizer_kwargs : dict ) -> bool : \"\"\"Check if optimizer is deterministic. Repeatedly initialize the optimizer with the same parameter space and a fixed seed, get an evaluation specification, report a placeholder result and get another evaluation specification. The configuration of all final evaluation specifications should be equal. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializating the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) Returns: `True` if the test is passed. \"\"\" final_configurations = [] for _ in range ( 2 ): opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , objective = Objective ( \"loss\" , False ), objectives = [ Objective ( \"loss\" , False )], ) es1 = opt . generate_evaluation_specification () evaluation1 = es1 . create_evaluation ( objectives = { \"loss\" : 0.42 }) opt . report ( evaluation1 ) es2 = opt . generate_evaluation_specification () final_configurations . append ( es2 . configuration . copy ()) assert final_configurations [ 0 ] == final_configurations [ 1 ] return True","title":"is_deterministic_with_fixed_seed()"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.optimize_single_parameter_sequentially_for_n_max_evaluations","text":"[summary] Parameters: Name Type Description Default optimizer_class [description] required optimizer_kwargs dict [description] required n_max_evaluations int [description] 20 Returns: Type Description bool [description] Source code in blackboxopt/optimizers/testing.py def optimize_single_parameter_sequentially_for_n_max_evaluations ( optimizer_class , optimizer_kwargs : dict , n_max_evaluations : int = 20 ) -> bool : \"\"\"[summary] Args: optimizer_class: [description] optimizer_kwargs: [description] n_max_evaluations: [description] Returns: [description] \"\"\" def quadratic_function ( p1 ): return p1 ** 2 assert issubclass ( optimizer_class , Optimizer ), ( \"The default test suite is only applicable for implementations of \" \"blackboxopt.base.Optimizer\" ) optimizer = _initialize_optimizer ( optimizer_class , optimizer_kwargs , objective = Objective ( \"loss\" , False ), objectives = [ Objective ( \"loss\" , False ), Objective ( \"score\" , True )], ) eval_spec = optimizer . generate_evaluation_specification () if issubclass ( optimizer_class , MultiObjectiveOptimizer ): evaluation = eval_spec . create_evaluation ( objectives = { \"loss\" : None , \"score\" : None } ) else : evaluation = eval_spec . create_evaluation ( objectives = { \"loss\" : None }) optimizer . report ( evaluation ) for _ in range ( n_max_evaluations ): try : eval_spec = optimizer . generate_evaluation_specification () except OptimizationComplete : break loss = quadratic_function ( p1 = eval_spec . configuration [ \"p1\" ]) if issubclass ( optimizer_class , MultiObjectiveOptimizer ): evaluation_result = { \"loss\" : loss , \"score\" : - loss } else : evaluation_result = { \"loss\" : loss } evaluation = eval_spec . create_evaluation ( objectives = evaluation_result ) optimizer . report ( evaluation ) return True","title":"optimize_single_parameter_sequentially_for_n_max_evaluations()"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.raises_evaluation_error_when_reporting_unknown_objective","text":"Check if optimizer's report method raises exception in case objective is unknown. Also make sure that the faulty evaluations (and only those) are included in the exception. Parameters: Name Type Description Default optimizer_class Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializating the optimizer. ( search_space and objective(s) are set automatically by the test.) required Returns: Type Description bool True if the test is passed. Source code in blackboxopt/optimizers/testing.py def raises_evaluation_error_when_reporting_unknown_objective ( optimizer_class , optimizer_kwargs : dict ) -> bool : \"\"\"Check if optimizer's report method raises exception in case objective is unknown. Also make sure that the faulty evaluations (and only those) are included in the exception. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializating the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) Returns: `True` if the test is passed. \"\"\" opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , objective = Objective ( \"loss\" , False ), objectives = [ Objective ( \"loss\" , False )], ) es_1 = opt . generate_evaluation_specification () es_2 = opt . generate_evaluation_specification () es_3 = opt . generate_evaluation_specification () # NOTE: The following is not using pytest.raises because this would add pytest as # a regular dependency to blackboxopt. try : evaluation_1 = es_1 . create_evaluation ( objectives = { \"loss\" : 1 }) evaluation_2 = es_2 . create_evaluation ( objectives = { \"unknown_objective\" : 2 }) evaluation_3 = es_3 . create_evaluation ( objectives = { \"loss\" : 4 }) opt . report ([ evaluation_1 , evaluation_2 , evaluation_3 ]) raise AssertionError ( f \"Optimizer { optimizer_class } did not raise an ObjectivesError when a \" + \"result including an unknown objective name was reported.\" ) except EvaluationsError as exception : invalid_evaluations = [ e for e , _ in exception . evaluations_with_errors ] assert len ( invalid_evaluations ) == 1 assert evaluation_2 in invalid_evaluations return True","title":"raises_evaluation_error_when_reporting_unknown_objective()"},{"location":"reference/optimizers/staged/bohb/","text":"blackboxopt.optimizers.staged.bohb convert_from_statsmodels_kde_representation ( array , vartypes ) Convert numerical representation for categoricals and ordinals back into the unit hypercube. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations following the statsmodels convention for categorical and ordinal values being integers. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation consistent with a numerical representation in the hypercube. Source code in blackboxopt/optimizers/staged/bohb.py def convert_from_statsmodels_kde_representation ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Convert numerical representation for categoricals and ordinals back into the unit hypercube. Args: array: Numerical representation of the configurations following the statsmodels convention for categorical and ordinal values being integers. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation consistent with a numerical representation in the hypercube. \"\"\" processed_vector = np . copy ( array ) for i in range ( len ( processed_vector )): if vartypes [ i ] != 0 : num_values = abs ( vartypes [ i ]) processed_vector [ i ] = ( processed_vector [ i ] + 0.5 ) / num_values return processed_vector convert_to_statsmodels_kde_representation ( array , vartypes ) Convert numerical representation for categoricals and ordinals to integers. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations with categorical and ordinal values mapped into the unit hypercube. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation consistent with the statsmodels package. Source code in blackboxopt/optimizers/staged/bohb.py def convert_to_statsmodels_kde_representation ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Convert numerical representation for categoricals and ordinals to integers. Args: array: Numerical representation of the configurations with categorical and ordinal values mapped into the unit hypercube. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation consistent with the statsmodels package. \"\"\" processed_vector = np . copy ( array ) for i in range ( len ( processed_vector )): if vartypes [ i ] == 0 : continue num_values = abs ( vartypes [ i ]) processed_vector [ i ] = np . around (( processed_vector [ i ] * num_values ) - 0.5 ) return processed_vector impute_conditional_data ( array , vartypes ) Impute NaNs in numerical representation with observed values or prior samples. This method is needed to use the statsmodels KDE, which doesn't handle missing values out of the box. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations which can include NaN values for inactive variables. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation where all NaNs have been replaced with observed values or prior samples. Source code in blackboxopt/optimizers/staged/bohb.py def impute_conditional_data ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Impute NaNs in numerical representation with observed values or prior samples. This method is needed to use the `statsmodels` KDE, which doesn't handle missing values out of the box. Args: array: Numerical representation of the configurations which can include NaN values for inactive variables. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation where all NaNs have been replaced with observed values or prior samples. \"\"\" return_array = np . empty_like ( array ) for i in range ( array . shape [ 0 ]): datum = np . copy ( array [ i ]) nan_indices = np . argwhere ( np . isnan ( datum )) . flatten () while np . any ( nan_indices ): nan_idx = nan_indices [ 0 ] valid_indices = np . argwhere ( np . isfinite ( array [:, nan_idx ])) . flatten () if len ( valid_indices ) > 0 : # pick one of them at random and overwrite all NaN values row_idx = np . random . choice ( valid_indices ) datum [ nan_indices ] = array [ row_idx , nan_indices ] else : # no point in the data has this value activated, so fill it with a valid # but random value t = vartypes [ nan_idx ] if t == 0 : datum [ nan_idx ] = np . random . rand () elif t > 0 : datum [ nan_idx ] = np . random . randint ( t ) elif t < 0 : datum [ nan_idx ] = np . random . randint ( - t ) nan_indices = np . argwhere ( np . isnan ( datum )) . flatten () return_array [ i , :] = datum return return_array sample_around_values ( datum , bandwidths , vartypes , min_bandwidth , bw_factor ) Sample numerical representation close to a given datum. This is specific to the KDE in statsmodels and their kernel for the different variable types. Parameters: Name Type Description Default datum ndarray Numerical representation of a configuration that is used as the 'center' for sampling. required bandwidths ndarray Bandwidth of the corresponding kernels in each dimension. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values. required min_bandwidth float Smallest allowed bandwidth. Ensures diversity even if all samples agree on a value in a dimension. required bw_factor float To increase diversity, the bandwidth is actually multiplied by this factor before sampling. required Returns: Type Description Optional[numpy.ndarray] Numerical representation of a configuration close to the provided datum. Source code in blackboxopt/optimizers/staged/bohb.py def sample_around_values ( datum : np . ndarray , bandwidths : np . ndarray , vartypes : Union [ list , np . ndarray ], min_bandwidth : float , bw_factor : float , ) -> Optional [ np . ndarray ]: \"\"\"Sample numerical representation close to a given datum. This is specific to the KDE in statsmodels and their kernel for the different variable types. Args: datum: Numerical representation of a configuration that is used as the 'center' for sampling. bandwidths: Bandwidth of the corresponding kernels in each dimension. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values. min_bandwidth: Smallest allowed bandwidth. Ensures diversity even if all samples agree on a value in a dimension. bw_factor: To increase diversity, the bandwidth is actually multiplied by this factor before sampling. Returns: Numerical representation of a configuration close to the provided datum. \"\"\" vector = [] for m , bw , t in zip ( datum , bandwidths , vartypes ): bw = max ( bw , min_bandwidth ) if t == 0 : bw = bw_factor * bw try : v = sps . truncnorm . rvs ( - m / bw , ( 1 - m ) / bw , loc = m , scale = bw ) except Exception : return None elif t > 0 : v = m if np . random . rand () < ( 1 - bw ) else np . random . randint ( t ) else : bw = min ( 0.9999 , bw ) # bandwidth has to be less the one for this kernel! diffs = np . abs ( np . arange ( - t ) - m ) probs = 0.5 * ( 1 - bw ) * ( bw ** diffs ) idx = diffs == 0 probs [ idx ] = ( idx * ( 1 - bw ))[ idx ] probs /= probs . sum () v = np . random . choice ( - t , p = probs ) vector . append ( v ) return np . array ( vector ) Sampler digest_evaluation ( self , evaluation ) [summary] Parameters: Name Type Description Default evaluation Evaluation [description] required Source code in blackboxopt/optimizers/staged/bohb.py def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"[summary] Args: evaluation: [description] \"\"\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is None : loss = np . inf else : loss = ( - objective_value if self . objective . greater_is_better else objective_value ) config_vector = self . search_space . to_numerical ( evaluation . configuration ) config_vector = convert_to_statsmodels_kde_representation ( config_vector , self . vartypes ) fidelity = evaluation . settings [ \"fidelity\" ] if fidelity not in self . configs . keys (): self . configs [ fidelity ] = [] self . losses [ fidelity ] = [] self . configs [ fidelity ] . append ( config_vector ) self . losses [ fidelity ] . append ( loss ) if bool ( self . kde_models . keys ()) and max ( self . kde_models . keys ()) > fidelity : return if np . isfinite ( self . losses [ fidelity ]) . sum () <= self . min_samples_in_model - 1 : n_runs_finite_loss = np . isfinite ( self . losses [ fidelity ]) . sum () self . logger . debug ( f \"Only { n_runs_finite_loss } run(s) with a finite loss for fidelity \" + f \" { fidelity } available, need more than { self . min_samples_in_model + 1 } \" + \"-> can't build model!\" ) return train_configs = np . array ( self . configs [ fidelity ]) train_losses = np . array ( self . losses [ fidelity ]) n_good = max ( self . min_samples_in_model , ( self . top_n_percent * train_configs . shape [ 0 ]) // 100 , ) n_bad = max ( self . min_samples_in_model , (( 100 - self . top_n_percent ) * train_configs . shape [ 0 ]) // 100 , ) # Refit KDE for the current fidelity idx = np . argsort ( train_losses ) train_data_good = impute_conditional_data ( train_configs [ idx [: n_good ]], self . vartypes ) train_data_bad = impute_conditional_data ( train_configs [ idx [ n_good : n_good + n_bad ]], self . vartypes , ) if train_data_good . shape [ 0 ] <= train_data_good . shape [ 1 ]: return if train_data_bad . shape [ 0 ] <= train_data_bad . shape [ 1 ]: return # more expensive crossvalidation method # bw_estimation = 'cv_ls' # quick rule of thumb bw_estimation = \"normal_reference\" bad_kde = sm . nonparametric . KDEMultivariate ( data = train_data_bad , var_type = self . kde_vartypes , bw = bw_estimation ) good_kde = sm . nonparametric . KDEMultivariate ( data = train_data_good , var_type = self . kde_vartypes , bw = bw_estimation ) bad_kde . bw = np . clip ( bad_kde . bw , self . min_bandwidth , None ) good_kde . bw = np . clip ( good_kde . bw , self . min_bandwidth , None ) self . kde_models [ fidelity ] = { \"good\" : good_kde , \"bad\" : bad_kde } # update probs for the categorical parameters for later sampling self . logger . debug ( f \"done building a new model for fidelity { fidelity } based on \" + f \" { n_good } / { n_bad } split \\n Best loss for this fidelity: \" + f \" { np . min ( train_losses ) } \\n \" + ( \"=\" * 40 ) ) sample_configuration ( self ) [summary] Returns: Type Description Tuple[dict, dict] [description] Source code in blackboxopt/optimizers/staged/bohb.py def sample_configuration ( self ) -> Tuple [ dict , dict ]: \"\"\"[summary] Returns: [description] \"\"\" self . logger . debug ( \"start sampling a new configuration.\" ) # Sample from prior, if no model is available or with given probability if len ( self . kde_models ) == 0 or np . random . rand () < self . random_fraction : return self . search_space . sample (), { \"model_based_pick\" : False } best = np . inf best_vector = None try : # sample from largest fidelity fidelity = max ( self . kde_models . keys ()) good = self . kde_models [ fidelity ][ \"good\" ] . pdf bad = self . kde_models [ fidelity ][ \"bad\" ] . pdf minimize_me = lambda x : max ( 1e-32 , bad ( x )) / max ( good ( x ), 1e-32 ) kde_good = self . kde_models [ fidelity ][ \"good\" ] kde_bad = self . kde_models [ fidelity ][ \"bad\" ] for _ in range ( self . num_samples ): idx = np . random . randint ( 0 , len ( kde_good . data )) datum = kde_good . data [ idx ] vector = sample_around_values ( datum , kde_good . bw , self . vartypes , self . min_bandwidth , self . bw_factor , ) if vector is None : continue val = minimize_me ( vector ) if not np . isfinite ( val ): self . logger . warning ( \"sampled vector: %s has EI value %s \" % ( vector , val ) ) self . logger . warning ( \"data in the KDEs: \\n %s \\n %s \" % ( kde_good . data , kde_bad . data ) ) self . logger . warning ( \"bandwidth of the KDEs: \\n %s \\n %s \" % ( kde_good . bw , kde_bad . bw ) ) # right now, this happens because a KDE does not contain all values # for a categorical parameter this cannot be fixed with the # statsmodels KDE, so for now, we are just going to evaluate this # one if the good_kde has a finite value, i.e. there is no config # with that value in the bad kde, so it shouldn't be terrible. if np . isfinite ( good ( vector )) and best_vector is not None : best_vector = vector continue if val < best : best = val best_vector = convert_from_statsmodels_kde_representation ( vector , self . vartypes ) if best_vector is None : self . logger . debug ( f \"Sampling based optimization with { self . num_samples } samples did \" + \"not find any finite/numerical acquisition function value \" + \"-> using random configuration\" ) return self . search_space . sample (), { \"model_based_pick\" : False } else : self . logger . debug ( \"best_vector: {} , {} , {} , {} \" . format ( best_vector , best , good ( best_vector ), bad ( best_vector ) ) ) return ( self . search_space . from_numerical ( best_vector ), { \"model_based_pick\" : True }, ) except Exception : self . logger . debug ( \"Sample base optimization failed. Falling back to a random sample.\" ) return self . search_space . sample (), { \"model_based_pick\" : False }","title":"Bohb"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb","text":"","title":"bohb"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.convert_from_statsmodels_kde_representation","text":"Convert numerical representation for categoricals and ordinals back into the unit hypercube. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations following the statsmodels convention for categorical and ordinal values being integers. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation consistent with a numerical representation in the hypercube. Source code in blackboxopt/optimizers/staged/bohb.py def convert_from_statsmodels_kde_representation ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Convert numerical representation for categoricals and ordinals back into the unit hypercube. Args: array: Numerical representation of the configurations following the statsmodels convention for categorical and ordinal values being integers. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation consistent with a numerical representation in the hypercube. \"\"\" processed_vector = np . copy ( array ) for i in range ( len ( processed_vector )): if vartypes [ i ] != 0 : num_values = abs ( vartypes [ i ]) processed_vector [ i ] = ( processed_vector [ i ] + 0.5 ) / num_values return processed_vector","title":"convert_from_statsmodels_kde_representation()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.convert_to_statsmodels_kde_representation","text":"Convert numerical representation for categoricals and ordinals to integers. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations with categorical and ordinal values mapped into the unit hypercube. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation consistent with the statsmodels package. Source code in blackboxopt/optimizers/staged/bohb.py def convert_to_statsmodels_kde_representation ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Convert numerical representation for categoricals and ordinals to integers. Args: array: Numerical representation of the configurations with categorical and ordinal values mapped into the unit hypercube. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation consistent with the statsmodels package. \"\"\" processed_vector = np . copy ( array ) for i in range ( len ( processed_vector )): if vartypes [ i ] == 0 : continue num_values = abs ( vartypes [ i ]) processed_vector [ i ] = np . around (( processed_vector [ i ] * num_values ) - 0.5 ) return processed_vector","title":"convert_to_statsmodels_kde_representation()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.impute_conditional_data","text":"Impute NaNs in numerical representation with observed values or prior samples. This method is needed to use the statsmodels KDE, which doesn't handle missing values out of the box. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations which can include NaN values for inactive variables. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation where all NaNs have been replaced with observed values or prior samples. Source code in blackboxopt/optimizers/staged/bohb.py def impute_conditional_data ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Impute NaNs in numerical representation with observed values or prior samples. This method is needed to use the `statsmodels` KDE, which doesn't handle missing values out of the box. Args: array: Numerical representation of the configurations which can include NaN values for inactive variables. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation where all NaNs have been replaced with observed values or prior samples. \"\"\" return_array = np . empty_like ( array ) for i in range ( array . shape [ 0 ]): datum = np . copy ( array [ i ]) nan_indices = np . argwhere ( np . isnan ( datum )) . flatten () while np . any ( nan_indices ): nan_idx = nan_indices [ 0 ] valid_indices = np . argwhere ( np . isfinite ( array [:, nan_idx ])) . flatten () if len ( valid_indices ) > 0 : # pick one of them at random and overwrite all NaN values row_idx = np . random . choice ( valid_indices ) datum [ nan_indices ] = array [ row_idx , nan_indices ] else : # no point in the data has this value activated, so fill it with a valid # but random value t = vartypes [ nan_idx ] if t == 0 : datum [ nan_idx ] = np . random . rand () elif t > 0 : datum [ nan_idx ] = np . random . randint ( t ) elif t < 0 : datum [ nan_idx ] = np . random . randint ( - t ) nan_indices = np . argwhere ( np . isnan ( datum )) . flatten () return_array [ i , :] = datum return return_array","title":"impute_conditional_data()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.sample_around_values","text":"Sample numerical representation close to a given datum. This is specific to the KDE in statsmodels and their kernel for the different variable types. Parameters: Name Type Description Default datum ndarray Numerical representation of a configuration that is used as the 'center' for sampling. required bandwidths ndarray Bandwidth of the corresponding kernels in each dimension. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values. required min_bandwidth float Smallest allowed bandwidth. Ensures diversity even if all samples agree on a value in a dimension. required bw_factor float To increase diversity, the bandwidth is actually multiplied by this factor before sampling. required Returns: Type Description Optional[numpy.ndarray] Numerical representation of a configuration close to the provided datum. Source code in blackboxopt/optimizers/staged/bohb.py def sample_around_values ( datum : np . ndarray , bandwidths : np . ndarray , vartypes : Union [ list , np . ndarray ], min_bandwidth : float , bw_factor : float , ) -> Optional [ np . ndarray ]: \"\"\"Sample numerical representation close to a given datum. This is specific to the KDE in statsmodels and their kernel for the different variable types. Args: datum: Numerical representation of a configuration that is used as the 'center' for sampling. bandwidths: Bandwidth of the corresponding kernels in each dimension. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values. min_bandwidth: Smallest allowed bandwidth. Ensures diversity even if all samples agree on a value in a dimension. bw_factor: To increase diversity, the bandwidth is actually multiplied by this factor before sampling. Returns: Numerical representation of a configuration close to the provided datum. \"\"\" vector = [] for m , bw , t in zip ( datum , bandwidths , vartypes ): bw = max ( bw , min_bandwidth ) if t == 0 : bw = bw_factor * bw try : v = sps . truncnorm . rvs ( - m / bw , ( 1 - m ) / bw , loc = m , scale = bw ) except Exception : return None elif t > 0 : v = m if np . random . rand () < ( 1 - bw ) else np . random . randint ( t ) else : bw = min ( 0.9999 , bw ) # bandwidth has to be less the one for this kernel! diffs = np . abs ( np . arange ( - t ) - m ) probs = 0.5 * ( 1 - bw ) * ( bw ** diffs ) idx = diffs == 0 probs [ idx ] = ( idx * ( 1 - bw ))[ idx ] probs /= probs . sum () v = np . random . choice ( - t , p = probs ) vector . append ( v ) return np . array ( vector )","title":"sample_around_values()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler","text":"","title":"Sampler"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler.digest_evaluation","text":"[summary] Parameters: Name Type Description Default evaluation Evaluation [description] required Source code in blackboxopt/optimizers/staged/bohb.py def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"[summary] Args: evaluation: [description] \"\"\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is None : loss = np . inf else : loss = ( - objective_value if self . objective . greater_is_better else objective_value ) config_vector = self . search_space . to_numerical ( evaluation . configuration ) config_vector = convert_to_statsmodels_kde_representation ( config_vector , self . vartypes ) fidelity = evaluation . settings [ \"fidelity\" ] if fidelity not in self . configs . keys (): self . configs [ fidelity ] = [] self . losses [ fidelity ] = [] self . configs [ fidelity ] . append ( config_vector ) self . losses [ fidelity ] . append ( loss ) if bool ( self . kde_models . keys ()) and max ( self . kde_models . keys ()) > fidelity : return if np . isfinite ( self . losses [ fidelity ]) . sum () <= self . min_samples_in_model - 1 : n_runs_finite_loss = np . isfinite ( self . losses [ fidelity ]) . sum () self . logger . debug ( f \"Only { n_runs_finite_loss } run(s) with a finite loss for fidelity \" + f \" { fidelity } available, need more than { self . min_samples_in_model + 1 } \" + \"-> can't build model!\" ) return train_configs = np . array ( self . configs [ fidelity ]) train_losses = np . array ( self . losses [ fidelity ]) n_good = max ( self . min_samples_in_model , ( self . top_n_percent * train_configs . shape [ 0 ]) // 100 , ) n_bad = max ( self . min_samples_in_model , (( 100 - self . top_n_percent ) * train_configs . shape [ 0 ]) // 100 , ) # Refit KDE for the current fidelity idx = np . argsort ( train_losses ) train_data_good = impute_conditional_data ( train_configs [ idx [: n_good ]], self . vartypes ) train_data_bad = impute_conditional_data ( train_configs [ idx [ n_good : n_good + n_bad ]], self . vartypes , ) if train_data_good . shape [ 0 ] <= train_data_good . shape [ 1 ]: return if train_data_bad . shape [ 0 ] <= train_data_bad . shape [ 1 ]: return # more expensive crossvalidation method # bw_estimation = 'cv_ls' # quick rule of thumb bw_estimation = \"normal_reference\" bad_kde = sm . nonparametric . KDEMultivariate ( data = train_data_bad , var_type = self . kde_vartypes , bw = bw_estimation ) good_kde = sm . nonparametric . KDEMultivariate ( data = train_data_good , var_type = self . kde_vartypes , bw = bw_estimation ) bad_kde . bw = np . clip ( bad_kde . bw , self . min_bandwidth , None ) good_kde . bw = np . clip ( good_kde . bw , self . min_bandwidth , None ) self . kde_models [ fidelity ] = { \"good\" : good_kde , \"bad\" : bad_kde } # update probs for the categorical parameters for later sampling self . logger . debug ( f \"done building a new model for fidelity { fidelity } based on \" + f \" { n_good } / { n_bad } split \\n Best loss for this fidelity: \" + f \" { np . min ( train_losses ) } \\n \" + ( \"=\" * 40 ) )","title":"digest_evaluation()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler.sample_configuration","text":"[summary] Returns: Type Description Tuple[dict, dict] [description] Source code in blackboxopt/optimizers/staged/bohb.py def sample_configuration ( self ) -> Tuple [ dict , dict ]: \"\"\"[summary] Returns: [description] \"\"\" self . logger . debug ( \"start sampling a new configuration.\" ) # Sample from prior, if no model is available or with given probability if len ( self . kde_models ) == 0 or np . random . rand () < self . random_fraction : return self . search_space . sample (), { \"model_based_pick\" : False } best = np . inf best_vector = None try : # sample from largest fidelity fidelity = max ( self . kde_models . keys ()) good = self . kde_models [ fidelity ][ \"good\" ] . pdf bad = self . kde_models [ fidelity ][ \"bad\" ] . pdf minimize_me = lambda x : max ( 1e-32 , bad ( x )) / max ( good ( x ), 1e-32 ) kde_good = self . kde_models [ fidelity ][ \"good\" ] kde_bad = self . kde_models [ fidelity ][ \"bad\" ] for _ in range ( self . num_samples ): idx = np . random . randint ( 0 , len ( kde_good . data )) datum = kde_good . data [ idx ] vector = sample_around_values ( datum , kde_good . bw , self . vartypes , self . min_bandwidth , self . bw_factor , ) if vector is None : continue val = minimize_me ( vector ) if not np . isfinite ( val ): self . logger . warning ( \"sampled vector: %s has EI value %s \" % ( vector , val ) ) self . logger . warning ( \"data in the KDEs: \\n %s \\n %s \" % ( kde_good . data , kde_bad . data ) ) self . logger . warning ( \"bandwidth of the KDEs: \\n %s \\n %s \" % ( kde_good . bw , kde_bad . bw ) ) # right now, this happens because a KDE does not contain all values # for a categorical parameter this cannot be fixed with the # statsmodels KDE, so for now, we are just going to evaluate this # one if the good_kde has a finite value, i.e. there is no config # with that value in the bad kde, so it shouldn't be terrible. if np . isfinite ( good ( vector )) and best_vector is not None : best_vector = vector continue if val < best : best = val best_vector = convert_from_statsmodels_kde_representation ( vector , self . vartypes ) if best_vector is None : self . logger . debug ( f \"Sampling based optimization with { self . num_samples } samples did \" + \"not find any finite/numerical acquisition function value \" + \"-> using random configuration\" ) return self . search_space . sample (), { \"model_based_pick\" : False } else : self . logger . debug ( \"best_vector: {} , {} , {} , {} \" . format ( best_vector , best , good ( best_vector ), bad ( best_vector ) ) ) return ( self . search_space . from_numerical ( best_vector ), { \"model_based_pick\" : True }, ) except Exception : self . logger . debug ( \"Sample base optimization failed. Falling back to a random sample.\" ) return self . search_space . sample (), { \"model_based_pick\" : False }","title":"sample_configuration()"},{"location":"reference/optimizers/staged/configuration_sampler/","text":"blackboxopt.optimizers.staged.configuration_sampler RandomSearchSampler digest_evaluation ( self , evaluation ) Random Search is stateless and does nothing with finished evaluations. Source code in blackboxopt/optimizers/staged/configuration_sampler.py def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"Random Search is stateless and does nothing with finished evaluations.\"\"\" sample_configuration ( self ) Pick the next configuration. Returns: Type Description Tuple[dict, dict] The configuration to be evaluated, Additional information that will be added to the optimizer_info dict. Source code in blackboxopt/optimizers/staged/configuration_sampler.py def sample_configuration ( self ) -> Tuple [ dict , dict ]: return self . search_space . sample (), {}","title":"Configuration sampler"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler","text":"","title":"configuration_sampler"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.RandomSearchSampler","text":"","title":"RandomSearchSampler"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.RandomSearchSampler.digest_evaluation","text":"Random Search is stateless and does nothing with finished evaluations. Source code in blackboxopt/optimizers/staged/configuration_sampler.py def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"Random Search is stateless and does nothing with finished evaluations.\"\"\"","title":"digest_evaluation()"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.RandomSearchSampler.sample_configuration","text":"Pick the next configuration. Returns: Type Description Tuple[dict, dict] The configuration to be evaluated, Additional information that will be added to the optimizer_info dict. Source code in blackboxopt/optimizers/staged/configuration_sampler.py def sample_configuration ( self ) -> Tuple [ dict , dict ]: return self . search_space . sample (), {}","title":"sample_configuration()"},{"location":"reference/optimizers/staged/hyperband/","text":"blackboxopt.optimizers.staged.hyperband create_hyperband_iteration ( iteration_index , min_fidelity , max_fidelity , eta , config_sampler , objective , logger ) Optimizer specific way to create a new blackboxopt.optimizer.staged.iteration.StagedIteration object Source code in blackboxopt/optimizers/staged/hyperband.py def create_hyperband_iteration ( iteration_index : int , min_fidelity : float , max_fidelity : float , eta : float , config_sampler : StagedIterationConfigurationSampler , objective : Objective , logger : logging . Logger , ) -> StagedIteration : \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.staged.iteration.StagedIteration` object \"\"\" # 's_max + 1' in the paper max_num_stages = 1 + int ( math . floor ( math . log ( max_fidelity / min_fidelity , eta ))) # 's+1' in the paper num_stages = max_num_stages - ( iteration_index % ( max_num_stages )) num_configs_first_stage = int ( math . ceil (( max_num_stages / num_stages ) * eta ** ( num_stages - 1 )) ) num_configs_per_stage = [ int ( num_configs_first_stage // ( eta ** i )) for i in range ( num_stages ) ] fidelities_per_stage = [ max_fidelity / eta ** i for i in range ( num_stages - 1 , - 1 , - 1 ) ] # Hyperband simple draws random configurations, and there is no additional # information that needs to be stored return StagedIteration ( iteration_index , num_configs_per_stage , fidelities_per_stage , config_sampler , greedy_promotion , objective , logger = logger , )","title":"Hyperband"},{"location":"reference/optimizers/staged/hyperband/#blackboxopt.optimizers.staged.hyperband","text":"","title":"hyperband"},{"location":"reference/optimizers/staged/hyperband/#blackboxopt.optimizers.staged.hyperband.create_hyperband_iteration","text":"Optimizer specific way to create a new blackboxopt.optimizer.staged.iteration.StagedIteration object Source code in blackboxopt/optimizers/staged/hyperband.py def create_hyperband_iteration ( iteration_index : int , min_fidelity : float , max_fidelity : float , eta : float , config_sampler : StagedIterationConfigurationSampler , objective : Objective , logger : logging . Logger , ) -> StagedIteration : \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.staged.iteration.StagedIteration` object \"\"\" # 's_max + 1' in the paper max_num_stages = 1 + int ( math . floor ( math . log ( max_fidelity / min_fidelity , eta ))) # 's+1' in the paper num_stages = max_num_stages - ( iteration_index % ( max_num_stages )) num_configs_first_stage = int ( math . ceil (( max_num_stages / num_stages ) * eta ** ( num_stages - 1 )) ) num_configs_per_stage = [ int ( num_configs_first_stage // ( eta ** i )) for i in range ( num_stages ) ] fidelities_per_stage = [ max_fidelity / eta ** i for i in range ( num_stages - 1 , - 1 , - 1 ) ] # Hyperband simple draws random configurations, and there is no additional # information that needs to be stored return StagedIteration ( iteration_index , num_configs_per_stage , fidelities_per_stage , config_sampler , greedy_promotion , objective , logger = logger , )","title":"create_hyperband_iteration()"},{"location":"reference/optimizers/staged/iteration/","text":"blackboxopt.optimizers.staged.iteration Datum dataclass Small container for bookkeeping only. StagedIteration digest_evaluation ( self , evaluation_specificiation_id , evaluation ) Registers the result of an evaluation. Parameters: Name Type Description Default id [description] required evaluation Evaluation [description] required Source code in blackboxopt/optimizers/staged/iteration.py def digest_evaluation ( self , evaluation_specificiation_id : UUID , evaluation : Evaluation ): \"\"\"Registers the result of an evaluation. Args: id: [description] evaluation: [description] \"\"\" self . config_sampler . digest_evaluation ( evaluation ) i = self . pending_evaluations . pop ( evaluation_specificiation_id ) d = self . evaluation_data [ self . current_stage ][ i ] d . status = \"FINISHED\" if not evaluation . all_objectives_none else \"CRASHED\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is not None : d . loss = ( - objective_value if self . objective . greater_is_better else objective_value ) # quick check if all configurations have finished yet if len ( self . evaluation_data [ self . current_stage ]) == self . num_configs [ self . current_stage ] and all ( [ e . status in [ \"FINISHED\" , \"CRASHED\" ] for e in self . evaluation_data [ self . current_stage ] ] ): self . _progress_to_next_stage () generate_evaluation_specification ( self ) Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: Type Description Optional[blackboxopt.evaluation.EvaluationSpecification] [description] Source code in blackboxopt/optimizers/staged/iteration.py def generate_evaluation_specification ( self ) -> Optional [ EvaluationSpecification ]: \"\"\"Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: [description] \"\"\" if self . finished : return None # try to find a queued entry first for i , d in enumerate ( self . evaluation_data [ self . current_stage ]): if d . status == \"QUEUED\" : es = copy . deepcopy ( self . eval_specs [ d . config_key ]) es . settings [ \"fidelity\" ] = self . fidelities [ self . current_stage ] d . status = \"RUNNING\" self . pending_evaluations [ es . optimizer_info [ \"id\" ]] = i return es # sample a new configuration if there are empty slots to be filled if ( len ( self . evaluation_data [ self . current_stage ]) < self . num_configs [ self . current_stage ] ): conf_key = ( self . iteration , self . current_stage , len ( self . evaluation_data [ self . current_stage ]), ) conf , opt_info = self . config_sampler . sample_configuration () opt_info . update ({ \"configuration_key\" : conf_key , \"id\" : str ( uuid4 ())}) self . eval_specs [ conf_key ] = EvaluationSpecification ( configuration = conf , settings = {}, optimizer_info = opt_info ) self . evaluation_data [ self . current_stage ] . append ( Datum ( conf_key , \"QUEUED\" )) # To understand recursion, you first must understand recursion :) return self . generate_evaluation_specification () # at this point there are pending evaluations and this iteration has to wait return None","title":"Iteration"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration","text":"","title":"iteration"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.Datum","text":"Small container for bookkeeping only.","title":"Datum"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration","text":"","title":"StagedIteration"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration.digest_evaluation","text":"Registers the result of an evaluation. Parameters: Name Type Description Default id [description] required evaluation Evaluation [description] required Source code in blackboxopt/optimizers/staged/iteration.py def digest_evaluation ( self , evaluation_specificiation_id : UUID , evaluation : Evaluation ): \"\"\"Registers the result of an evaluation. Args: id: [description] evaluation: [description] \"\"\" self . config_sampler . digest_evaluation ( evaluation ) i = self . pending_evaluations . pop ( evaluation_specificiation_id ) d = self . evaluation_data [ self . current_stage ][ i ] d . status = \"FINISHED\" if not evaluation . all_objectives_none else \"CRASHED\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is not None : d . loss = ( - objective_value if self . objective . greater_is_better else objective_value ) # quick check if all configurations have finished yet if len ( self . evaluation_data [ self . current_stage ]) == self . num_configs [ self . current_stage ] and all ( [ e . status in [ \"FINISHED\" , \"CRASHED\" ] for e in self . evaluation_data [ self . current_stage ] ] ): self . _progress_to_next_stage ()","title":"digest_evaluation()"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration.generate_evaluation_specification","text":"Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: Type Description Optional[blackboxopt.evaluation.EvaluationSpecification] [description] Source code in blackboxopt/optimizers/staged/iteration.py def generate_evaluation_specification ( self ) -> Optional [ EvaluationSpecification ]: \"\"\"Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: [description] \"\"\" if self . finished : return None # try to find a queued entry first for i , d in enumerate ( self . evaluation_data [ self . current_stage ]): if d . status == \"QUEUED\" : es = copy . deepcopy ( self . eval_specs [ d . config_key ]) es . settings [ \"fidelity\" ] = self . fidelities [ self . current_stage ] d . status = \"RUNNING\" self . pending_evaluations [ es . optimizer_info [ \"id\" ]] = i return es # sample a new configuration if there are empty slots to be filled if ( len ( self . evaluation_data [ self . current_stage ]) < self . num_configs [ self . current_stage ] ): conf_key = ( self . iteration , self . current_stage , len ( self . evaluation_data [ self . current_stage ]), ) conf , opt_info = self . config_sampler . sample_configuration () opt_info . update ({ \"configuration_key\" : conf_key , \"id\" : str ( uuid4 ())}) self . eval_specs [ conf_key ] = EvaluationSpecification ( configuration = conf , settings = {}, optimizer_info = opt_info ) self . evaluation_data [ self . current_stage ] . append ( Datum ( conf_key , \"QUEUED\" )) # To understand recursion, you first must understand recursion :) return self . generate_evaluation_specification () # at this point there are pending evaluations and this iteration has to wait return None","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/staged/optimizer/","text":"blackboxopt.optimizers.staged.optimizer StagedIterationOptimizer generate_evaluation_specification ( self ) Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/staged/optimizer.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady report ( self , evaluations ) Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/staged/optimizer.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"Optimizer"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer","text":"","title":"optimizer"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer","text":"","title":"StagedIterationOptimizer"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/staged/optimizer.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer.report","text":"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/staged/optimizer.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"report()"},{"location":"reference/optimizers/staged/utils/","text":"blackboxopt.optimizers.staged.utils best_evaluation_at_highest_fidelity ( evaluations , objective ) From given list of evaluations, get the best in terms of minimal loss at the highest fidelity. Parameters: Name Type Description Default evaluations List[blackboxopt.evaluation.Evaluation] [description] required objective Objective [description] required Returns: Type Description Optional[blackboxopt.evaluation.Evaluation] [description] Source code in blackboxopt/optimizers/staged/utils.py def best_evaluation_at_highest_fidelity ( evaluations : List [ Evaluation ], objective : Objective , ) -> Optional [ Evaluation ]: \"\"\"From given list of evaluations, get the best in terms of minimal loss at the highest fidelity. Args: evaluations: [description] objective: [description] Returns: [description] \"\"\" if not evaluations : return None successes = [ evaluation for evaluation in evaluations if evaluation . objectives [ objective . name ] is not None ] if not successes : return None successful_fidelities = [ evaluation . settings [ \"fidelity\" ] for evaluation in successes ] if not successful_fidelities : return None max_successful_fidelities = max ( successful_fidelities ) successful_max_fidelity_evaluations = [ evaluation for evaluation in successes if evaluation . settings [ \"fidelity\" ] == max_successful_fidelities ] if not successful_max_fidelity_evaluations : return None sort_function = max if objective . greater_is_better else min best_evaluation = sort_function ( successful_max_fidelity_evaluations , key = lambda e : e . objectives [ objective . name ] ) return best_evaluation greedy_promotion ( data , num_configs ) Promotes the best configurations to the next stage solely relying on the current loss. Parameters: Name Type Description Default data List[blackboxopt.optimizers.staged.iteration.Datum] List with all successful evaluations for this stage. All failed configurations have already been removed. required num_configs int Maximum number of configurations to be promoted. required Returns: Type Description list List of the config_keys to be evaluated on the next higher fidelity. These must only include config_keys found in data und must also be of at most length num_configs . If fewer ids are returned, the remaining configurations for the next stage will be sampled using the config_sample_function of the staged_iteration. Source code in blackboxopt/optimizers/staged/utils.py def greedy_promotion ( data : List [ Datum ], num_configs : int ) -> list : \"\"\"Promotes the best configurations to the next stage solely relying on the current loss. Args: data: List with all successful evaluations for this stage. All failed configurations have already been removed. num_configs: Maximum number of configurations to be promoted. Returns: List of the config_keys to be evaluated on the next higher fidelity. These must only include config_keys found in `data` und must also be of at most length `num_configs`. If fewer ids are returned, the remaining configurations for the next stage will be sampled using the `config_sample_function` of the staged_iteration. \"\"\" losses = [ d . loss for d in data ] ranks = np . argsort ( np . argsort ( losses )) n = min ( num_configs , len ( data )) return [ datum . config_key for rank , datum in zip ( ranks , data ) if rank < n ]","title":"Utils"},{"location":"reference/optimizers/staged/utils/#blackboxopt.optimizers.staged.utils","text":"","title":"utils"},{"location":"reference/optimizers/staged/utils/#blackboxopt.optimizers.staged.utils.best_evaluation_at_highest_fidelity","text":"From given list of evaluations, get the best in terms of minimal loss at the highest fidelity. Parameters: Name Type Description Default evaluations List[blackboxopt.evaluation.Evaluation] [description] required objective Objective [description] required Returns: Type Description Optional[blackboxopt.evaluation.Evaluation] [description] Source code in blackboxopt/optimizers/staged/utils.py def best_evaluation_at_highest_fidelity ( evaluations : List [ Evaluation ], objective : Objective , ) -> Optional [ Evaluation ]: \"\"\"From given list of evaluations, get the best in terms of minimal loss at the highest fidelity. Args: evaluations: [description] objective: [description] Returns: [description] \"\"\" if not evaluations : return None successes = [ evaluation for evaluation in evaluations if evaluation . objectives [ objective . name ] is not None ] if not successes : return None successful_fidelities = [ evaluation . settings [ \"fidelity\" ] for evaluation in successes ] if not successful_fidelities : return None max_successful_fidelities = max ( successful_fidelities ) successful_max_fidelity_evaluations = [ evaluation for evaluation in successes if evaluation . settings [ \"fidelity\" ] == max_successful_fidelities ] if not successful_max_fidelity_evaluations : return None sort_function = max if objective . greater_is_better else min best_evaluation = sort_function ( successful_max_fidelity_evaluations , key = lambda e : e . objectives [ objective . name ] ) return best_evaluation","title":"best_evaluation_at_highest_fidelity()"},{"location":"reference/optimizers/staged/utils/#blackboxopt.optimizers.staged.utils.greedy_promotion","text":"Promotes the best configurations to the next stage solely relying on the current loss. Parameters: Name Type Description Default data List[blackboxopt.optimizers.staged.iteration.Datum] List with all successful evaluations for this stage. All failed configurations have already been removed. required num_configs int Maximum number of configurations to be promoted. required Returns: Type Description list List of the config_keys to be evaluated on the next higher fidelity. These must only include config_keys found in data und must also be of at most length num_configs . If fewer ids are returned, the remaining configurations for the next stage will be sampled using the config_sample_function of the staged_iteration. Source code in blackboxopt/optimizers/staged/utils.py def greedy_promotion ( data : List [ Datum ], num_configs : int ) -> list : \"\"\"Promotes the best configurations to the next stage solely relying on the current loss. Args: data: List with all successful evaluations for this stage. All failed configurations have already been removed. num_configs: Maximum number of configurations to be promoted. Returns: List of the config_keys to be evaluated on the next higher fidelity. These must only include config_keys found in `data` und must also be of at most length `num_configs`. If fewer ids are returned, the remaining configurations for the next stage will be sampled using the `config_sample_function` of the staged_iteration. \"\"\" losses = [ d . loss for d in data ] ranks = np . argsort ( np . argsort ( losses )) n = min ( num_configs , len ( data )) return [ datum . config_key for rank , datum in zip ( ranks , data ) if rank < n ]","title":"greedy_promotion()"},{"location":"reference/visualizations/utils/","text":"blackboxopt.visualizations.utils get_incumbent_objective_over_time_single_fidelity ( objective , objective_values , times , fidelities , target_fidelity ) Filter for results with given target fidelity and generate incumbent trace. Source code in blackboxopt/visualizations/utils.py def get_incumbent_objective_over_time_single_fidelity ( objective : Objective , objective_values : np . ndarray , times : np . ndarray , fidelities : np . ndarray , target_fidelity : float , ): \"\"\"Filter for results with given target fidelity and generate incumbent trace.\"\"\" # filter out fidelity and take min/max of objective_values idx = np . logical_and ( fidelities == target_fidelity , np . isfinite ( objective_values )) _times = times [ idx ] if objective . greater_is_better : _objective_values = np . maximum . accumulate ( objective_values [ idx ]) else : _objective_values = np . minimum . accumulate ( objective_values [ idx ]) # get unique objective values and sort their indices (to be in chronological order) _ , idx = np . unique ( _objective_values , return_index = True ) idx . sort () # find objective_values _objective_values = _objective_values [ idx ] _times = _times [ idx ] # add steps where a new incumbent was found _times = np . repeat ( _times , 2 )[ 1 :] _objective_values = np . repeat ( _objective_values , 2 )[: - 1 ] # append best value for largest time to extend the lines _times = np . concatenate ([ _times , np . nanmax ( times , keepdims = True )]) _objective_values = np . concatenate ([ _objective_values , _objective_values [ - 1 :]]) return _times , _objective_values mask_pareto_efficient ( costs ) For a given array of objective values where lower values are considered better and the dimensions are samples x objectives, return a mask that is True for all pareto efficient values. NOTE: The result marks multiple occurrences of the same point all as pareto efficient. Source code in blackboxopt/visualizations/utils.py def mask_pareto_efficient ( costs : np . ndarray ): \"\"\"For a given array of objective values where lower values are considered better and the dimensions are samples x objectives, return a mask that is `True` for all pareto efficient values. NOTE: The result marks multiple occurrences of the same point all as pareto efficient. \"\"\" is_efficient = np . ones ( costs . shape [ 0 ], dtype = bool ) for i , c in enumerate ( costs ): if not is_efficient [ i ]: continue # Keep any point with a lower cost or when they are the same efficient = np . any ( costs [ is_efficient ] < c , axis = 1 ) duplicates = np . all ( costs [ is_efficient ] == c , axis = 1 ) is_efficient [ is_efficient ] = np . logical_or ( efficient , duplicates ) return is_efficient patch_plotly_io_to_html ( method ) Patch plotly.io.to_html with additional javascript to improve usability. Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets fixed. Injects <script> -tag with content from to_html_patch.js at the end of the HTML output. But only, if the chart title starts with \"[BBO]\" (to minimize side effects, if the user uses plotly.io for something else). plotly.io.to_html is also internally used for figure.show() and figure.to_html() , so this is covered, too. Parameters: Name Type Description Default method Original plotly.io.to_html method. required Returns: Type Description Patched method. Source code in blackboxopt/visualizations/utils.py def patch_plotly_io_to_html ( method ): \"\"\"Patch `plotly.io.to_html` with additional javascript to improve usability. Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets fixed. Injects `<script>`-tag with content from `to_html_patch.js` at the end of the HTML output. But only, if the chart title starts with \"[BBO]\" (to minimize side effects, if the user uses `plotly.io` for something else). `plotly.io.to_html` is also internally used for `figure.show()` and `figure.to_html()`, so this is covered, too. Args: method: Original `plotly.io.to_html` method. Returns: Patched method. \"\"\" @wraps ( method ) def wrapped ( * args , ** kwargs ): html = method ( * args , ** kwargs ) # Test if title text contains \"[BBO]\" if html . find ( '\"title\": {\"text\": \"[BBO]' ) < 0 : return html js = importlib . resources . read_text ( blackboxopt . visualizations , \"to_html_patch.js\" ) html_to_inject = f \"<script> { js } </script>\" insert_idx = html . rfind ( \"</body>\" ) if insert_idx >= 0 : # Full html page got rendered, inject <script> before <\\body> html = html [: insert_idx ] + html_to_inject + html [ insert_idx :] else : # Only chart part got rendered: append <script> at the end html = html + html_to_inject return html return wrapped","title":"Utils"},{"location":"reference/visualizations/utils/#blackboxopt.visualizations.utils","text":"","title":"utils"},{"location":"reference/visualizations/utils/#blackboxopt.visualizations.utils.get_incumbent_objective_over_time_single_fidelity","text":"Filter for results with given target fidelity and generate incumbent trace. Source code in blackboxopt/visualizations/utils.py def get_incumbent_objective_over_time_single_fidelity ( objective : Objective , objective_values : np . ndarray , times : np . ndarray , fidelities : np . ndarray , target_fidelity : float , ): \"\"\"Filter for results with given target fidelity and generate incumbent trace.\"\"\" # filter out fidelity and take min/max of objective_values idx = np . logical_and ( fidelities == target_fidelity , np . isfinite ( objective_values )) _times = times [ idx ] if objective . greater_is_better : _objective_values = np . maximum . accumulate ( objective_values [ idx ]) else : _objective_values = np . minimum . accumulate ( objective_values [ idx ]) # get unique objective values and sort their indices (to be in chronological order) _ , idx = np . unique ( _objective_values , return_index = True ) idx . sort () # find objective_values _objective_values = _objective_values [ idx ] _times = _times [ idx ] # add steps where a new incumbent was found _times = np . repeat ( _times , 2 )[ 1 :] _objective_values = np . repeat ( _objective_values , 2 )[: - 1 ] # append best value for largest time to extend the lines _times = np . concatenate ([ _times , np . nanmax ( times , keepdims = True )]) _objective_values = np . concatenate ([ _objective_values , _objective_values [ - 1 :]]) return _times , _objective_values","title":"get_incumbent_objective_over_time_single_fidelity()"},{"location":"reference/visualizations/utils/#blackboxopt.visualizations.utils.mask_pareto_efficient","text":"For a given array of objective values where lower values are considered better and the dimensions are samples x objectives, return a mask that is True for all pareto efficient values. NOTE: The result marks multiple occurrences of the same point all as pareto efficient. Source code in blackboxopt/visualizations/utils.py def mask_pareto_efficient ( costs : np . ndarray ): \"\"\"For a given array of objective values where lower values are considered better and the dimensions are samples x objectives, return a mask that is `True` for all pareto efficient values. NOTE: The result marks multiple occurrences of the same point all as pareto efficient. \"\"\" is_efficient = np . ones ( costs . shape [ 0 ], dtype = bool ) for i , c in enumerate ( costs ): if not is_efficient [ i ]: continue # Keep any point with a lower cost or when they are the same efficient = np . any ( costs [ is_efficient ] < c , axis = 1 ) duplicates = np . all ( costs [ is_efficient ] == c , axis = 1 ) is_efficient [ is_efficient ] = np . logical_or ( efficient , duplicates ) return is_efficient","title":"mask_pareto_efficient()"},{"location":"reference/visualizations/utils/#blackboxopt.visualizations.utils.patch_plotly_io_to_html","text":"Patch plotly.io.to_html with additional javascript to improve usability. Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets fixed. Injects <script> -tag with content from to_html_patch.js at the end of the HTML output. But only, if the chart title starts with \"[BBO]\" (to minimize side effects, if the user uses plotly.io for something else). plotly.io.to_html is also internally used for figure.show() and figure.to_html() , so this is covered, too. Parameters: Name Type Description Default method Original plotly.io.to_html method. required Returns: Type Description Patched method. Source code in blackboxopt/visualizations/utils.py def patch_plotly_io_to_html ( method ): \"\"\"Patch `plotly.io.to_html` with additional javascript to improve usability. Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets fixed. Injects `<script>`-tag with content from `to_html_patch.js` at the end of the HTML output. But only, if the chart title starts with \"[BBO]\" (to minimize side effects, if the user uses `plotly.io` for something else). `plotly.io.to_html` is also internally used for `figure.show()` and `figure.to_html()`, so this is covered, too. Args: method: Original `plotly.io.to_html` method. Returns: Patched method. \"\"\" @wraps ( method ) def wrapped ( * args , ** kwargs ): html = method ( * args , ** kwargs ) # Test if title text contains \"[BBO]\" if html . find ( '\"title\": {\"text\": \"[BBO]' ) < 0 : return html js = importlib . resources . read_text ( blackboxopt . visualizations , \"to_html_patch.js\" ) html_to_inject = f \"<script> { js } </script>\" insert_idx = html . rfind ( \"</body>\" ) if insert_idx >= 0 : # Full html page got rendered, inject <script> before <\\body> html = html [: insert_idx ] + html_to_inject + html [ insert_idx :] else : # Only chart part got rendered: append <script> at the end html = html + html_to_inject return html return wrapped","title":"patch_plotly_io_to_html()"},{"location":"reference/visualizations/visualizer/","text":"blackboxopt.visualizations.visualizer create_hover_information ( sections ) Create a hovertemplate which is used to render hover hints in plotly charts. The data for the chart hovertext has to be provided as custom_data attribute to the chart and can be e.g. a list of column names. One oddness is, that in the template the columns can't be referenced by name, but only by index. That's why it is important to have the same ordering in the template as in the custom_data and the reason why this is done together in one function. Parameters: Name Type Description Default sections dict Sections to render. The kyeys will show up as the section titles, values are expected to be a list of column names to be rendered under the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] } required Returns: Type Description Tuple[str, List] (plotly hover template, data column names) Source code in blackboxopt/visualizations/visualizer.py def create_hover_information ( sections : dict ) -> Tuple [ str , List ]: \"\"\" Create a [hovertemplate](https://plotly.com/python/reference/pie/#pie-hovertemplate) which is used to render hover hints in plotly charts. The data for the chart hovertext has to be provided as `custom_data` attribute to the chart and can be e.g. a list of column names. One oddness is, that in the template the columns can't be referenced by name, but only by index. That's why it is important to have the same ordering in the template as in the `custom_data` and the reason why this is done together in one function. Args: sections: Sections to render. The kyeys will show up as the section titles, values are expected to be a list of column names to be rendered under the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] } Returns: (plotly hover template, data column names) \"\"\" template = \"\" idx = 0 for section , columns in sections . items (): template += f \"<br><b> { section . replace ( '_' , ' ' ) . title () } </b><br>\" for column in columns : template += f \" { column } : % {{ customdata[ { idx } ] }} <br>\" idx += 1 template += \"<extra></extra>\" data_columns : list = sum ( sections . values (), []) return template , data_columns evaluations_to_df ( evaluations ) Convert evaluations into multi index dataframe. The evaluations will be casted to dictionaries which will be normalized. The keys of the dicts will be used as secondary column index. Evaluations with one or more missing objective-value will be dropped. Examples: Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...) Will be transformed into: | objectives | stacktrace | ... | <- \"group\" index | loss_1 | loss_2 | stacktrace | ... | <- \"field\" index | ------ | ------ | ---------- | --- | | 1.0 | -0.0 | None | ... | Source code in blackboxopt/visualizations/visualizer.py def evaluations_to_df ( evaluations : List [ Evaluation ]) -> pd . DataFrame : \"\"\"Convert evaluations into multi index dataframe. The evaluations will be casted to dictionaries which will be normalized. The keys of the dicts will be used as secondary column index. Evaluations with one or more missing objective-value will be dropped. Example: ``` Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...) ``` Will be transformed into: | objectives | stacktrace | ... | <- \"group\" index | loss_1 | loss_2 | stacktrace | ... | <- \"field\" index | ------ | ------ | ---------- | --- | | 1.0 | -0.0 | None | ... | \"\"\" if not evaluations or len ( evaluations ) == 0 : raise NoSuccessfulEvaluationsError # Filter out e.g. EvaluationSpecifications which might be passed into evaluations = [ e for e in evaluations if isinstance ( e , Evaluation )] # Transform to dicts, filter out evaluations with missing objectives evaluation_dicts = [ e . __dict__ for e in evaluations if not e . any_objective_none ] if len ( evaluation_dicts ) == 0 : raise NoSuccessfulEvaluationsError df = pd . DataFrame ( evaluation_dicts ) # Flatten json/dict columns into single multi-index dataframe dfs_expanded = [] for column in df . columns : # Normalize json columns keep original column for non-json columns try : df_temp = pd . json_normalize ( df [ column ], errors = \"ignore\" , max_level = 0 ) except AttributeError : df_temp = df [[ column ]] # Use keys of dicts as second level of column index df_temp . columns = pd . MultiIndex . from_product ( [[ column ], df_temp . columns ], names = [ \"group\" , \"field\" ] ) # Drop empty columns df_temp = df_temp . dropna ( axis = 1 , how = \"all\" ) dfs_expanded . append ( df_temp ) df = pd . concat ( dfs_expanded , join = \"outer\" , axis = 1 ) # Parse datetime columns date_columns = [ c for c in df . columns if \"unixtime\" in str ( c )] df [ date_columns ] = df [ date_columns ] . apply ( pd . to_datetime , unit = \"s\" ) # Calculate duration in seconds df [ \"duration\" , \"duration\" ] = ( df [ \"finished_unixtime\" , \"finished_unixtime\" ] - df [ \"created_unixtime\" , \"created_unixtime\" ] ) return df","title":"Visualizer"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer","text":"","title":"visualizer"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.create_hover_information","text":"Create a hovertemplate which is used to render hover hints in plotly charts. The data for the chart hovertext has to be provided as custom_data attribute to the chart and can be e.g. a list of column names. One oddness is, that in the template the columns can't be referenced by name, but only by index. That's why it is important to have the same ordering in the template as in the custom_data and the reason why this is done together in one function. Parameters: Name Type Description Default sections dict Sections to render. The kyeys will show up as the section titles, values are expected to be a list of column names to be rendered under the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] } required Returns: Type Description Tuple[str, List] (plotly hover template, data column names) Source code in blackboxopt/visualizations/visualizer.py def create_hover_information ( sections : dict ) -> Tuple [ str , List ]: \"\"\" Create a [hovertemplate](https://plotly.com/python/reference/pie/#pie-hovertemplate) which is used to render hover hints in plotly charts. The data for the chart hovertext has to be provided as `custom_data` attribute to the chart and can be e.g. a list of column names. One oddness is, that in the template the columns can't be referenced by name, but only by index. That's why it is important to have the same ordering in the template as in the `custom_data` and the reason why this is done together in one function. Args: sections: Sections to render. The kyeys will show up as the section titles, values are expected to be a list of column names to be rendered under the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] } Returns: (plotly hover template, data column names) \"\"\" template = \"\" idx = 0 for section , columns in sections . items (): template += f \"<br><b> { section . replace ( '_' , ' ' ) . title () } </b><br>\" for column in columns : template += f \" { column } : % {{ customdata[ { idx } ] }} <br>\" idx += 1 template += \"<extra></extra>\" data_columns : list = sum ( sections . values (), []) return template , data_columns","title":"create_hover_information()"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.evaluations_to_df","text":"Convert evaluations into multi index dataframe. The evaluations will be casted to dictionaries which will be normalized. The keys of the dicts will be used as secondary column index. Evaluations with one or more missing objective-value will be dropped. Examples: Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...) Will be transformed into: | objectives | stacktrace | ... | <- \"group\" index | loss_1 | loss_2 | stacktrace | ... | <- \"field\" index | ------ | ------ | ---------- | --- | | 1.0 | -0.0 | None | ... | Source code in blackboxopt/visualizations/visualizer.py def evaluations_to_df ( evaluations : List [ Evaluation ]) -> pd . DataFrame : \"\"\"Convert evaluations into multi index dataframe. The evaluations will be casted to dictionaries which will be normalized. The keys of the dicts will be used as secondary column index. Evaluations with one or more missing objective-value will be dropped. Example: ``` Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...) ``` Will be transformed into: | objectives | stacktrace | ... | <- \"group\" index | loss_1 | loss_2 | stacktrace | ... | <- \"field\" index | ------ | ------ | ---------- | --- | | 1.0 | -0.0 | None | ... | \"\"\" if not evaluations or len ( evaluations ) == 0 : raise NoSuccessfulEvaluationsError # Filter out e.g. EvaluationSpecifications which might be passed into evaluations = [ e for e in evaluations if isinstance ( e , Evaluation )] # Transform to dicts, filter out evaluations with missing objectives evaluation_dicts = [ e . __dict__ for e in evaluations if not e . any_objective_none ] if len ( evaluation_dicts ) == 0 : raise NoSuccessfulEvaluationsError df = pd . DataFrame ( evaluation_dicts ) # Flatten json/dict columns into single multi-index dataframe dfs_expanded = [] for column in df . columns : # Normalize json columns keep original column for non-json columns try : df_temp = pd . json_normalize ( df [ column ], errors = \"ignore\" , max_level = 0 ) except AttributeError : df_temp = df [[ column ]] # Use keys of dicts as second level of column index df_temp . columns = pd . MultiIndex . from_product ( [[ column ], df_temp . columns ], names = [ \"group\" , \"field\" ] ) # Drop empty columns df_temp = df_temp . dropna ( axis = 1 , how = \"all\" ) dfs_expanded . append ( df_temp ) df = pd . concat ( dfs_expanded , join = \"outer\" , axis = 1 ) # Parse datetime columns date_columns = [ c for c in df . columns if \"unixtime\" in str ( c )] df [ date_columns ] = df [ date_columns ] . apply ( pd . to_datetime , unit = \"s\" ) # Calculate duration in seconds df [ \"duration\" , \"duration\" ] = ( df [ \"finished_unixtime\" , \"finished_unixtime\" ] - df [ \"created_unixtime\" , \"created_unixtime\" ] ) return df","title":"evaluations_to_df()"}]}