{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Blackbox Optimization","text":"<p>The <code>blackboxopt</code> Python package contains blackbox optimization algorithms with a common interface, along with useful helpers like parallel optimization loops, analysis and visualization tools.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#common-interface","title":"Common Interface","text":"<p>The <code>blackboxopt</code> base classes along with the <code>EvaluationSpecification</code> and <code>Evaluation</code> data classes specify a unified interface for different blackbox optimization method implementations. In addition to these interfaces, a standard pytest compatible testsuite is available to ensure functional compatibility of an optimizer implementation with the <code>blackboxopt</code> framework.</p>"},{"location":"#optimizers","title":"Optimizers","text":"<p>Aside from random search and a Sobol sequence based space filling method, the main ones in this package are Hyperband, BOHB and a BoTorch based Bayesian optimization base implementation. BOHB is provided as a cleaner replacement of the former implementation in HpBandSter.</p>"},{"location":"#optimization-loops","title":"Optimization Loops","text":"<p>As part of the <code>blackboxopt.optimization_loops</code> module compatible implementations for optimization loops are avilable bot for local, serial execution as well as for distributed optimization via <code>dask.distributed</code>.</p>"},{"location":"#visualizations","title":"Visualizations","text":"<p>Interactive visualizations like objective value over time or duration for single objective optimization, as well as an objectives pair plot with a highlighted pareto front for multi objective optimization is available as part of the <code>blackboxopt.visualizations</code> module.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>The following example outlines how a quadratic function can be optimized with random search in a distributed manner.</p> <pre><code># Copyright (c) 2020 - for information on the respective copyright owner\n# see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt\n#\n# SPDX-License-Identifier: Apache-2.0\n\nimport parameterspace as ps\n\ntry:\n    import dask.distributed as dd\nexcept ImportError:\n    raise ImportError(\n        \"Unable to import Dask Distributed specific dependencies. \"\n        + \"Make sure to install blackboxopt[dask]\"\n    )\n\nfrom blackboxopt import Evaluation, EvaluationSpecification, Objective\nfrom blackboxopt.optimization_loops.dask_distributed import (\n    run_optimization_loop,\n)\nfrom blackboxopt.optimizers.random_search import RandomSearch\n\n\ndef evaluation_function(eval_spec: EvaluationSpecification) -&gt; Evaluation:\n    return eval_spec.create_evaluation(\n        objectives={\"loss\": eval_spec.configuration[\"p1\"] ** 2},\n        user_info={\"weather\": \"sunny\"},\n    )\n\n\nif __name__ == \"__main__\":\n    space = ps.ParameterSpace()\n    space.add(ps.ContinuousParameter(\"p1\", (-1.0, 1.0)))\n    optimizer = RandomSearch(\n        space,\n        [Objective(\"loss\", greater_is_better=False)],\n        max_steps=1000,\n    )\n\n    evaluations = run_optimization_loop(\n        optimizer, evaluation_function, dd.Client(), max_evaluations=100\n    )\n\n    n_successes = len([e for e in evaluations if not e.all_objectives_none])\n    print(f\"Successfully evaluated {n_successes}/{len(evaluations)}\")\n</code></pre>"},{"location":"#license","title":"License","text":"<p><code>blackboxopt</code> is open-sourced under the Apache-2.0 license. See the LICENSE file for details.</p> <p>For a list of other open source components included in <code>blackboxopt</code>, see the file 3rd-party-licenses.txt.</p>"},{"location":"examples/dask-distributed/","title":"Dask Distributed","text":"<pre><code># Copyright (c) 2020 - for information on the respective copyright owner\n# see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt\n#\n# SPDX-License-Identifier: Apache-2.0\n\nimport parameterspace as ps\n\ntry:\n    import dask.distributed as dd\nexcept ImportError:\n    raise ImportError(\n        \"Unable to import Dask Distributed specific dependencies. \"\n        + \"Make sure to install blackboxopt[dask]\"\n    )\n\nfrom blackboxopt import Evaluation, EvaluationSpecification, Objective\nfrom blackboxopt.optimization_loops.dask_distributed import (\n    run_optimization_loop,\n)\nfrom blackboxopt.optimizers.random_search import RandomSearch\n\n\ndef evaluation_function(eval_spec: EvaluationSpecification) -&gt; Evaluation:\n    return eval_spec.create_evaluation(\n        objectives={\"loss\": eval_spec.configuration[\"p1\"] ** 2},\n        user_info={\"weather\": \"sunny\"},\n    )\n\n\nif __name__ == \"__main__\":\n    space = ps.ParameterSpace()\n    space.add(ps.ContinuousParameter(\"p1\", (-1.0, 1.0)))\n    optimizer = RandomSearch(\n        space,\n        [Objective(\"loss\", greater_is_better=False)],\n        max_steps=1000,\n    )\n\n    evaluations = run_optimization_loop(\n        optimizer, evaluation_function, dd.Client(), max_evaluations=100\n    )\n\n    n_successes = len([e for e in evaluations if not e.all_objectives_none])\n    print(f\"Successfully evaluated {n_successes}/{len(evaluations)}\")\n</code></pre>"},{"location":"examples/multi-objective-multi-param/","title":"Mixed Space &amp; Multi Objective","text":"<p>Aside from continuous parameters, different types are supported by <code>parameterspace</code>. In the following example we use continuous, integer and categorical parameters. Also, we are having a glance at optimizing multiple objectives at the same time.</p> <pre><code># Copyright (c) 2020 - for information on the respective copyright owner\n# see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt\n#\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport time\n\nimport numpy as np\nimport parameterspace as ps\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nimport blackboxopt as bbo\nfrom blackboxopt.optimization_loops.sequential import run_optimization_loop\nfrom blackboxopt.optimizers.space_filling import SpaceFilling\n\n# Load a sklearn sample dataset\nX_TRAIN, X_VALIDATE, Y_TRAIN, Y_VALIDATE = train_test_split(\n    *load_diabetes(return_X_y=True)\n)\n\n# Set up search space with multiple ML model hyperparameters of different types\nSPACE = ps.ParameterSpace()\nSPACE.add(ps.IntegerParameter(\"n_estimators\", bounds=(256, 2048), transformation=\"log\"))\nSPACE.add(ps.IntegerParameter(\"min_samples_leaf\", bounds=(1, 32), transformation=\"log\"))\nSPACE.add(ps.ContinuousParameter(\"max_samples\", bounds=(0.1, 1)))\nSPACE.add(ps.ContinuousParameter(\"max_features\", bounds=(0.1, 1)))\nSPACE.add(ps.IntegerParameter(\"max_depth\", bounds=(1, 128)))\nSPACE.add(ps.CategoricalParameter(\"criterion\", values=(\"squared_error\", \"poisson\")))\n\n\ndef evaluation_function(\n    eval_spec: bbo.EvaluationSpecification,\n) -&gt; bbo.Evaluation:\n    \"\"\"Train and evaluate a random forest with given parameter configuration.\"\"\"\n    regr = RandomForestRegressor(\n        n_estimators=eval_spec.configuration[\"n_estimators\"],\n        max_samples=eval_spec.configuration[\"max_samples\"],\n        max_features=eval_spec.configuration[\"max_features\"],\n        max_depth=eval_spec.configuration[\"max_depth\"],\n        min_samples_leaf=eval_spec.configuration[\"min_samples_leaf\"],\n        criterion=eval_spec.configuration[\"criterion\"],\n    )\n\n    start = time.time()\n    regr.fit(X_TRAIN, Y_TRAIN)\n    fit_duration = time.time() - start\n\n    y_pred = regr.predict(X_VALIDATE)\n    objectives = {\n        \"R\u00b2\": r2_score(Y_VALIDATE, y_pred),\n        \"Fit Duration\": fit_duration,\n        \"Max Error\": np.abs(Y_VALIDATE - y_pred).max(),\n    }\n    evaluation = eval_spec.create_evaluation(objectives=objectives)\n    return evaluation\n\n\ndef main():\n    logger = bbo.init_logger(logging.INFO)\n\n    # Create an optimization run based on a parameterspace and optimizer choice\n    optimizer = SpaceFilling(\n        search_space=SPACE,\n        objectives=[\n            bbo.Objective(\"R\u00b2\", greater_is_better=True),\n            bbo.Objective(\"Max Error\", greater_is_better=False),\n            bbo.Objective(\"Fit Duration\", greater_is_better=False),\n        ],\n    )\n\n    # Fetch new configurations to evaluate until the optimization is done or\n    # a given timeout is reached\n    evaluations = run_optimization_loop(\n        optimizer=optimizer,\n        evaluation_function=evaluation_function,\n        timeout_s=60.0,\n    )\n\n    logger.info(f\"Evaluated {len(evaluations)} specifications\")\n\n    pareto_front = bbo.utils.filter_pareto_efficient(evaluations, optimizer.objectives)\n    logger.info(f\"{len(pareto_front)} evaluation(s) are pareto efficient\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/overview/","title":"Examples - Overview","text":"<p>To get all dependencies for the examples, run:</p> <pre><code>pip install blackboxopt[examples]\n</code></pre>"},{"location":"examples/overview/#list-of-available-examples","title":"List of available examples","text":"<ul> <li>Dask Distributed</li> <li>Multi Objective Optimization</li> </ul>"},{"location":"optimization-loops/dask-distributed/","title":"Dask Distributed Optimization Loop","text":"<p>In case you are working with dask, this optimization loop can help you run <code>blackboxopt</code> based optimization leveraging your dask cluster. See also the corresponding example for more details.</p>"},{"location":"optimization-loops/dask-distributed/#blackboxopt.optimization_loops.dask_distributed.run_optimization_loop","title":"<code>run_optimization_loop(optimizer, evaluation_function, dask_client, timeout_s=inf, max_evaluations=None, pre_evaluation_callback=None, post_evaluation_callback=None, logger=None)</code>","text":"<p>Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached.</p> <p>This already handles signals from the optimizer in case there is no evaluation specification available yet.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer]</code> <p>The blackboxopt optimizer to run.</p> required <code>dask_client</code> <code>Client</code> <p>A Dask Distributed client that is configured with workers.</p> required <code>evaluation_function</code> <code>Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation]</code> <p>The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a <code>blackboxopt.Evaluation</code> as a result.</p> required <code>timeout_s</code> <code>float</code> <p>If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf.</p> <code>inf</code> <code>max_evaluations</code> <code>Optional[int]</code> <p>If given, the optimization loop will terminate after the given number of steps. Defaults to None.</p> <code>None</code> <code>pre_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.EvaluationSpecification], Any]]</code> <p>Reference to a callable that is invoked before each evaluation and takes a <code>blackboxopt.EvaluationSpecification</code> as an argument.</p> <code>None</code> <code>post_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.Evaluation], Any]]</code> <p>Reference to a callable that is invoked after each evaluation and takes a <code>blackboxopt.Evaluation</code> as an argument.</p> <code>None</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>The logger to use for logging progress. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[blackboxopt.evaluation.Evaluation]</code> <p>List of evluation specification and result for all evaluations.</p> Source code in <code>blackboxopt/optimization_loops/dask_distributed.py</code> <pre><code>def run_optimization_loop(\n    optimizer: Union[SingleObjectiveOptimizer, MultiObjectiveOptimizer],\n    evaluation_function: Callable[[EvaluationSpecification], Evaluation],\n    dask_client: dd.Client,\n    timeout_s: float = float(\"inf\"),\n    max_evaluations: Optional[int] = None,\n    pre_evaluation_callback: Optional[Callable[[EvaluationSpecification], Any]] = None,\n    post_evaluation_callback: Optional[Callable[[Evaluation], Any]] = None,\n    logger: Optional[logging.Logger] = None,\n) -&gt; List[Evaluation]:\n    \"\"\"Convenience wrapper for an optimization loop that uses Dask to parallelize\n    optimization until a given timeout or maximum number of evaluations is reached.\n\n    This already handles signals from the optimizer in case there is no evaluation\n    specification available yet.\n\n    Args:\n        optimizer: The blackboxopt optimizer to run.\n        dask_client: A Dask Distributed client that is configured with workers.\n        evaluation_function: The function that is called with configuration, settings\n            and optimizer info dictionaries as arguments like provided by an evaluation\n            specification.\n            This is the function that encapsulates the actual execution of\n            a parametrized experiment (e.g. ML model training) and should return a\n            `blackboxopt.Evaluation` as a result.\n        timeout_s: If given, the optimization loop will terminate after the first\n            optimization step that exceeded the timeout (in seconds). Defaults to inf.\n        max_evaluations: If given, the optimization loop will terminate after the given\n            number of steps. Defaults to None.\n        pre_evaluation_callback: Reference to a callable that is invoked before each\n            evaluation and takes a `blackboxopt.EvaluationSpecification` as an argument.\n        post_evaluation_callback: Reference to a callable that is invoked after each\n            evaluation and takes a `blackboxopt.Evaluation` as an argument.\n        logger: The logger to use for logging progress. Defaults to None.\n\n    Returns:\n        List of evluation specification and result for all evaluations.\n    \"\"\"\n    logger = logging.getLogger(\"blackboxopt\") if logger is None else logger\n\n    objectives = (\n        optimizer.objectives\n        if isinstance(optimizer, MultiObjectiveOptimizer)\n        else [optimizer.objective]\n    )\n    evaluations: List[Evaluation] = []\n\n    dask_scheduler = MinimalDaskScheduler(\n        dask_client=dask_client, objectives=objectives, logger=logger\n    )\n\n    _max_evaluations = init_max_evaluations_with_limit_logging(\n        max_evaluations=max_evaluations, timeout_s=timeout_s, logger=logger\n    )\n\n    n_eval_specs = 0\n    start = time.time()\n    while time.time() - start &lt; timeout_s and n_eval_specs &lt; _max_evaluations:\n        if dask_scheduler.has_capacity():\n            try:\n                eval_spec = optimizer.generate_evaluation_specification()\n\n                if pre_evaluation_callback:\n                    pre_evaluation_callback(eval_spec)\n\n                dask_scheduler.submit(evaluation_function, eval_spec)\n                n_eval_specs += 1\n                continue\n\n            except OptimizerNotReady:\n                logger.info(\"Optimizer is not ready yet; will retry after short pause.\")\n\n            except OptimizationComplete:\n                logger.info(\"Optimization is complete\")\n                break\n\n        new_evaluations = dask_scheduler.check_for_results(timeout_s=20)\n\n        if post_evaluation_callback:\n            list(map(post_evaluation_callback, new_evaluations))\n\n        optimizer.report(new_evaluations)\n        evaluations.extend(new_evaluations)\n\n    while dask_scheduler.has_running_jobs():\n        new_evaluations = dask_scheduler.check_for_results(timeout_s=20)\n        if post_evaluation_callback:\n            list(map(post_evaluation_callback, new_evaluations))\n        optimizer.report(new_evaluations)\n        evaluations.extend(new_evaluations)\n\n    return evaluations\n</code></pre>"},{"location":"optimization-loops/overview/","title":"Optimization Loops","text":"<p>We include a handful optimization loop implementations for different scenarios from a simple sequential loop to one distributed potentially across nodes in a cluster setup. Additionally, a set of reference tests are included for anyone extending the selection of optimization loops as part of a contribution to <code>blackboxopt</code> or in a separate project.</p>"},{"location":"optimization-loops/sequential/","title":"Sequential Optimization Loop","text":""},{"location":"optimization-loops/sequential/#blackboxopt.optimization_loops.sequential.run_optimization_loop","title":"<code>run_optimization_loop(optimizer, evaluation_function, timeout_s=inf, max_evaluations=None, catch_exceptions_from_evaluation_function=False, pre_evaluation_callback=None, post_evaluation_callback=None, logger=None)</code>","text":"<p>Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached.</p> <p>This already handles signals from the optimizer in case there is no evaluation specification available yet.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer]</code> <p>The blackboxopt optimizer to run.</p> required <code>evaluation_function</code> <code>Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation]</code> <p>The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a <code>blackboxopt.Evaluation</code> as a result.</p> required <code>timeout_s</code> <code>float</code> <p>If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf.</p> <code>inf</code> <code>max_evaluations</code> <code>Optional[int]</code> <p>If given, the optimization loop will terminate after the given number of steps.</p> <code>None</code> <code>catch_exceptions_from_evaluation_function</code> <code>bool</code> <p>Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's <code>stacktrace</code> attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. For more details, see the wrapper that is used internally <code>blackboxopt.optimization_loops.utils.evaluation_function_wrapper</code></p> <code>False</code> <code>pre_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.EvaluationSpecification], Any]]</code> <p>Reference to a callable that is invoked before each evaluation and takes a <code>blackboxopt.EvaluationSpecification</code> as an argument.</p> <code>None</code> <code>post_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.Evaluation], Any]]</code> <p>Reference to a callable that is invoked after each evaluation and takes a <code>blackboxopt.Evaluation</code> as an argument.</p> <code>None</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>The logger to use for logging progress. Default: <code>blackboxopt.logger</code></p> <code>None</code> <p>Returns:</p> Type Description <code>List[blackboxopt.evaluation.Evaluation]</code> <p>List of evaluation specification and result for all evaluations.</p> Source code in <code>blackboxopt/optimization_loops/sequential.py</code> <pre><code>def run_optimization_loop(\n    optimizer: Union[SingleObjectiveOptimizer, MultiObjectiveOptimizer],\n    evaluation_function: Callable[[EvaluationSpecification], Evaluation],\n    timeout_s: float = float(\"inf\"),\n    max_evaluations: Optional[int] = None,\n    catch_exceptions_from_evaluation_function: bool = False,\n    pre_evaluation_callback: Optional[Callable[[EvaluationSpecification], Any]] = None,\n    post_evaluation_callback: Optional[Callable[[Evaluation], Any]] = None,\n    logger: Optional[logging.Logger] = None,\n) -&gt; List[Evaluation]:\n    \"\"\"Convenience wrapper for an optimization loop that sequentially fetches evaluation\n    specifications until a given timeout or maximum number of evaluations is reached.\n\n    This already handles signals from the optimizer in case there is no evaluation\n    specification available yet.\n\n    Args:\n        optimizer: The blackboxopt optimizer to run.\n        evaluation_function: The function that is called with configuration, settings\n            and optimizer info dictionaries as arguments like provided by an evaluation\n            specification.\n            This is the function that encapsulates the actual execution of\n            a parametrized experiment (e.g. ML model training) and should return a\n            `blackboxopt.Evaluation` as a result.\n        timeout_s: If given, the optimization loop will terminate after the first\n            optimization step that exceeded the timeout (in seconds). Defaults to inf.\n        max_evaluations: If given, the optimization loop will terminate after the given\n            number of steps.\n        catch_exceptions_from_evaluation_function: Whether to exit on an unhandled\n            exception raised by the evaluation function or instead store their stack\n            trace in the evaluation's `stacktrace` attribute. Set to True if there are\n            spurious errors due to e.g. numerical instability that should not halt the\n            optimization loop. For more details, see the wrapper that is used internally\n            `blackboxopt.optimization_loops.utils.evaluation_function_wrapper`\n        pre_evaluation_callback: Reference to a callable that is invoked before each\n            evaluation and takes a `blackboxopt.EvaluationSpecification` as an argument.\n        post_evaluation_callback: Reference to a callable that is invoked after each\n            evaluation and takes a `blackboxopt.Evaluation` as an argument.\n        logger: The logger to use for logging progress. Default: `blackboxopt.logger`\n\n    Returns:\n        List of evaluation specification and result for all evaluations.\n    \"\"\"\n    if logger is None:\n        logger = default_logger\n\n    objectives = (\n        optimizer.objectives\n        if isinstance(optimizer, MultiObjectiveOptimizer)\n        else [optimizer.objective]\n    )\n    evaluations: List[Evaluation] = []\n\n    _max_evaluations = init_max_evaluations_with_limit_logging(\n        max_evaluations=max_evaluations, timeout_s=timeout_s, logger=logger\n    )\n\n    start = time.time()\n    num_evaluations = 0\n    while time.time() - start &lt; timeout_s and num_evaluations &lt; _max_evaluations:\n        num_evaluations += 1\n\n        try:\n            evaluation_specification = optimizer.generate_evaluation_specification()\n\n            logger.info(\n                \"Starting evaluation %s (of %s). \"\n                \"Overall runtime so far: %s (timeout at %s)\",\n                num_evaluations,\n                _max_evaluations,\n                timedelta(seconds=int(time.time() - start)),\n                timedelta(seconds=timeout_s) if timeout_s != float(\"inf\") else \"inf\",\n            )\n\n            logger.info(\n                \"The optimizer proposed the following evaluation specification:\\n%s\",\n                pprint.pformat(evaluation_specification.to_dict(), compact=True),\n            )\n            if pre_evaluation_callback:\n                pre_evaluation_callback(evaluation_specification)\n\n            evaluation = evaluation_function_wrapper(\n                evaluation_function=evaluation_function,\n                evaluation_specification=evaluation_specification,\n                logger=logger,\n                objectives=objectives,\n                catch_exceptions_from_evaluation_function=catch_exceptions_from_evaluation_function,\n            )\n\n            logger.info(\n                \"Reporting the following evaluation result to the optimizer:\\n%s\",\n                pprint.pformat(evaluation.to_dict(), compact=True),\n            )\n            if post_evaluation_callback:\n                post_evaluation_callback(evaluation)\n\n            optimizer.report(evaluation)\n            evaluations.append(evaluation)\n\n        except OptimizerNotReady:\n            logger.info(\"Optimizer is not ready yet, retrying in two seconds\")\n            time.sleep(2)\n            continue\n\n        except OptimizationComplete:\n            logger.info(\"Optimization is complete\")\n            return evaluations\n\n    logger.info(\"Aborting optimization due to specified maximum evaluations or timeout\")\n    return evaluations\n</code></pre>"},{"location":"optimization-loops/testing/","title":"Reference Tests","text":"<p>To test an optimization loop implementation across various reference scenarios, follow:</p> <pre><code>import pytest\nfrom blackboxopt.optimization_loops.testing import ALL_REFERENCE_TESTS\n\n@pytest.mark.parametrize(\"reference_test\", testing.ALL_REFERENCE_TESTS)\ndef test_all_reference_tests(reference_test):\n    reference_test(custom_optimization_loop, {\"opt_loop_specific_kwarg\": 123})\n</code></pre> <p>where you can include custom keyword arguments that are passed to the optimization loop calls in the reference tests.</p>"},{"location":"optimizers/bohb/","title":"BOHB Optimizer","text":"<p>BOHB performs robust and efficient hyperparameter optimization at scale by combining the speed of Hyperband searches with the guidance and guarantees of convergence of Bayesian Optimization. Instead of sampling new configurations at random, BOHB uses kernel density estimators to select promising candidates.</p> <p>This implementation is meant to supersede the initial release of HpBandSter.</p>"},{"location":"optimizers/bohb/#fidelities","title":"Fidelities","text":"<p>Here you can calculate the fidelity schedule resulting from BOHB's hyper-parameters:</p> min_fidelity max_fidelity eta Calculate <p></p>"},{"location":"optimizers/bohb/#reference","title":"Reference","text":""},{"location":"optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB","title":"<code> BOHB            (StagedIterationOptimizer)         </code>","text":"Source code in <code>blackboxopt/optimizers/bohb.py</code> <pre><code>class BOHB(StagedIterationOptimizer):\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objective: Objective,\n        min_fidelity: float,\n        max_fidelity: float,\n        num_iterations: int,\n        eta: float = 3.0,\n        top_n_percent: int = 15,\n        min_samples_in_model: Optional[int] = None,\n        num_samples: int = 64,\n        random_fraction: float = 1 / 3,\n        bandwidth_factor: float = 3.0,\n        min_bandwidth: float = 1e-3,\n        seed: Optional[int] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"BOHB Optimizer.\n\n        BOHB performs robust and efficient hyperparameter optimization\n        at scale by combining the speed of Hyperband searches with the\n        guidance and guarantees of convergence of Bayesian\n        Optimization. Instead of sampling new configurations at random,\n        BOHB uses kernel density estimators to select promising candidates.\n\n        For reference:\n        ```\n        @InProceedings{falkner-icml-18,\n            title =     {{BOHB}: Robust and Efficient Hyperparameter Optimization at\n                Scale},\n            author =    {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n            booktitle = {Proceedings of the 35th International Conference on Machine\n                Learning},\n            pages =     {1436--1445},\n            year =      {2018},\n        }\n        ```\n\n        Args:\n            search_space: [description]\n            objective: [description]\n            min_fidelity: The smallest fidelity value that is still meaningful.\n                Must be strictly greater than zero!\n            max_fidelity: The largest fidelity value used during the optimization.\n                Must not be smaller than `min_fidelity`.\n            num_iterations: The number of iterations that the optimizer will run.\n            eta: Scaling parameter to control the aggressiveness of Hyperband's racing.\n            top_n_percent: Determines the percentile of configurations that will be\n                used as training data for the kernel density estimator of the good\n                configuration, e.g if set to 10 the best 10% configurations will be\n                considered for training.\n            min_samples_in_model: Minimum number of datapoints needed to fit a model.\n            num_samples: Number of samples drawn to optimize EI via sampling.\n            random_fraction: Fraction of random configurations returned.\n            bandwidth_factor: Widens the bandwidth for contiuous parameters for\n                proposed points to optimize EI\n            min_bandwidth: to keep diversity, even when all (good) samples have the\n                same value for one of the parameters, a minimum bandwidth\n                (reasonable default: 1e-3) is used instead of zero.\n            seed: [description]\n            logger: [description]\n        \"\"\"\n        if min_samples_in_model is None:\n            min_samples_in_model = 3 * len(search_space)\n\n        self.min_fidelity = min_fidelity\n        self.max_fidelity = max_fidelity\n        self.eta = eta\n\n        self.config_sampler = BOHBSampler(\n            search_space=search_space,\n            objective=objective,\n            min_samples_in_model=min_samples_in_model,\n            top_n_percent=top_n_percent,\n            num_samples=num_samples,\n            random_fraction=random_fraction,\n            bandwidth_factor=bandwidth_factor,\n            min_bandwidth=min_bandwidth,\n            seed=seed,\n        )\n\n        super().__init__(\n            search_space=search_space,\n            objective=objective,\n            num_iterations=num_iterations,\n            seed=seed,\n            logger=logger,\n        )\n\n    def _create_new_iteration(self, iteration_index):\n        \"\"\"Optimizer specific way to create a new\n        `blackboxopt.optimizer.utils.staged_iteration.StagedIteration` object\n        \"\"\"\n        return create_hyperband_iteration(\n            iteration_index,\n            self.min_fidelity,\n            self.max_fidelity,\n            self.eta,\n            self.config_sampler,\n            self.objective,\n            self.logger,\n        )\n</code></pre>"},{"location":"optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB.__init__","title":"<code>__init__(self, search_space, objective, min_fidelity, max_fidelity, num_iterations, eta=3.0, top_n_percent=15, min_samples_in_model=None, num_samples=64, random_fraction=0.3333333333333333, bandwidth_factor=3.0, min_bandwidth=0.001, seed=None, logger=None)</code>  <code>special</code>","text":"<p>BOHB Optimizer.</p> <p>BOHB performs robust and efficient hyperparameter optimization at scale by combining the speed of Hyperband searches with the guidance and guarantees of convergence of Bayesian Optimization. Instead of sampling new configurations at random, BOHB uses kernel density estimators to select promising candidates.</p> <p>For reference: <pre><code>@InProceedings{falkner-icml-18,\n    title =     {{BOHB}: Robust and Efficient Hyperparameter Optimization at\n        Scale},\n    author =    {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n    booktitle = {Proceedings of the 35th International Conference on Machine\n        Learning},\n    pages =     {1436--1445},\n    year =      {2018},\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>[description]</p> required <code>objective</code> <code>Objective</code> <p>[description]</p> required <code>min_fidelity</code> <code>float</code> <p>The smallest fidelity value that is still meaningful. Must be strictly greater than zero!</p> required <code>max_fidelity</code> <code>float</code> <p>The largest fidelity value used during the optimization. Must not be smaller than <code>min_fidelity</code>.</p> required <code>num_iterations</code> <code>int</code> <p>The number of iterations that the optimizer will run.</p> required <code>eta</code> <code>float</code> <p>Scaling parameter to control the aggressiveness of Hyperband's racing.</p> <code>3.0</code> <code>top_n_percent</code> <code>int</code> <p>Determines the percentile of configurations that will be used as training data for the kernel density estimator of the good configuration, e.g if set to 10 the best 10% configurations will be considered for training.</p> <code>15</code> <code>min_samples_in_model</code> <code>Optional[int]</code> <p>Minimum number of datapoints needed to fit a model.</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>Number of samples drawn to optimize EI via sampling.</p> <code>64</code> <code>random_fraction</code> <code>float</code> <p>Fraction of random configurations returned.</p> <code>0.3333333333333333</code> <code>bandwidth_factor</code> <code>float</code> <p>Widens the bandwidth for contiuous parameters for proposed points to optimize EI</p> <code>3.0</code> <code>min_bandwidth</code> <code>float</code> <p>to keep diversity, even when all (good) samples have the same value for one of the parameters, a minimum bandwidth (reasonable default: 1e-3) is used instead of zero.</p> <code>0.001</code> <code>seed</code> <code>Optional[int]</code> <p>[description]</p> <code>None</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>[description]</p> <code>None</code> Source code in <code>blackboxopt/optimizers/bohb.py</code> <pre><code>def __init__(\n    self,\n    search_space: ParameterSpace,\n    objective: Objective,\n    min_fidelity: float,\n    max_fidelity: float,\n    num_iterations: int,\n    eta: float = 3.0,\n    top_n_percent: int = 15,\n    min_samples_in_model: Optional[int] = None,\n    num_samples: int = 64,\n    random_fraction: float = 1 / 3,\n    bandwidth_factor: float = 3.0,\n    min_bandwidth: float = 1e-3,\n    seed: Optional[int] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"BOHB Optimizer.\n\n    BOHB performs robust and efficient hyperparameter optimization\n    at scale by combining the speed of Hyperband searches with the\n    guidance and guarantees of convergence of Bayesian\n    Optimization. Instead of sampling new configurations at random,\n    BOHB uses kernel density estimators to select promising candidates.\n\n    For reference:\n    ```\n    @InProceedings{falkner-icml-18,\n        title =     {{BOHB}: Robust and Efficient Hyperparameter Optimization at\n            Scale},\n        author =    {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n        booktitle = {Proceedings of the 35th International Conference on Machine\n            Learning},\n        pages =     {1436--1445},\n        year =      {2018},\n    }\n    ```\n\n    Args:\n        search_space: [description]\n        objective: [description]\n        min_fidelity: The smallest fidelity value that is still meaningful.\n            Must be strictly greater than zero!\n        max_fidelity: The largest fidelity value used during the optimization.\n            Must not be smaller than `min_fidelity`.\n        num_iterations: The number of iterations that the optimizer will run.\n        eta: Scaling parameter to control the aggressiveness of Hyperband's racing.\n        top_n_percent: Determines the percentile of configurations that will be\n            used as training data for the kernel density estimator of the good\n            configuration, e.g if set to 10 the best 10% configurations will be\n            considered for training.\n        min_samples_in_model: Minimum number of datapoints needed to fit a model.\n        num_samples: Number of samples drawn to optimize EI via sampling.\n        random_fraction: Fraction of random configurations returned.\n        bandwidth_factor: Widens the bandwidth for contiuous parameters for\n            proposed points to optimize EI\n        min_bandwidth: to keep diversity, even when all (good) samples have the\n            same value for one of the parameters, a minimum bandwidth\n            (reasonable default: 1e-3) is used instead of zero.\n        seed: [description]\n        logger: [description]\n    \"\"\"\n    if min_samples_in_model is None:\n        min_samples_in_model = 3 * len(search_space)\n\n    self.min_fidelity = min_fidelity\n    self.max_fidelity = max_fidelity\n    self.eta = eta\n\n    self.config_sampler = BOHBSampler(\n        search_space=search_space,\n        objective=objective,\n        min_samples_in_model=min_samples_in_model,\n        top_n_percent=top_n_percent,\n        num_samples=num_samples,\n        random_fraction=random_fraction,\n        bandwidth_factor=bandwidth_factor,\n        min_bandwidth=min_bandwidth,\n        seed=seed,\n    )\n\n    super().__init__(\n        search_space=search_space,\n        objective=objective,\n        num_iterations=num_iterations,\n        seed=seed,\n        logger=logger,\n    )\n</code></pre>"},{"location":"optimizers/botorch/","title":"BoTorch Base Optimizer","text":"<p>This optimizer is a basic Gaussian Process based Bayesian optimization implementation leveraging BoTorch in a way that is compatible with the <code>blackboxopt</code> interface. While this is a functional optimizer, it is more intended as a basis for other BoTorch based optimizer implementations.</p>"},{"location":"optimizers/botorch/#reference","title":"Reference","text":""},{"location":"optimizers/botorch/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer","title":"<code> SingleObjectiveBOTorchOptimizer            (SingleObjectiveOptimizer)         </code>","text":"Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>class SingleObjectiveBOTorchOptimizer(SingleObjectiveOptimizer):\n    def __init__(\n        self,\n        search_space: ps.ParameterSpace,\n        objective: Objective,\n        model: Model,\n        acquisition_function_factory: Callable[[Model], AcquisitionFunction],\n        af_optimizer_kwargs=None,\n        num_initial_random_samples: int = 1,\n        max_pending_evaluations: Optional[int] = 1,\n        batch_shape: torch.Size = torch.Size(),\n        logger: Optional[logging.Logger] = None,\n        seed: Optional[int] = None,\n        torch_dtype: torch.dtype = torch.float64,\n    ):\n        \"\"\"Single objective BO optimizer that uses as a surrogate model the `model`\n        object provided by user.\n\n        The `model` is expected to be extended from BoTorch base model `Model` class,\n        and does not require to be a GP model.\n\n        Args:\n            search_space: The space in which to optimize.\n            objective: The objective to optimize.\n            model: Surrogate model of `Model` type.\n            acquisition_function_factory: Callable that produces an acquisition function\n                instance, could also be a compatible acquisition function class.\n                Only acquisition functions to be minimized are supported.\n                Providing a partially initialized class is possible with, e.g.\n                `functools.partial(UpperConfidenceBound, beta=6.0, maximize=False)`.\n            af_optimizer_kwargs: Settings for acquisition function optimizer,\n                see `botorch.optim.optimize_acqf` and in case the whole search space\n                is discrete: `botorch.optim.optimize_acqf_discrete`. The former can be\n                enforced by providing `raw_samples` or `num_restarts`, the latter by\n                providing `num_random_choices`.\n            num_initial_random_samples: Size of the initial space-filling design that\n                is used before starting BO. The points are sampled randomly in the\n                search space. If no random sampling is required, set it to 0.\n                When random sampling is enabled, but evaluations with missing objective\n                values are reported, more specifications are sampled until\n                `num_initial_random_samples` many valid evaluations were reported.\n            max_pending_evaluations: Maximum number of parallel evaluations. For\n                sequential BO use the default value of 1. If no limit is required,\n                set it to None.\n            batch_shape: Batch dimension(s) used for batched models.\n            logger: Custom logger.\n            seed: A seed to make the optimization reproducible.\n            torch_dtype: Torch data type used for storing the data. This needs to match\n                the dtype of the model used\n        \"\"\"\n        super().__init__(search_space=search_space, objective=objective, seed=seed)\n        self.num_initial_random = num_initial_random_samples\n        self.max_pending_evaluations = max_pending_evaluations\n        self.batch_shape = batch_shape\n        self.logger = logger or logging.getLogger(\"blackboxopt\")\n\n        self.torch_dtype = torch_dtype\n        self.X = torch.empty(\n            (*self.batch_shape, 0, len(search_space)), dtype=torch_dtype\n        )\n        self.losses = torch.empty((*self.batch_shape, 0, 1), dtype=torch_dtype)\n        self.pending_specifications: Dict[int, EvaluationSpecification] = {}\n        if seed is not None:\n            torch.manual_seed(seed=seed)\n\n        self.model = model\n        self.acquisition_function_factory = acquisition_function_factory\n        self.af_optimizer_kwargs = af_optimizer_kwargs\n\n    def _create_fantasy_model(self, model: Model) -&gt; Model:\n        \"\"\"Create model with the pending specifications and model based\n        outcomes added to the training data.\"\"\"\n\n        if not self.pending_specifications:\n            # nothing to do when there are no pending specs\n            return model\n\n        pending_X = torch.tensor(\n            np.array(\n                [\n                    self.search_space.to_numerical(e.configuration)\n                    for e in self.pending_specifications.values()\n                ]\n            ),\n            dtype=self.torch_dtype,\n        )\n\n        model = model.fantasize(pending_X, IIDNormalSampler(1), observation_noise=False)\n\n        if isinstance(model, ExactGP):\n            # ExactGP.fantasize extends model's X and Y with batch_size, even if\n            # originally not given -&gt; need to reshape these to their original\n            # representation\n            n_samples = model.train_targets.size(-1)\n            n_features = len(self.search_space)\n            model.train_inputs[0] = model.train_inputs[0].reshape(\n                torch.Size((*self.batch_shape, n_samples, n_features))\n            )\n            model.train_targets = model.train_targets.reshape(\n                torch.Size((*self.batch_shape, n_samples, 1))\n            )\n        return model\n\n    def _generate_evaluation_specification(self):\n        \"\"\"Optimize acquisition on fantasy model to pick next point.\"\"\"\n        fantasy_model = self._create_fantasy_model(self.model)\n        fantasy_model.eval()\n\n        af = self.acquisition_function_factory(fantasy_model)\n        if getattr(af, \"maximize\", False):\n            raise ValueError(\n                \"Only acquisition functions that need to be minimized are supported. \"\n                f\"The given {af.__class__.__name__} has maximize=True. \"\n                \"One potential fix is using functools.partial(\"\n                f\"{af.__class__.__name__}, maximize=False) as the \"\n                \"acquisition_function_factory init argument.\"\n            )\n\n        acquisition_function_optimizer = _acquisition_function_optimizer_factory(\n            search_space=self.search_space,\n            af_opt_kwargs=self.af_optimizer_kwargs,\n            torch_dtype=self.torch_dtype,\n        )\n        configuration, _ = acquisition_function_optimizer(af)\n\n        return EvaluationSpecification(\n            configuration=self.search_space.from_numerical(configuration[0].numpy()),\n        )\n\n    def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n        \"\"\"Call the optimizer specific function and append a unique integer id\n        to the specification.\n\n        Please refer to the docstring of\n        `blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification`\n        for a description of the method.\n        \"\"\"\n        if (\n            self.max_pending_evaluations\n            and len(self.pending_specifications) == self.max_pending_evaluations\n        ):\n            raise OptimizerNotReady\n\n        # Generate random samples until there are enough samples where at least one of\n        # the objective values is available\n        if self.num_initial_random &gt; 0 and (\n            sum(~torch.any(self.losses.isnan(), dim=1)) &lt; self.num_initial_random\n        ):\n            eval_spec = EvaluationSpecification(\n                configuration=self.search_space.sample(),\n                optimizer_info={\"model_based_pick\": False},\n            )\n        else:\n            eval_spec = self._generate_evaluation_specification()\n            eval_spec.optimizer_info[\"model_based_pick\"] = True\n\n        eval_id = self.X.size(-2) + len(self.pending_specifications)\n        eval_spec.optimizer_info[\"evaluation_id\"] = eval_id\n        self.pending_specifications[eval_id] = eval_spec\n        return eval_spec\n\n    def _remove_pending_specifications(\n        self, evaluations: Union[Evaluation, Iterable[Evaluation]]\n    ):\n        \"\"\"Find and remove the corresponding entries in `self.pending_specifications`.\n\n        Args:\n            evaluations: List of completed evaluations.\n        Raises:\n            ValueError: If an evaluation is reported with an ID that was not issued\n            by the optimizer, the method will fail.\n        \"\"\"\n        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n        for e in _evals:\n            if \"evaluation_id\" not in e.optimizer_info:\n                self.logger.debug(\"User provided EvaluationSpecification received.\")\n                continue\n\n            if e.optimizer_info[\"evaluation_id\"] not in self.pending_specifications:\n                msg = (\n                    \"Unknown evaluation_id reported. This could indicate that the \"\n                    \"evaluation has been reported before!\"\n                )\n                self.logger.error(msg)\n                raise ValueError(msg)\n\n            del self.pending_specifications[e.optimizer_info[\"evaluation_id\"]]\n\n    def _append_evaluations_to_data(\n        self, evaluations: Union[Evaluation, Iterable[Evaluation]]\n    ):\n        \"\"\"Convert the reported evaluation into its numerical representation\n        and append it to the training data.\n\n        Args:\n            evaluations: List of completed evaluations.\n        \"\"\"\n        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n        X, Y = to_numerical(\n            _evals,\n            self.search_space,\n            objectives=[self.objective],\n            batch_shape=self.batch_shape,\n            torch_dtype=self.torch_dtype,\n        )\n\n        # fill in NaNs originating from inactive parameters (conditional spaces support)\n        # botorch expect numerical representation of inputs to be within the unit\n        # hypercube, thus we can't use the default c=-1.0\n        X = impute_nans_with_constant(X, c=0.0)\n\n        self.logger.debug(f\"Next training configuration(s):{X}, {Y}\")\n\n        self.X = torch.cat([self.X, X], dim=-2)\n        self.losses = torch.cat([self.losses, Y], dim=-2)\n\n    def _update_internal_evaluation_data(\n        self, evaluations: Iterable[Evaluation]\n    ) -&gt; None:\n        \"\"\"Check validity of the evaluations and do optimizer agnostic bookkeeping.\"\"\"\n        call_functions_with_evaluations_and_collect_errors(\n            [\n                functools.partial(validate_objectives, objectives=[self.objective]),\n                self._remove_pending_specifications,\n                self._append_evaluations_to_data,\n            ],\n            sort_evaluations(evaluations),\n        )\n\n    def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n        \"\"\"A simple report method that conditions the model on data.\n        This likely needs to be overridden for more specific BO implementations.\n        \"\"\"\n        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n        self._update_internal_evaluation_data(_evals)\n        # Just for populating all relevant caches\n        self.model.posterior(self.X)\n\n        x_filtered, y_filtered = filter_y_nans(self.X, self.losses)\n\n        # The actual model update\n        # Ignore BotorchTensorDimensionWarning which is always reported to make the user\n        # aware that they are reponsible for the right input Tensors dimensionality.\n        with warnings.catch_warnings():\n            warnings.simplefilter(\n                action=\"ignore\", category=BotorchTensorDimensionWarning\n            )\n            self.model = self.model.condition_on_observations(x_filtered, y_filtered)\n\n    def predict_model_based_best(self) -&gt; Optional[Evaluation]:\n        \"\"\"Get the current configuration that is estimated to be the best (in terms of\n        optimal objective value) without waiting for a reported evaluation of that\n        configuration. Instead, the objective value estimation relies on BO's\n        underlying model.\n\n        This might return `None` in case there is no successfully evaluated\n        configuration yet (thus, the optimizer has not been given training data yet).\n\n        Returns:\n            blackboxopt.evaluation.Evaluation\n                The evaluated specification containing the estimated best configuration\n                or `None` in case no evaluations have been reported yet.\n        \"\"\"\n        return predict_model_based_best(\n            model=self.model,\n            objective=self.objective,\n            search_space=self.search_space,\n            torch_dtype=self.torch_dtype,\n        )\n</code></pre>"},{"location":"optimizers/botorch/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.__init__","title":"<code>__init__(self, search_space, objective, model, acquisition_function_factory, af_optimizer_kwargs=None, num_initial_random_samples=1, max_pending_evaluations=1, batch_shape=torch.Size([]), logger=None, seed=None, torch_dtype=torch.float64)</code>  <code>special</code>","text":"<p>Single objective BO optimizer that uses as a surrogate model the <code>model</code> object provided by user.</p> <p>The <code>model</code> is expected to be extended from BoTorch base model <code>Model</code> class, and does not require to be a GP model.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>The space in which to optimize.</p> required <code>objective</code> <code>Objective</code> <p>The objective to optimize.</p> required <code>model</code> <code>Model</code> <p>Surrogate model of <code>Model</code> type.</p> required <code>acquisition_function_factory</code> <code>Callable[[botorch.models.model.Model], botorch.acquisition.acquisition.AcquisitionFunction]</code> <p>Callable that produces an acquisition function instance, could also be a compatible acquisition function class. Only acquisition functions to be minimized are supported. Providing a partially initialized class is possible with, e.g. <code>functools.partial(UpperConfidenceBound, beta=6.0, maximize=False)</code>.</p> required <code>af_optimizer_kwargs</code> <p>Settings for acquisition function optimizer, see <code>botorch.optim.optimize_acqf</code> and in case the whole search space is discrete: <code>botorch.optim.optimize_acqf_discrete</code>. The former can be enforced by providing <code>raw_samples</code> or <code>num_restarts</code>, the latter by providing <code>num_random_choices</code>.</p> <code>None</code> <code>num_initial_random_samples</code> <code>int</code> <p>Size of the initial space-filling design that is used before starting BO. The points are sampled randomly in the search space. If no random sampling is required, set it to 0. When random sampling is enabled, but evaluations with missing objective values are reported, more specifications are sampled until <code>num_initial_random_samples</code> many valid evaluations were reported.</p> <code>1</code> <code>max_pending_evaluations</code> <code>Optional[int]</code> <p>Maximum number of parallel evaluations. For sequential BO use the default value of 1. If no limit is required, set it to None.</p> <code>1</code> <code>batch_shape</code> <code>Size</code> <p>Batch dimension(s) used for batched models.</p> <code>torch.Size([])</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>Custom logger.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>A seed to make the optimization reproducible.</p> <code>None</code> <code>torch_dtype</code> <code>dtype</code> <p>Torch data type used for storing the data. This needs to match the dtype of the model used</p> <code>torch.float64</code> Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>def __init__(\n    self,\n    search_space: ps.ParameterSpace,\n    objective: Objective,\n    model: Model,\n    acquisition_function_factory: Callable[[Model], AcquisitionFunction],\n    af_optimizer_kwargs=None,\n    num_initial_random_samples: int = 1,\n    max_pending_evaluations: Optional[int] = 1,\n    batch_shape: torch.Size = torch.Size(),\n    logger: Optional[logging.Logger] = None,\n    seed: Optional[int] = None,\n    torch_dtype: torch.dtype = torch.float64,\n):\n    \"\"\"Single objective BO optimizer that uses as a surrogate model the `model`\n    object provided by user.\n\n    The `model` is expected to be extended from BoTorch base model `Model` class,\n    and does not require to be a GP model.\n\n    Args:\n        search_space: The space in which to optimize.\n        objective: The objective to optimize.\n        model: Surrogate model of `Model` type.\n        acquisition_function_factory: Callable that produces an acquisition function\n            instance, could also be a compatible acquisition function class.\n            Only acquisition functions to be minimized are supported.\n            Providing a partially initialized class is possible with, e.g.\n            `functools.partial(UpperConfidenceBound, beta=6.0, maximize=False)`.\n        af_optimizer_kwargs: Settings for acquisition function optimizer,\n            see `botorch.optim.optimize_acqf` and in case the whole search space\n            is discrete: `botorch.optim.optimize_acqf_discrete`. The former can be\n            enforced by providing `raw_samples` or `num_restarts`, the latter by\n            providing `num_random_choices`.\n        num_initial_random_samples: Size of the initial space-filling design that\n            is used before starting BO. The points are sampled randomly in the\n            search space. If no random sampling is required, set it to 0.\n            When random sampling is enabled, but evaluations with missing objective\n            values are reported, more specifications are sampled until\n            `num_initial_random_samples` many valid evaluations were reported.\n        max_pending_evaluations: Maximum number of parallel evaluations. For\n            sequential BO use the default value of 1. If no limit is required,\n            set it to None.\n        batch_shape: Batch dimension(s) used for batched models.\n        logger: Custom logger.\n        seed: A seed to make the optimization reproducible.\n        torch_dtype: Torch data type used for storing the data. This needs to match\n            the dtype of the model used\n    \"\"\"\n    super().__init__(search_space=search_space, objective=objective, seed=seed)\n    self.num_initial_random = num_initial_random_samples\n    self.max_pending_evaluations = max_pending_evaluations\n    self.batch_shape = batch_shape\n    self.logger = logger or logging.getLogger(\"blackboxopt\")\n\n    self.torch_dtype = torch_dtype\n    self.X = torch.empty(\n        (*self.batch_shape, 0, len(search_space)), dtype=torch_dtype\n    )\n    self.losses = torch.empty((*self.batch_shape, 0, 1), dtype=torch_dtype)\n    self.pending_specifications: Dict[int, EvaluationSpecification] = {}\n    if seed is not None:\n        torch.manual_seed(seed=seed)\n\n    self.model = model\n    self.acquisition_function_factory = acquisition_function_factory\n    self.af_optimizer_kwargs = af_optimizer_kwargs\n</code></pre>"},{"location":"optimizers/botorch/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.generate_evaluation_specification","title":"<code>generate_evaluation_specification(self)</code>","text":"<p>Call the optimizer specific function and append a unique integer id to the specification.</p> <p>Please refer to the docstring of <code>blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification</code> for a description of the method.</p> Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n    \"\"\"Call the optimizer specific function and append a unique integer id\n    to the specification.\n\n    Please refer to the docstring of\n    `blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification`\n    for a description of the method.\n    \"\"\"\n    if (\n        self.max_pending_evaluations\n        and len(self.pending_specifications) == self.max_pending_evaluations\n    ):\n        raise OptimizerNotReady\n\n    # Generate random samples until there are enough samples where at least one of\n    # the objective values is available\n    if self.num_initial_random &gt; 0 and (\n        sum(~torch.any(self.losses.isnan(), dim=1)) &lt; self.num_initial_random\n    ):\n        eval_spec = EvaluationSpecification(\n            configuration=self.search_space.sample(),\n            optimizer_info={\"model_based_pick\": False},\n        )\n    else:\n        eval_spec = self._generate_evaluation_specification()\n        eval_spec.optimizer_info[\"model_based_pick\"] = True\n\n    eval_id = self.X.size(-2) + len(self.pending_specifications)\n    eval_spec.optimizer_info[\"evaluation_id\"] = eval_id\n    self.pending_specifications[eval_id] = eval_spec\n    return eval_spec\n</code></pre>"},{"location":"optimizers/botorch/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.predict_model_based_best","title":"<code>predict_model_based_best(self)</code>","text":"<p>Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model.</p> <p>This might return <code>None</code> in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet).</p> <p>Returns:</p> Type Description <code>Optional[blackboxopt.evaluation.Evaluation]</code> <p>blackboxopt.evaluation.Evaluation     The evaluated specification containing the estimated best configuration     or <code>None</code> in case no evaluations have been reported yet.</p> Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>def predict_model_based_best(self) -&gt; Optional[Evaluation]:\n    \"\"\"Get the current configuration that is estimated to be the best (in terms of\n    optimal objective value) without waiting for a reported evaluation of that\n    configuration. Instead, the objective value estimation relies on BO's\n    underlying model.\n\n    This might return `None` in case there is no successfully evaluated\n    configuration yet (thus, the optimizer has not been given training data yet).\n\n    Returns:\n        blackboxopt.evaluation.Evaluation\n            The evaluated specification containing the estimated best configuration\n            or `None` in case no evaluations have been reported yet.\n    \"\"\"\n    return predict_model_based_best(\n        model=self.model,\n        objective=self.objective,\n        search_space=self.search_space,\n        torch_dtype=self.torch_dtype,\n    )\n</code></pre>"},{"location":"optimizers/botorch/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.report","title":"<code>report(self, evaluations)</code>","text":"<p>A simple report method that conditions the model on data. This likely needs to be overridden for more specific BO implementations.</p> Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n    \"\"\"A simple report method that conditions the model on data.\n    This likely needs to be overridden for more specific BO implementations.\n    \"\"\"\n    _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n    self._update_internal_evaluation_data(_evals)\n    # Just for populating all relevant caches\n    self.model.posterior(self.X)\n\n    x_filtered, y_filtered = filter_y_nans(self.X, self.losses)\n\n    # The actual model update\n    # Ignore BotorchTensorDimensionWarning which is always reported to make the user\n    # aware that they are reponsible for the right input Tensors dimensionality.\n    with warnings.catch_warnings():\n        warnings.simplefilter(\n            action=\"ignore\", category=BotorchTensorDimensionWarning\n        )\n        self.model = self.model.condition_on_observations(x_filtered, y_filtered)\n</code></pre>"},{"location":"optimizers/space-filling/","title":"Space Filling Optimizer","text":"<p>The <code>SpaceFilling</code> optimizer is a Sobol sequence based optimizer that covers the search space based on a quasi-random low-discrepancy sequence. This strategy requires a larger budget for evaluations but can be a good initial approach to get to know the optimization problem at hand. While this implementation follows the overall interface including the specification and reporting of objectives and their values, the actual objective values are inconsequential for the underlying Sobol sequence and do not guide the optimization.</p>"},{"location":"optimizers/space-filling/#reference","title":"Reference","text":""},{"location":"optimizers/space-filling/#blackboxopt.optimizers.space_filling.SpaceFilling","title":"<code> SpaceFilling            (MultiObjectiveOptimizer)         </code>","text":"<p>Sobol sequence based, space filling optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>The search space to optimize</p> required <code>objectives</code> <code>List[blackboxopt.base.Objective]</code> <p>The objectives of the optimization</p> required <code>seed</code> <code>Optional[int]</code> <p>The sobol sequence is Owen scrambled and can be seeded for reproducibility</p> <code>None</code> Source code in <code>blackboxopt/optimizers/space_filling.py</code> <pre><code>class SpaceFilling(MultiObjectiveOptimizer):\n    \"\"\"Sobol sequence based, space filling optimizer.\n\n    Args:\n        search_space: The search space to optimize\n        objectives: The objectives of the optimization\n        seed: The sobol sequence is Owen scrambled and can be seeded for reproducibility\n    \"\"\"\n\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objectives: List[Objective],\n        seed: Optional[int] = None,\n    ) -&gt; None:\n        super().__init__(search_space=search_space, objectives=objectives, seed=seed)\n        self.sobol = Sobol(d=len(self.search_space), scramble=True, seed=seed)\n\n    def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n        vector = self.sobol.random().flatten()\n        configuration = self.search_space.from_numerical(vector)\n        return EvaluationSpecification(configuration=configuration)\n</code></pre>"},{"location":"optimizers/space-filling/#blackboxopt.optimizers.space_filling.SpaceFilling.generate_evaluation_specification","title":"<code>generate_evaluation_specification(self)</code>","text":"<p>Get next configuration and settings to evaluate.</p> <p>Exceptions:</p> Type Description <code>OptimizationComplete</code> <p>When the optimization run is finished, e.g. when the budget has been exhausted.</p> <code>OptimizerNotReady</code> <p>When the optimizer is not ready to propose a new evaluation specification.</p> Source code in <code>blackboxopt/optimizers/space_filling.py</code> <pre><code>def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n    vector = self.sobol.random().flatten()\n    configuration = self.search_space.from_numerical(vector)\n    return EvaluationSpecification(configuration=configuration)\n</code></pre>"},{"location":"optimizers/testing/","title":"Reference Tests","text":"<p>When you develop an optimizer based on the interface defined as part of <code>blackboxopt.base</code>, you can use <code>blackboxopt.testing</code> to directly test whether your implementation follows the specification by adding a test like this to your test suite:</p> <pre><code>import pytest\nfrom blackboxopt.testing import ALL_REFERENCE_TESTS\n\n@pytest.mark.parametrize(\"reference_test\", ALL_REFERENCE_TESTS)\ndef test_all_reference_tests(reference_test):\n    reference_test(CustomOptimizer, optional_optimizer_init_kwargs)\n</code></pre>"},{"location":"reference/base/","title":"Base","text":""},{"location":"reference/base/#blackboxopt.base.ConstraintsError","title":"<code> ConstraintsError            (ValueError)         </code>","text":"<p>Raised on incomplete or missing constraints.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>class ConstraintsError(ValueError):\n    \"\"\"Raised on incomplete or missing constraints.\"\"\"\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.ContextError","title":"<code> ContextError            (ValueError)         </code>","text":"<p>Raised on incomplete or missing context information.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>class ContextError(ValueError):\n    \"\"\"Raised on incomplete or missing context information.\"\"\"\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.EvaluationsError","title":"<code> EvaluationsError            (ValueError)         </code>","text":"<p>Raised on invalid evaluations.</p> <p>The problematic evaluations and their respective exceptions are passed in the <code>evaluations_with_errors</code> attribute.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>class EvaluationsError(ValueError):\n    \"\"\"Raised on invalid evaluations.\n\n    The problematic evaluations and their respective exceptions are passed in the\n    `evaluations_with_errors` attribute.\n    \"\"\"\n\n    def __init__(self, evaluations_with_errors: List[Tuple[Evaluation, Exception]]):\n        self.message = (\n            \"An error with one or more evaluations occurred. Check the \"\n            \"'evaluations_with_errors' attribute of this exception for details.\"\n        )\n        self.evaluations_with_errors = evaluations_with_errors\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.MultiObjectiveOptimizer","title":"<code> MultiObjectiveOptimizer            (Optimizer)         </code>","text":"Source code in <code>blackboxopt/base.py</code> <pre><code>class MultiObjectiveOptimizer(Optimizer):\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objectives: List[Objective],\n        seed: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer with an optional seed for reproducibility.\n\n        Args:\n            search_space: The search space to optimize.\n            objectives: The objectives of the optimization.\n            seed: A seed for the optimizer, which is also used to re-seed the provided\n                search space.\n        \"\"\"\n        super().__init__(search_space=search_space, seed=seed)\n\n        _raise_on_duplicate_objective_names(objectives)\n        self.objectives = objectives\n\n    def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n        call_functions_with_evaluations_and_collect_errors(\n            [functools.partial(validate_objectives, objectives=self.objectives)],\n            _evals,\n        )\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.MultiObjectiveOptimizer.__init__","title":"<code>__init__(self, search_space, objectives, seed=None)</code>  <code>special</code>","text":"<p>Initialize the optimizer with an optional seed for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>The search space to optimize.</p> required <code>objectives</code> <code>List[blackboxopt.base.Objective]</code> <p>The objectives of the optimization.</p> required <code>seed</code> <code>Optional[int]</code> <p>A seed for the optimizer, which is also used to re-seed the provided search space.</p> <code>None</code> Source code in <code>blackboxopt/base.py</code> <pre><code>def __init__(\n    self,\n    search_space: ParameterSpace,\n    objectives: List[Objective],\n    seed: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer with an optional seed for reproducibility.\n\n    Args:\n        search_space: The search space to optimize.\n        objectives: The objectives of the optimization.\n        seed: A seed for the optimizer, which is also used to re-seed the provided\n            search space.\n    \"\"\"\n    super().__init__(search_space=search_space, seed=seed)\n\n    _raise_on_duplicate_objective_names(objectives)\n    self.objectives = objectives\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.MultiObjectiveOptimizer.report","title":"<code>report(self, evaluations)</code>","text":"<p>Report one or more evaluated evaluation specifications.</p> <p>NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations</code> <code>Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]]</code> <p>A single evaluated evaluation specifications, or an iterable of many.</p> required Source code in <code>blackboxopt/base.py</code> <pre><code>def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n    _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n    call_functions_with_evaluations_and_collect_errors(\n        [functools.partial(validate_objectives, objectives=self.objectives)],\n        _evals,\n    )\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.Objective","title":"<code> Objective        </code>  <code>dataclass</code>","text":"<p>Objective(name: str, greater_is_better: bool)</p> Source code in <code>blackboxopt/base.py</code> <pre><code>@dataclass\nclass Objective:\n    name: str\n    greater_is_better: bool\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.ObjectivesError","title":"<code> ObjectivesError            (ValueError)         </code>","text":"<p>Raised on incomplete or missing objectives.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>class ObjectivesError(ValueError):\n    \"\"\"Raised on incomplete or missing objectives.\"\"\"\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.OptimizationComplete","title":"<code> OptimizationComplete            (Exception)         </code>","text":"<p>Exception that is raised when the optimization run is finished, e.g. when the budget has been exhausted.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>class OptimizationComplete(Exception):\n    \"\"\"Exception that is raised when the optimization run is finished, e.g. when the\n    budget has been exhausted.\n    \"\"\"\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.Optimizer","title":"<code> Optimizer            (ABC)         </code>","text":"<p>Abstract base class for blackbox optimizer implementations.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>class Optimizer(abc.ABC):\n    \"\"\"Abstract base class for blackbox optimizer implementations.\"\"\"\n\n    def __init__(\n        self, search_space: ParameterSpace, seed: Optional[int] = None\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer with an optional seed for reproducibility.\n\n        Args:\n            search_space: The search space to optimize.\n            seed: A seed for the optimizer, which is also used to re-seed the provided\n                search space.\n        \"\"\"\n        super().__init__()\n        self.search_space = search_space\n        self.seed = seed\n\n        if self.seed is not None:\n            self.search_space.seed(self.seed)\n\n    @abc.abstractmethod\n    def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n        \"\"\"Get next configuration and settings to evaluate.\n\n        Raises:\n            OptimizationComplete: When the optimization run is finished, e.g. when the\n                budget has been exhausted.\n            OptimizerNotReady: When the optimizer is not ready to propose a new\n                evaluation specification.\n        \"\"\"\n\n    @abc.abstractmethod\n    def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n        \"\"\"Report one or more evaluated evaluation specifications.\n\n        NOTE: Not all optimizers support reporting results for evaluation specifications\n        that were not proposed by the optimizer.\n\n        Args:\n            evaluations: A single evaluated evaluation specifications, or an iterable\n                of many.\n        \"\"\"\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.Optimizer.__init__","title":"<code>__init__(self, search_space, seed=None)</code>  <code>special</code>","text":"<p>Initialize the optimizer with an optional seed for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>The search space to optimize.</p> required <code>seed</code> <code>Optional[int]</code> <p>A seed for the optimizer, which is also used to re-seed the provided search space.</p> <code>None</code> Source code in <code>blackboxopt/base.py</code> <pre><code>def __init__(\n    self, search_space: ParameterSpace, seed: Optional[int] = None\n) -&gt; None:\n    \"\"\"Initialize the optimizer with an optional seed for reproducibility.\n\n    Args:\n        search_space: The search space to optimize.\n        seed: A seed for the optimizer, which is also used to re-seed the provided\n            search space.\n    \"\"\"\n    super().__init__()\n    self.search_space = search_space\n    self.seed = seed\n\n    if self.seed is not None:\n        self.search_space.seed(self.seed)\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.Optimizer.generate_evaluation_specification","title":"<code>generate_evaluation_specification(self)</code>","text":"<p>Get next configuration and settings to evaluate.</p> <p>Exceptions:</p> Type Description <code>OptimizationComplete</code> <p>When the optimization run is finished, e.g. when the budget has been exhausted.</p> <code>OptimizerNotReady</code> <p>When the optimizer is not ready to propose a new evaluation specification.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>@abc.abstractmethod\ndef generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n    \"\"\"Get next configuration and settings to evaluate.\n\n    Raises:\n        OptimizationComplete: When the optimization run is finished, e.g. when the\n            budget has been exhausted.\n        OptimizerNotReady: When the optimizer is not ready to propose a new\n            evaluation specification.\n    \"\"\"\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.Optimizer.report","title":"<code>report(self, evaluations)</code>","text":"<p>Report one or more evaluated evaluation specifications.</p> <p>NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations</code> <code>Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]]</code> <p>A single evaluated evaluation specifications, or an iterable of many.</p> required Source code in <code>blackboxopt/base.py</code> <pre><code>@abc.abstractmethod\ndef report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n    \"\"\"Report one or more evaluated evaluation specifications.\n\n    NOTE: Not all optimizers support reporting results for evaluation specifications\n    that were not proposed by the optimizer.\n\n    Args:\n        evaluations: A single evaluated evaluation specifications, or an iterable\n            of many.\n    \"\"\"\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.OptimizerNotReady","title":"<code> OptimizerNotReady            (Exception)         </code>","text":"<p>Exception that is raised when the optimizer is not ready to propose a new evaluation specification.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>class OptimizerNotReady(Exception):\n    \"\"\"Exception that is raised when the optimizer is not ready to propose a new\n    evaluation specification.\n    \"\"\"\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.SingleObjectiveOptimizer","title":"<code> SingleObjectiveOptimizer            (Optimizer)         </code>","text":"Source code in <code>blackboxopt/base.py</code> <pre><code>class SingleObjectiveOptimizer(Optimizer):\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objective: Objective,\n        seed: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the optimizer with an optional seed for reproducibility.\n\n        Args:\n            search_space: The search space to optimize.\n            objective: The objectives of the optimization.\n            seed: A seed for the optimizer, which is also used to re-seed the provided\n                search space.\n        \"\"\"\n        super().__init__(search_space=search_space, seed=seed)\n        self.objective = objective\n\n    def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n        \"\"\"Report one or multiple evaluations to the optimizer.\n\n        All valid evaluations are processed. Faulty evaluations are not processed,\n        instead an `EvaluationsError` is raised, which includes the problematic\n        evaluations with their respective Exceptions in the `evaluations_with_errors`\n        attribute.\n\n        Args:\n            evaluations: A single evaluated evaluation specifications, or an iterable\n                of many.\n\n        Raises:\n            EvaluationsError: Raised when an evaluation could not be processed.\n        \"\"\"\n        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n        call_functions_with_evaluations_and_collect_errors(\n            [functools.partial(validate_objectives, objectives=[self.objective])],\n            _evals,\n        )\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.SingleObjectiveOptimizer.__init__","title":"<code>__init__(self, search_space, objective, seed=None)</code>  <code>special</code>","text":"<p>Initialize the optimizer with an optional seed for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>The search space to optimize.</p> required <code>objective</code> <code>Objective</code> <p>The objectives of the optimization.</p> required <code>seed</code> <code>Optional[int]</code> <p>A seed for the optimizer, which is also used to re-seed the provided search space.</p> <code>None</code> Source code in <code>blackboxopt/base.py</code> <pre><code>def __init__(\n    self,\n    search_space: ParameterSpace,\n    objective: Objective,\n    seed: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer with an optional seed for reproducibility.\n\n    Args:\n        search_space: The search space to optimize.\n        objective: The objectives of the optimization.\n        seed: A seed for the optimizer, which is also used to re-seed the provided\n            search space.\n    \"\"\"\n    super().__init__(search_space=search_space, seed=seed)\n    self.objective = objective\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.SingleObjectiveOptimizer.report","title":"<code>report(self, evaluations)</code>","text":"<p>Report one or multiple evaluations to the optimizer.</p> <p>All valid evaluations are processed. Faulty evaluations are not processed, instead an <code>EvaluationsError</code> is raised, which includes the problematic evaluations with their respective Exceptions in the <code>evaluations_with_errors</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations</code> <code>Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]]</code> <p>A single evaluated evaluation specifications, or an iterable of many.</p> required <p>Exceptions:</p> Type Description <code>EvaluationsError</code> <p>Raised when an evaluation could not be processed.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n    \"\"\"Report one or multiple evaluations to the optimizer.\n\n    All valid evaluations are processed. Faulty evaluations are not processed,\n    instead an `EvaluationsError` is raised, which includes the problematic\n    evaluations with their respective Exceptions in the `evaluations_with_errors`\n    attribute.\n\n    Args:\n        evaluations: A single evaluated evaluation specifications, or an iterable\n            of many.\n\n    Raises:\n        EvaluationsError: Raised when an evaluation could not be processed.\n    \"\"\"\n    _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n    call_functions_with_evaluations_and_collect_errors(\n        [functools.partial(validate_objectives, objectives=[self.objective])],\n        _evals,\n    )\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.call_functions_with_evaluations_and_collect_errors","title":"<code>call_functions_with_evaluations_and_collect_errors(functions, evaluations)</code>","text":"<p>The given evaluations are passed to all given functions in order and the first Exception that occurrs for an evaluation is recorded alongside the evaluation and raised together with all erroneous evaluations as part of an <code>EvaluationsError</code>.</p> <p>NOTE: Even if reporting some evaluations fails, all that can be are successfully reported. Also, if an evaluation passes through some of the functions before causing issues, the effect the evaluation had on the previous functions can't be reverted.</p> <p>Exceptions:</p> Type Description <code>EvaluationsError</code> <p>In case exceptions occurred when calling the functions with the evaluations.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>def call_functions_with_evaluations_and_collect_errors(\n    functions: Iterable[Callable[[Evaluation], None]],\n    evaluations: Iterable[Evaluation],\n) -&gt; None:\n    \"\"\"The given evaluations are passed to all given functions in order and the first\n    Exception that occurrs for an evaluation is recorded alongside the evaluation and\n    raised together with all erroneous evaluations as part of an `EvaluationsError`.\n\n    NOTE: Even if reporting some evaluations fails, all that can be are successfully\n    reported. Also, if an evaluation passes through some of the functions before causing\n    issues, the effect the evaluation had on the previous functions can't be reverted.\n\n    Raises:\n        EvaluationsError: In case exceptions occurred when calling the functions with\n            the evaluations.\n    \"\"\"\n    evaluations_with_errors = []\n    for evaluation in evaluations:\n        for func in functions:\n            try:\n                func(evaluation)\n            except EvaluationsError as e:\n                evaluations_with_errors.extend(e.evaluations_with_errors)\n                break\n            except Exception as e:\n                evaluations_with_errors.append((evaluation, e))\n                break\n\n    if evaluations_with_errors:\n        raise EvaluationsError(evaluations_with_errors)\n</code></pre>"},{"location":"reference/base/#blackboxopt.base.raise_on_unknown_or_incomplete","title":"<code>raise_on_unknown_or_incomplete(exception, known, reported)</code>","text":"<p>Raise the given exception if not all known strings are contained in reported or the other way around.</p> Source code in <code>blackboxopt/base.py</code> <pre><code>def raise_on_unknown_or_incomplete(\n    exception: Type[ValueError], known: Iterable[str], reported: Iterable[str]\n) -&gt; None:\n    \"\"\"Raise the given exception if not all known strings are contained in reported or\n    the other way around.\n    \"\"\"\n    known_set = set(known)\n    reported_set = set(reported)\n\n    unknown = reported_set - known_set\n    if unknown:\n        raise exception(\n            f\"Unknown reported: {list(unknown)}. Valid are only: {list(known_set)}\"\n        )\n    missing = known_set - reported_set\n    if missing:\n        raise exception(f\"Missing: {list(missing)}\")\n</code></pre>"},{"location":"reference/evaluation/","title":"Evaluation","text":""},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation","title":"<code> Evaluation            (EvaluationSpecification, _EvaluationBase)         </code>  <code>dataclass</code>","text":"<p>An evaluated specification with a timestamp indicating the time of the evaluation, and a result dictionary for all objective values.</p> <p>NOTE: <code>NaN</code> is not allowed as an objective value, use <code>None</code> instead.</p> Source code in <code>blackboxopt/evaluation.py</code> <pre><code>@dataclass\nclass Evaluation(EvaluationSpecification, _EvaluationBase):\n    \"\"\"An evaluated specification with a timestamp indicating the time of the\n    evaluation, and a result dictionary for all objective values.\n\n    NOTE: `NaN` is not allowed as an objective value, use `None` instead.\n    \"\"\"\n\n    constraints: Optional[Dict[str, Optional[float]]] = field(\n        default=None,\n        metadata={\n            \"Description\": \"For each constraint name the float value indicates \"\n            + \"how much the constraint was satisfied, with negative values implying \"\n            + \"a violated and positive values indicating a satisfied constraint.\"\n        },\n    )\n\n    finished_unixtime: float = field(\n        default_factory=_datetime_now_timestamp,\n        metadata={\"Description\": \"Timestamp at completion of this evaluation.\"},\n    )\n\n    stacktrace: Optional[str] = field(\n        default=None,\n        metadata={\n            \"Description\": \"The stacktrace in case an unhandled exception occurred \"\n            + \"inside the evaluation function.\"\n        },\n    )\n\n    user_info: Optional[dict] = field(\n        default=None,\n        metadata={\"Description\": \"Miscellaneous information provided by the user.\"},\n    )\n\n    def get_specification(\n        self, reset_created_unixtime: bool = False\n    ) -&gt; EvaluationSpecification:\n        \"\"\"Get the evaluation specifiation for which this result was evaluated.\"\"\"\n        eval_spec_kwargs = deepcopy(\n            dict(\n                configuration=self.configuration,\n                settings=self.settings,\n                optimizer_info=self.optimizer_info,\n                context=self.context,\n            )\n        )\n\n        if reset_created_unixtime:\n            return EvaluationSpecification(\n                created_unixtime=_datetime_now_timestamp(), **eval_spec_kwargs\n            )\n\n        return EvaluationSpecification(\n            created_unixtime=self.created_unixtime, **eval_spec_kwargs\n        )\n\n    @property\n    def any_objective_none(self) -&gt; bool:\n        return any([v is None for v in self.objectives.values()])\n\n    @property\n    def all_objectives_none(self) -&gt; bool:\n        return all([v is None for v in self.objectives.values()])\n</code></pre>"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.get_specification","title":"<code>get_specification(self, reset_created_unixtime=False)</code>","text":"<p>Get the evaluation specifiation for which this result was evaluated.</p> Source code in <code>blackboxopt/evaluation.py</code> <pre><code>def get_specification(\n    self, reset_created_unixtime: bool = False\n) -&gt; EvaluationSpecification:\n    \"\"\"Get the evaluation specifiation for which this result was evaluated.\"\"\"\n    eval_spec_kwargs = deepcopy(\n        dict(\n            configuration=self.configuration,\n            settings=self.settings,\n            optimizer_info=self.optimizer_info,\n            context=self.context,\n        )\n    )\n\n    if reset_created_unixtime:\n        return EvaluationSpecification(\n            created_unixtime=_datetime_now_timestamp(), **eval_spec_kwargs\n        )\n\n    return EvaluationSpecification(\n        created_unixtime=self.created_unixtime, **eval_spec_kwargs\n    )\n</code></pre>"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification","title":"<code> EvaluationSpecification            (Mapping, Generic)         </code>  <code>dataclass</code>","text":"<p>EvaluationSpecification(configuration: dict, settings: dict = , optimizer_info: dict = , created_unixtime: float = , context: Optional[Dict[str, Any]] = None) Source code in <code>blackboxopt/evaluation.py</code> <pre><code>@dataclass\nclass EvaluationSpecification(Mapping[str, Any]):\n    configuration: dict = field(\n        metadata={\"Description\": \"The configuration to be evaluated next.\"}\n    )\n\n    settings: dict = field(\n        default_factory=dict,\n        metadata={\n            \"Description\": \"Additional settings like the fidelity or target task.\"\n        },\n    )\n\n    optimizer_info: dict = field(\n        default_factory=dict,\n        metadata={\"Description\": \"Information about and for internal optimizer state.\"},\n    )\n\n    created_unixtime: float = field(\n        default_factory=_datetime_now_timestamp,\n        metadata={\"Description\": \"Creation time of the evaluation specificiation.\"},\n    )\n\n    context: Optional[Dict[str, Any]] = field(\n        default=None,\n        metadata={\n            \"Description\": \"Contextual information is what you can determine but not \"\n            + \"influence, like the environmental temperature.\"\n        },\n    )\n\n    def keys(self):\n        return self.__dataclass_fields__.keys()  # pylint: disable=no-member\n\n    def create_evaluation(\n        self,\n        objectives: Dict[str, Optional[float]],\n        constraints: Optional[Dict[str, Optional[float]]] = None,\n        user_info: Optional[dict] = None,\n        stacktrace: Optional[str] = None,\n        finished_unixtime: Optional[float] = None,\n    ):\n        \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification.\n\n        Args:\n            objectives: For each objective name the respective value.\n            constraints: For each constraint name the float value indicates how much the\n                constraint was satisfied, with negative values implying a violated and\n                positive values indicating a satisfied constraint.\n            user_info: Miscellaneous information provided by the user.\n            stacktrace: The stacktrace in case an unhandled exception occurred inside\n                the evaluation function.\n            finished_unixtime: Timestamp at completion of this evaluation. If none is\n                provided, the current time is used.\n        \"\"\"\n        evaluation = Evaluation(\n            objectives=objectives,\n            constraints=constraints,\n            user_info=user_info,\n            stacktrace=stacktrace,\n            **self,\n        )\n\n        # Data class default factories like in this case time.time are only triggered\n        # when the argument is not provided, so in case of it being None we can't just\n        # pass the argument value in, because it would set it to None instead of\n        # triggering the default factory for the current time.\n        if finished_unixtime is not None:\n            evaluation.finished_unixtime = finished_unixtime\n\n        return evaluation\n\n    def __getitem__(self, key):\n        if key not in self.__dataclass_fields__:  # pylint: disable=no-member\n            raise KeyError(\n                f\"Only dataclass fields are accessible via __getitem__, '{key}' is not.\"\n            )\n\n        return deepcopy(getattr(self, key))\n\n    def __iter__(self):\n        return self.__dataclass_fields__.__iter__  # pylint: disable=no-member\n\n    def __len__(self):\n        return self.__dataclass_fields__.__len__  # pylint: disable=no-member\n\n    def to_json(self, **json_dump_kwargs):\n        return json.dumps(asdict(self), **json_dump_kwargs)\n\n    def to_dict(self):\n        return self.__dict__\n</code></pre>"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.create_evaluation","title":"<code>create_evaluation(self, objectives, constraints=None, user_info=None, stacktrace=None, finished_unixtime=None)</code>","text":"<p>Create a blackboxopt.Evaluation based on this evaluation specification.</p> <p>Parameters:</p> Name Type Description Default <code>objectives</code> <code>Dict[str, Optional[float]]</code> <p>For each objective name the respective value.</p> required <code>constraints</code> <code>Optional[Dict[str, Optional[float]]]</code> <p>For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint.</p> <code>None</code> <code>user_info</code> <code>Optional[dict]</code> <p>Miscellaneous information provided by the user.</p> <code>None</code> <code>stacktrace</code> <code>Optional[str]</code> <p>The stacktrace in case an unhandled exception occurred inside the evaluation function.</p> <code>None</code> <code>finished_unixtime</code> <code>Optional[float]</code> <p>Timestamp at completion of this evaluation. If none is provided, the current time is used.</p> <code>None</code> Source code in <code>blackboxopt/evaluation.py</code> <pre><code>def create_evaluation(\n    self,\n    objectives: Dict[str, Optional[float]],\n    constraints: Optional[Dict[str, Optional[float]]] = None,\n    user_info: Optional[dict] = None,\n    stacktrace: Optional[str] = None,\n    finished_unixtime: Optional[float] = None,\n):\n    \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification.\n\n    Args:\n        objectives: For each objective name the respective value.\n        constraints: For each constraint name the float value indicates how much the\n            constraint was satisfied, with negative values implying a violated and\n            positive values indicating a satisfied constraint.\n        user_info: Miscellaneous information provided by the user.\n        stacktrace: The stacktrace in case an unhandled exception occurred inside\n            the evaluation function.\n        finished_unixtime: Timestamp at completion of this evaluation. If none is\n            provided, the current time is used.\n    \"\"\"\n    evaluation = Evaluation(\n        objectives=objectives,\n        constraints=constraints,\n        user_info=user_info,\n        stacktrace=stacktrace,\n        **self,\n    )\n\n    # Data class default factories like in this case time.time are only triggered\n    # when the argument is not provided, so in case of it being None we can't just\n    # pass the argument value in, because it would set it to None instead of\n    # triggering the default factory for the current time.\n    if finished_unixtime is not None:\n        evaluation.finished_unixtime = finished_unixtime\n\n    return evaluation\n</code></pre>"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.keys","title":"<code>keys(self)</code>","text":"<p>D.keys() -&gt; a set-like object providing a view on D's keys</p> Source code in <code>blackboxopt/evaluation.py</code> <pre><code>def keys(self):\n    return self.__dataclass_fields__.keys()  # pylint: disable=no-member\n</code></pre>"},{"location":"reference/io/","title":"Io","text":""},{"location":"reference/io/#blackboxopt.io.load_study_from_json","title":"<code>load_study_from_json(json_file_path)</code>","text":"<p>Load space, objectives and evaluations from a given <code>json_file_path</code>.</p> Source code in <code>blackboxopt/io.py</code> <pre><code>def load_study_from_json(\n    json_file_path: os.PathLike,\n) -&gt; Tuple[ps.ParameterSpace, List[Objective], List[Evaluation]]:\n    \"\"\"Load space, objectives and evaluations from a given `json_file_path`.\"\"\"\n    with open(json_file_path, \"r\", encoding=\"UTF-8\") as fh:\n        study = json.load(fh)\n\n    search_space = ps.ParameterSpace.from_dict(study[\"search_space\"])\n    objectives = [Objective(**o) for o in study[\"objectives\"]]\n    evaluations = [Evaluation(**e) for e in study[\"evaluations\"]]\n\n    return search_space, objectives, evaluations\n</code></pre>"},{"location":"reference/io/#blackboxopt.io.load_study_from_pickle","title":"<code>load_study_from_pickle(pickle_file_path)</code>","text":"<p>Load space, objectives and evaluations from a given lzma compressed pickle.</p> Source code in <code>blackboxopt/io.py</code> <pre><code>def load_study_from_pickle(\n    pickle_file_path: os.PathLike,\n) -&gt; Tuple[ps.ParameterSpace, List[Objective], List[Evaluation]]:\n    \"\"\"Load space, objectives and evaluations from a given lzma compressed pickle.\"\"\"\n    with lzma.open(pickle_file_path, \"rb\") as fh:\n        study = pickle.load(fh)\n\n    return study[\"search_space\"], study[\"objectives\"], study[\"evaluations\"]\n</code></pre>"},{"location":"reference/io/#blackboxopt.io.save_study_as_json","title":"<code>save_study_as_json(search_space, objectives, evaluations, json_file_path, overwrite=False)</code>","text":"<p>Save space, objectives and evaluations as json at <code>json_file_path</code>.</p> Source code in <code>blackboxopt/io.py</code> <pre><code>def save_study_as_json(\n    search_space: ps.ParameterSpace,\n    objectives: List[Objective],\n    evaluations: List[Evaluation],\n    json_file_path: os.PathLike,\n    overwrite: bool = False,\n):\n    \"\"\"Save space, objectives and evaluations as json at `json_file_path`.\"\"\"\n    _file_path = Path(json_file_path)\n    if not _file_path.parent.exists():\n        raise IOError(\n            f\"The parent directory for {_file_path} does not exist, please create it.\"\n        )\n    if _file_path.exists() and not overwrite:\n        raise IOError(f\"{_file_path} exists and overwrite is False\")\n\n    with open(_file_path, \"w\", encoding=\"UTF-8\") as fh:\n        json.dump(\n            {\n                \"search_space\": search_space.to_dict(),\n                \"objectives\": [o.__dict__ for o in objectives],\n                \"evaluations\": [e.__dict__ for e in evaluations],\n            },\n            fh,\n        )\n</code></pre>"},{"location":"reference/io/#blackboxopt.io.save_study_as_pickle","title":"<code>save_study_as_pickle(search_space, objectives, evaluations, pickle_file_path, overwrite=False)</code>","text":"<p>Save space, objectives and evaluations as an lzma compressed pickle.</p> Source code in <code>blackboxopt/io.py</code> <pre><code>def save_study_as_pickle(\n    search_space: ps.ParameterSpace,\n    objectives: List[Objective],\n    evaluations: List[Evaluation],\n    pickle_file_path: os.PathLike,\n    overwrite: bool = False,\n):\n    \"\"\"Save space, objectives and evaluations as an lzma compressed pickle.\"\"\"\n    _file_path = Path(pickle_file_path)\n    if not _file_path.parent.exists():\n        raise IOError(\n            f\"The parent directory for {_file_path} does not exist, please create it.\"\n        )\n    if _file_path.exists() and not overwrite:\n        raise IOError(f\"{_file_path} exists and overwrite is False\")\n\n    with lzma.open(_file_path, \"wb\") as fh:\n        pickle.dump(\n            {\n                \"search_space\": search_space,\n                \"objectives\": objectives,\n                \"evaluations\": evaluations,\n            },\n            fh,\n        )\n</code></pre>"},{"location":"reference/logger/","title":"Logger","text":""},{"location":"reference/utils/","title":"Utils","text":""},{"location":"reference/utils/#blackboxopt.utils.filter_pareto_efficient","title":"<code>filter_pareto_efficient(evaluations, objectives)</code>","text":"<p>Filter pareto efficient evaluations with respect to given objectives.</p> Source code in <code>blackboxopt/utils.py</code> <pre><code>def filter_pareto_efficient(\n    evaluations: List[Evaluation], objectives: List[Objective]\n) -&gt; List[Evaluation]:\n    \"\"\"Filter pareto efficient evaluations with respect to given objectives.\"\"\"\n    losses = np.array(\n        [\n            get_loss_vector(\n                known_objectives=objectives, reported_objectives=e.objectives\n            )\n            for e in evaluations\n        ]\n    )\n\n    pareto_efficient_mask = mask_pareto_efficient(losses)\n\n    return list(compress(evaluations, pareto_efficient_mask))\n</code></pre>"},{"location":"reference/utils/#blackboxopt.utils.get_loss_vector","title":"<code>get_loss_vector(known_objectives, reported_objectives, none_replacement=nan)</code>","text":"<p>Convert reported objectives into a vector of known objectives.</p> <p>Parameters:</p> Name Type Description Default <code>known_objectives</code> <code>Sequence[blackboxopt.base.Objective]</code> <p>A sequence of objectives with names and directions (whether greate is better). The order of the objectives dictates the order of the returned loss values.</p> required <code>reported_objectives</code> <code>Dict[str, Optional[float]]</code> <p>A dictionary with the objective value for each of the known objectives' names.</p> required <code>none_replacement</code> <code>float</code> <p>The value to use for missing objective values that are <code>None</code></p> <code>nan</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>A list of loss values.</p> Source code in <code>blackboxopt/utils.py</code> <pre><code>def get_loss_vector(\n    known_objectives: Sequence[Objective],\n    reported_objectives: Dict[str, Optional[float]],\n    none_replacement: float = float(\"NaN\"),\n) -&gt; List[float]:\n    \"\"\"Convert reported objectives into a vector of known objectives.\n\n    Args:\n        known_objectives: A sequence of objectives with names and directions\n            (whether greate is better). The order of the objectives dictates the order\n            of the returned loss values.\n        reported_objectives: A dictionary with the objective value for each of the known\n            objectives' names.\n        none_replacement: The value to use for missing objective values that are `None`\n\n    Returns:\n        A list of loss values.\n    \"\"\"\n    losses = []\n    for objective in known_objectives:\n        objective_value = reported_objectives[objective.name]\n        if objective_value is None:\n            losses.append(none_replacement)\n        elif objective.greater_is_better:\n            losses.append(-1.0 * objective_value)\n        else:\n            losses.append(objective_value)\n\n    return losses\n</code></pre>"},{"location":"reference/utils/#blackboxopt.utils.init_logger","title":"<code>init_logger(level=None)</code>","text":"<p>Initialize the default <code>blackboxopt.logger</code> as a nicely formatted stdout logger.</p> <p>Should no log level be given, the environment variable <code>BBO_LOG_LEVEL</code> is used and if that is not present, the default is <code>logging.DEBUG</code>.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>Optional[int]</code> <p>the log level to set</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>The logger instance (equivalent to <code>blackboxopt.logger</code>)</p> Source code in <code>blackboxopt/utils.py</code> <pre><code>def init_logger(level: Optional[int] = None) -&gt; logging.Logger:  # pragma: no cover\n    \"\"\"Initialize the default `blackboxopt.logger` as a nicely formatted stdout logger.\n\n    Should no log level be given, the environment variable `BBO_LOG_LEVEL` is used\n    and if that is not present, the default is `logging.DEBUG`.\n\n    Args:\n        level: the log level to set\n\n    Returns:\n        The logger instance (equivalent to `blackboxopt.logger`)\n    \"\"\"\n    if level is None:\n        level_name = os.environ.get(\"BBO_LOG_LEVEL\", \"DEBUG\")\n        level = getattr(logging, level_name)\n\n    logger.setLevel(level)\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setLevel(level)\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    return logger\n</code></pre>"},{"location":"reference/utils/#blackboxopt.utils.mask_pareto_efficient","title":"<code>mask_pareto_efficient(losses)</code>","text":"<p>For a given array of objective values where lower values are considered better and the dimensions are samples x objectives, return a mask that is <code>True</code> for all pareto efficient values.</p> <p>NOTE: The result marks multiple occurrences of the same point all as pareto efficient.</p> Source code in <code>blackboxopt/utils.py</code> <pre><code>def mask_pareto_efficient(losses: np.ndarray):\n    \"\"\"For a given array of objective values where lower values are considered better\n    and the dimensions are samples x objectives, return a mask that is `True` for all\n    pareto efficient values.\n\n    NOTE: The result marks multiple occurrences of the same point all as pareto\n    efficient.\n    \"\"\"\n    is_efficient = np.ones(losses.shape[0], dtype=bool)\n    for i, c in enumerate(losses):\n        if not is_efficient[i]:\n            continue\n\n        # Keep any point with a lower cost or when they are the same\n        efficient = np.any(losses[is_efficient] &lt; c, axis=1)\n        duplicates = np.all(losses[is_efficient] == c, axis=1)\n        is_efficient[is_efficient] = np.logical_or(efficient, duplicates)\n\n    return is_efficient\n</code></pre>"},{"location":"reference/optimization_loops/dask_distributed/","title":"Dask distributed","text":""},{"location":"reference/optimization_loops/dask_distributed/#blackboxopt.optimization_loops.dask_distributed.run_optimization_loop","title":"<code>run_optimization_loop(optimizer, evaluation_function, dask_client, timeout_s=inf, max_evaluations=None, pre_evaluation_callback=None, post_evaluation_callback=None, logger=None)</code>","text":"<p>Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached.</p> <p>This already handles signals from the optimizer in case there is no evaluation specification available yet.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer]</code> <p>The blackboxopt optimizer to run.</p> required <code>dask_client</code> <code>Client</code> <p>A Dask Distributed client that is configured with workers.</p> required <code>evaluation_function</code> <code>Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation]</code> <p>The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a <code>blackboxopt.Evaluation</code> as a result.</p> required <code>timeout_s</code> <code>float</code> <p>If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf.</p> <code>inf</code> <code>max_evaluations</code> <code>Optional[int]</code> <p>If given, the optimization loop will terminate after the given number of steps. Defaults to None.</p> <code>None</code> <code>pre_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.EvaluationSpecification], Any]]</code> <p>Reference to a callable that is invoked before each evaluation and takes a <code>blackboxopt.EvaluationSpecification</code> as an argument.</p> <code>None</code> <code>post_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.Evaluation], Any]]</code> <p>Reference to a callable that is invoked after each evaluation and takes a <code>blackboxopt.Evaluation</code> as an argument.</p> <code>None</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>The logger to use for logging progress. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[blackboxopt.evaluation.Evaluation]</code> <p>List of evluation specification and result for all evaluations.</p> Source code in <code>blackboxopt/optimization_loops/dask_distributed.py</code> <pre><code>def run_optimization_loop(\n    optimizer: Union[SingleObjectiveOptimizer, MultiObjectiveOptimizer],\n    evaluation_function: Callable[[EvaluationSpecification], Evaluation],\n    dask_client: dd.Client,\n    timeout_s: float = float(\"inf\"),\n    max_evaluations: Optional[int] = None,\n    pre_evaluation_callback: Optional[Callable[[EvaluationSpecification], Any]] = None,\n    post_evaluation_callback: Optional[Callable[[Evaluation], Any]] = None,\n    logger: Optional[logging.Logger] = None,\n) -&gt; List[Evaluation]:\n    \"\"\"Convenience wrapper for an optimization loop that uses Dask to parallelize\n    optimization until a given timeout or maximum number of evaluations is reached.\n\n    This already handles signals from the optimizer in case there is no evaluation\n    specification available yet.\n\n    Args:\n        optimizer: The blackboxopt optimizer to run.\n        dask_client: A Dask Distributed client that is configured with workers.\n        evaluation_function: The function that is called with configuration, settings\n            and optimizer info dictionaries as arguments like provided by an evaluation\n            specification.\n            This is the function that encapsulates the actual execution of\n            a parametrized experiment (e.g. ML model training) and should return a\n            `blackboxopt.Evaluation` as a result.\n        timeout_s: If given, the optimization loop will terminate after the first\n            optimization step that exceeded the timeout (in seconds). Defaults to inf.\n        max_evaluations: If given, the optimization loop will terminate after the given\n            number of steps. Defaults to None.\n        pre_evaluation_callback: Reference to a callable that is invoked before each\n            evaluation and takes a `blackboxopt.EvaluationSpecification` as an argument.\n        post_evaluation_callback: Reference to a callable that is invoked after each\n            evaluation and takes a `blackboxopt.Evaluation` as an argument.\n        logger: The logger to use for logging progress. Defaults to None.\n\n    Returns:\n        List of evluation specification and result for all evaluations.\n    \"\"\"\n    logger = logging.getLogger(\"blackboxopt\") if logger is None else logger\n\n    objectives = (\n        optimizer.objectives\n        if isinstance(optimizer, MultiObjectiveOptimizer)\n        else [optimizer.objective]\n    )\n    evaluations: List[Evaluation] = []\n\n    dask_scheduler = MinimalDaskScheduler(\n        dask_client=dask_client, objectives=objectives, logger=logger\n    )\n\n    _max_evaluations = init_max_evaluations_with_limit_logging(\n        max_evaluations=max_evaluations, timeout_s=timeout_s, logger=logger\n    )\n\n    n_eval_specs = 0\n    start = time.time()\n    while time.time() - start &lt; timeout_s and n_eval_specs &lt; _max_evaluations:\n        if dask_scheduler.has_capacity():\n            try:\n                eval_spec = optimizer.generate_evaluation_specification()\n\n                if pre_evaluation_callback:\n                    pre_evaluation_callback(eval_spec)\n\n                dask_scheduler.submit(evaluation_function, eval_spec)\n                n_eval_specs += 1\n                continue\n\n            except OptimizerNotReady:\n                logger.info(\"Optimizer is not ready yet; will retry after short pause.\")\n\n            except OptimizationComplete:\n                logger.info(\"Optimization is complete\")\n                break\n\n        new_evaluations = dask_scheduler.check_for_results(timeout_s=20)\n\n        if post_evaluation_callback:\n            list(map(post_evaluation_callback, new_evaluations))\n\n        optimizer.report(new_evaluations)\n        evaluations.extend(new_evaluations)\n\n    while dask_scheduler.has_running_jobs():\n        new_evaluations = dask_scheduler.check_for_results(timeout_s=20)\n        if post_evaluation_callback:\n            list(map(post_evaluation_callback, new_evaluations))\n        optimizer.report(new_evaluations)\n        evaluations.extend(new_evaluations)\n\n    return evaluations\n</code></pre>"},{"location":"reference/optimization_loops/file_based_distributed/","title":"File based distributed","text":""},{"location":"reference/optimization_loops/file_based_distributed/#blackboxopt.optimization_loops.file_based_distributed.evaluate_specifications","title":"<code>evaluate_specifications(target_directory, evaluation_function, objectives, timeout_s=inf, max_evaluations=None, catch_exceptions_from_evaluation_function=False, pre_evaluation_callback=None, post_evaluation_callback=None, logger=None)</code>","text":"<p>Evaluate specifications from the target directory until the <code>timeout_s</code> or <code>max_evaluations</code> is reached.</p> <p>Parameters:</p> Name Type Description Default <code>target_directory</code> <code>PathLike</code> <p>The directory where the evaluation specifications and results are stored.</p> required <code>evaluation_function</code> <code>Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation]</code> <p>The function that evaluates the evaluation specification.</p> required <code>objectives</code> <code>List[blackboxopt.base.Objective]</code> <p>The objectives reported in the evaluation function. This is used to specify None values for objectives in case of an error.</p> required <code>timeout_s</code> <code>float</code> <p>The maximum time in seconds to run the optimization loop.</p> <code>inf</code> <code>max_evaluations</code> <code>Optional[int]</code> <p>The maximum number of evaluations to perform on this agent.</p> <code>None</code> <code>catch_exceptions_from_evaluation_function</code> <code>bool</code> <p>Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's <code>stacktrace</code> attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. For more details, see the wrapper that is used internally <code>blackboxopt.optimization_loops.utils.evaluation_function_wrapper</code></p> <code>False</code> <code>pre_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.EvaluationSpecification], Any]]</code> <p>Reference to a callable that is invoked before each evaluation and takes a <code>blackboxopt.EvaluationSpecification</code> as an argument.</p> <code>None</code> <code>post_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.Evaluation], Any]]</code> <p>Reference to a callable that is invoked after each evaluation and takes a <code>blackboxopt.Evaluation</code> as an argument.</p> <code>None</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>The logger to use for logging. If <code>None</code>, the default logger is used.</p> <code>None</code> Source code in <code>blackboxopt/optimization_loops/file_based_distributed.py</code> <pre><code>def evaluate_specifications(\n    target_directory: PathLike,\n    evaluation_function: Callable[[EvaluationSpecification], Evaluation],\n    objectives: List[Objective],\n    timeout_s: float = float(\"inf\"),\n    max_evaluations: Optional[int] = None,\n    catch_exceptions_from_evaluation_function: bool = False,\n    pre_evaluation_callback: Optional[Callable[[EvaluationSpecification], Any]] = None,\n    post_evaluation_callback: Optional[Callable[[Evaluation], Any]] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"Evaluate specifications from the target directory until the `timeout_s` or\n    `max_evaluations` is reached.\n\n    Args:\n        target_directory: The directory where the evaluation specifications and results\n            are stored.\n        evaluation_function: The function that evaluates the evaluation specification.\n        objectives: The objectives reported in the evaluation function. This is used to\n            specify None values for objectives in case of an error.\n        timeout_s: The maximum time in seconds to run the optimization loop.\n        max_evaluations: The maximum number of evaluations to perform on this agent.\n        catch_exceptions_from_evaluation_function: Whether to exit on an unhandled\n            exception raised by the evaluation function or instead store their stack\n            trace in the evaluation's `stacktrace` attribute. Set to True if there are\n            spurious errors due to e.g. numerical instability that should not halt the\n            optimization loop. For more details, see the wrapper that is used internally\n            `blackboxopt.optimization_loops.utils.evaluation_function_wrapper`\n        pre_evaluation_callback: Reference to a callable that is invoked before each\n            evaluation and takes a `blackboxopt.EvaluationSpecification` as an argument.\n        post_evaluation_callback: Reference to a callable that is invoked after each\n            evaluation and takes a `blackboxopt.Evaluation` as an argument.\n        logger: The logger to use for logging. If `None`, the default logger is used.\n    \"\"\"\n    if logger is None:\n        logger = default_logger\n\n    target_directory = Path(target_directory)\n\n    _max_evaluations = init_max_evaluations_with_limit_logging(\n        max_evaluations=max_evaluations, timeout_s=timeout_s, logger=logger\n    )\n\n    start = time.time()\n    num_evaluations = 0\n    while time.time() - start &lt; timeout_s and num_evaluations &lt; _max_evaluations:\n        current_proposals = glob.glob(\n            str(target_directory / f\"{EVALUATION_SPECIFICATION_FILE_NAME_PREFIX}*.json\")\n        )\n        if not current_proposals:\n            logger.info(\"No proposals found, retrying in one second.\")\n            time.sleep(1.0)\n            continue\n\n        # Just pick one random proposal instead of iterating over all available ones, to\n        # reduce the risk of a race condition when two or more agents are running in\n        # parallel and trying to evaluate the same proposals.\n        eval_spec_path = random.choice(current_proposals)\n        try:\n            with open(eval_spec_path, \"r\", encoding=\"utf-8\") as fh:\n                eval_spec = EvaluationSpecification(**json.load(fh))\n            # Allow missing, in case the proposal was already evaluated by another agent\n        except FileNotFoundError:\n            logging.warning(\n                f\"Could not read evaluation specification from {eval_spec_path}, \"\n                + \"it was likely already evaluated elsewhere.\"\n            )\n            continue\n        Path(eval_spec_path).unlink(missing_ok=True)\n\n        logger.info(\n            \"The optimizer proposed this evaluation specification:\\n%s\",\n            pprint.pformat(eval_spec.to_dict(), compact=True),\n        )\n        if pre_evaluation_callback:\n            pre_evaluation_callback(eval_spec)\n\n        evaluation = evaluation_function_wrapper(\n            evaluation_function=evaluation_function,\n            evaluation_specification=eval_spec,\n            logger=logger,\n            objectives=objectives,\n            catch_exceptions_from_evaluation_function=catch_exceptions_from_evaluation_function,\n        )\n\n        logger.info(\n            \"Reporting this evaluation result to the optimizer:\\n%s\",\n            pprint.pformat(evaluation.to_dict(), compact=True),\n        )\n        if post_evaluation_callback:\n            post_evaluation_callback(evaluation)\n\n        with open(\n            target_directory\n            / (\n                EVALUATION_RESULT_FILE_NAME_PREFIX\n                + f\"{eval_spec.optimizer_info['eval_spec_id']}.json\"\n            ),\n            \"w\",\n            encoding=\"utf-8\",\n        ) as fh:\n            json.dump(evaluation.to_dict(), fh)\n            num_evaluations += 1\n</code></pre>"},{"location":"reference/optimization_loops/file_based_distributed/#blackboxopt.optimization_loops.file_based_distributed.run_optimization_loop","title":"<code>run_optimization_loop(optimizer, target_directory, timeout_s=inf, max_evaluations=None, proposal_queue_size=1, pre_evaluation_callback=None, post_evaluation_callback=None, logger=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer]</code> <p>The optimizer instance to use for specification generation and evaluation ingestion.</p> required <code>target_directory</code> <code>PathLike</code> <p>The directory where the evaluation specifications and results are stored.</p> required <code>timeout_s</code> <code>float</code> <p>The maximum time in seconds to run the optimization loop.</p> <code>inf</code> <code>max_evaluations</code> <code>Optional[int]</code> <p>The maximum number of evaluations to perform.</p> <code>None</code> <code>proposal_queue_size</code> <code>int</code> <p>The number of proposals to keep in the queue, i.e. the size of the evaluation batch for parallel/batch evaluation.</p> <code>1</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>The logger to use for logging. If <code>None</code>, the default logger is used.</p> <code>None</code> Source code in <code>blackboxopt/optimization_loops/file_based_distributed.py</code> <pre><code>def run_optimization_loop(\n    optimizer: Union[SingleObjectiveOptimizer, MultiObjectiveOptimizer],\n    target_directory: PathLike,\n    timeout_s: float = float(\"inf\"),\n    max_evaluations: Optional[int] = None,\n    proposal_queue_size: int = 1,\n    pre_evaluation_callback: Optional[Callable[[EvaluationSpecification], Any]] = None,\n    post_evaluation_callback: Optional[Callable[[Evaluation], Any]] = None,\n    logger: Optional[logging.Logger] = None,\n) -&gt; List[Evaluation]:\n    \"\"\"\n    Args:\n        optimizer: The optimizer instance to use for specification generation and\n            evaluation ingestion.\n        target_directory: The directory where the evaluation specifications and results\n            are stored.\n        timeout_s: The maximum time in seconds to run the optimization loop.\n        max_evaluations: The maximum number of evaluations to perform.\n        proposal_queue_size: The number of proposals to keep in the queue, i.e. the size\n            of the evaluation batch for parallel/batch evaluation.\n        logger: The logger to use for logging. If `None`, the default logger is used.\n    \"\"\"\n    if logger is None:\n        logger = default_logger\n\n    target_directory = Path(target_directory)\n    objectives = (\n        optimizer.objectives\n        if isinstance(optimizer, MultiObjectiveOptimizer)\n        else [optimizer.objective]\n    )\n    evaluations: List[Evaluation] = []\n\n    _max_evaluations = init_max_evaluations_with_limit_logging(\n        max_evaluations=max_evaluations, timeout_s=timeout_s, logger=logger\n    )\n\n    start = time.time()\n    while True:\n        evaluations_to_report = glob.glob(\n            str(target_directory / f\"{EVALUATION_RESULT_FILE_NAME_PREFIX}*.json\")\n        )\n        for eval_result_file_path in evaluations_to_report:\n            with open(eval_result_file_path, \"r\", encoding=\"utf-8\") as fh:\n                evaluation = Evaluation(**json.load(fh))\n            Path(eval_result_file_path).unlink()\n            if not evaluation.objectives and evaluation.stacktrace:\n                evaluation.objectives = {o.name: None for o in objectives}\n\n            logger.info(\n                \"Reporting this evaluation result to the optimizer:\\n%s\",\n                pprint.pformat(evaluation.to_dict(), compact=True),\n            )\n            if post_evaluation_callback:\n                post_evaluation_callback(evaluation)\n\n            optimizer.report(evaluation)\n            evaluations.append(evaluation)\n\n        if time.time() - start &gt;= timeout_s or len(evaluations) &gt;= _max_evaluations:\n            return evaluations\n\n        current_proposals = glob.glob(\n            str(target_directory / f\"{EVALUATION_SPECIFICATION_FILE_NAME_PREFIX}*.json\")\n        )\n        while len(current_proposals) &lt; proposal_queue_size:\n            eval_spec_id = str(uuid4())\n            try:\n                eval_spec = optimizer.generate_evaluation_specification()\n                eval_spec.optimizer_info[\"eval_spec_id\"] = eval_spec_id\n\n                logger.info(\n                    \"The optimizer proposed this evaluation specification:\\n%s\",\n                    pprint.pformat(eval_spec.to_dict(), compact=True),\n                )\n                if pre_evaluation_callback:\n                    pre_evaluation_callback(eval_spec)\n\n                with open(\n                    target_directory / f\"eval_spec_{eval_spec_id}.json\",\n                    \"w\",\n                    encoding=\"utf-8\",\n                ) as fh:\n                    json.dump(eval_spec.to_dict(), fh)\n                current_proposals = glob.glob(\n                    str(target_directory / \"eval_spec_*.json\")\n                )\n            except OptimizerNotReady:\n                logger.info(\"Optimizer is not ready yet, retrying in two seconds\")\n                time.sleep(2.0)\n            except OptimizationComplete:\n                logger.info(\"Optimization is complete\")\n                return evaluations\n\n        time.sleep(0.5)\n</code></pre>"},{"location":"reference/optimization_loops/sequential/","title":"Sequential","text":""},{"location":"reference/optimization_loops/sequential/#blackboxopt.optimization_loops.sequential.run_optimization_loop","title":"<code>run_optimization_loop(optimizer, evaluation_function, timeout_s=inf, max_evaluations=None, catch_exceptions_from_evaluation_function=False, pre_evaluation_callback=None, post_evaluation_callback=None, logger=None)</code>","text":"<p>Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached.</p> <p>This already handles signals from the optimizer in case there is no evaluation specification available yet.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer]</code> <p>The blackboxopt optimizer to run.</p> required <code>evaluation_function</code> <code>Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation]</code> <p>The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a <code>blackboxopt.Evaluation</code> as a result.</p> required <code>timeout_s</code> <code>float</code> <p>If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf.</p> <code>inf</code> <code>max_evaluations</code> <code>Optional[int]</code> <p>If given, the optimization loop will terminate after the given number of steps.</p> <code>None</code> <code>catch_exceptions_from_evaluation_function</code> <code>bool</code> <p>Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's <code>stacktrace</code> attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. For more details, see the wrapper that is used internally <code>blackboxopt.optimization_loops.utils.evaluation_function_wrapper</code></p> <code>False</code> <code>pre_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.EvaluationSpecification], Any]]</code> <p>Reference to a callable that is invoked before each evaluation and takes a <code>blackboxopt.EvaluationSpecification</code> as an argument.</p> <code>None</code> <code>post_evaluation_callback</code> <code>Optional[Callable[[blackboxopt.evaluation.Evaluation], Any]]</code> <p>Reference to a callable that is invoked after each evaluation and takes a <code>blackboxopt.Evaluation</code> as an argument.</p> <code>None</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>The logger to use for logging progress. Default: <code>blackboxopt.logger</code></p> <code>None</code> <p>Returns:</p> Type Description <code>List[blackboxopt.evaluation.Evaluation]</code> <p>List of evaluation specification and result for all evaluations.</p> Source code in <code>blackboxopt/optimization_loops/sequential.py</code> <pre><code>def run_optimization_loop(\n    optimizer: Union[SingleObjectiveOptimizer, MultiObjectiveOptimizer],\n    evaluation_function: Callable[[EvaluationSpecification], Evaluation],\n    timeout_s: float = float(\"inf\"),\n    max_evaluations: Optional[int] = None,\n    catch_exceptions_from_evaluation_function: bool = False,\n    pre_evaluation_callback: Optional[Callable[[EvaluationSpecification], Any]] = None,\n    post_evaluation_callback: Optional[Callable[[Evaluation], Any]] = None,\n    logger: Optional[logging.Logger] = None,\n) -&gt; List[Evaluation]:\n    \"\"\"Convenience wrapper for an optimization loop that sequentially fetches evaluation\n    specifications until a given timeout or maximum number of evaluations is reached.\n\n    This already handles signals from the optimizer in case there is no evaluation\n    specification available yet.\n\n    Args:\n        optimizer: The blackboxopt optimizer to run.\n        evaluation_function: The function that is called with configuration, settings\n            and optimizer info dictionaries as arguments like provided by an evaluation\n            specification.\n            This is the function that encapsulates the actual execution of\n            a parametrized experiment (e.g. ML model training) and should return a\n            `blackboxopt.Evaluation` as a result.\n        timeout_s: If given, the optimization loop will terminate after the first\n            optimization step that exceeded the timeout (in seconds). Defaults to inf.\n        max_evaluations: If given, the optimization loop will terminate after the given\n            number of steps.\n        catch_exceptions_from_evaluation_function: Whether to exit on an unhandled\n            exception raised by the evaluation function or instead store their stack\n            trace in the evaluation's `stacktrace` attribute. Set to True if there are\n            spurious errors due to e.g. numerical instability that should not halt the\n            optimization loop. For more details, see the wrapper that is used internally\n            `blackboxopt.optimization_loops.utils.evaluation_function_wrapper`\n        pre_evaluation_callback: Reference to a callable that is invoked before each\n            evaluation and takes a `blackboxopt.EvaluationSpecification` as an argument.\n        post_evaluation_callback: Reference to a callable that is invoked after each\n            evaluation and takes a `blackboxopt.Evaluation` as an argument.\n        logger: The logger to use for logging progress. Default: `blackboxopt.logger`\n\n    Returns:\n        List of evaluation specification and result for all evaluations.\n    \"\"\"\n    if logger is None:\n        logger = default_logger\n\n    objectives = (\n        optimizer.objectives\n        if isinstance(optimizer, MultiObjectiveOptimizer)\n        else [optimizer.objective]\n    )\n    evaluations: List[Evaluation] = []\n\n    _max_evaluations = init_max_evaluations_with_limit_logging(\n        max_evaluations=max_evaluations, timeout_s=timeout_s, logger=logger\n    )\n\n    start = time.time()\n    num_evaluations = 0\n    while time.time() - start &lt; timeout_s and num_evaluations &lt; _max_evaluations:\n        num_evaluations += 1\n\n        try:\n            evaluation_specification = optimizer.generate_evaluation_specification()\n\n            logger.info(\n                \"Starting evaluation %s (of %s). \"\n                \"Overall runtime so far: %s (timeout at %s)\",\n                num_evaluations,\n                _max_evaluations,\n                timedelta(seconds=int(time.time() - start)),\n                timedelta(seconds=timeout_s) if timeout_s != float(\"inf\") else \"inf\",\n            )\n\n            logger.info(\n                \"The optimizer proposed the following evaluation specification:\\n%s\",\n                pprint.pformat(evaluation_specification.to_dict(), compact=True),\n            )\n            if pre_evaluation_callback:\n                pre_evaluation_callback(evaluation_specification)\n\n            evaluation = evaluation_function_wrapper(\n                evaluation_function=evaluation_function,\n                evaluation_specification=evaluation_specification,\n                logger=logger,\n                objectives=objectives,\n                catch_exceptions_from_evaluation_function=catch_exceptions_from_evaluation_function,\n            )\n\n            logger.info(\n                \"Reporting the following evaluation result to the optimizer:\\n%s\",\n                pprint.pformat(evaluation.to_dict(), compact=True),\n            )\n            if post_evaluation_callback:\n                post_evaluation_callback(evaluation)\n\n            optimizer.report(evaluation)\n            evaluations.append(evaluation)\n\n        except OptimizerNotReady:\n            logger.info(\"Optimizer is not ready yet, retrying in two seconds\")\n            time.sleep(2)\n            continue\n\n        except OptimizationComplete:\n            logger.info(\"Optimization is complete\")\n            return evaluations\n\n    logger.info(\"Aborting optimization due to specified maximum evaluations or timeout\")\n    return evaluations\n</code></pre>"},{"location":"reference/optimization_loops/testing/","title":"Testing","text":""},{"location":"reference/optimization_loops/utils/","title":"Utils","text":""},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils.EvaluationFunctionError","title":"<code> EvaluationFunctionError            (ValueError)         </code>","text":"<p>Raised on errors originating from the user defined evaluation function.</p> Source code in <code>blackboxopt/optimization_loops/utils.py</code> <pre><code>class EvaluationFunctionError(ValueError):\n    \"\"\"Raised on errors originating from the user defined evaluation function.\"\"\"\n\n    def __init__(self, evaluation_specification: EvaluationSpecification):\n        self.message = (\n            \"An error occurred when attempting to call the user specified evaluation \"\n            \"function with the specification below. Please check the cause of this \"\n            \"exception in the output further up for the original stacktrace.\\n\"\n            f\"{evaluation_specification}\"\n        )\n        self.evaluation_specification = evaluation_specification\n</code></pre>"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils.evaluation_function_wrapper","title":"<code>evaluation_function_wrapper(evaluation_function, evaluation_specification, objectives, catch_exceptions_from_evaluation_function, logger)</code>","text":"<p>Wrapper for evaluation functions. The evaluation result returned by the evaluation function is checked to contain all relevant objectives. An empty evaluation with a stacktrace is reported to the optimizer in case an unhandled Exception occurrs during the evaluation function call when <code>catch_exceptions_from_evaluation_function</code> is set to <code>True</code>, otherwise an <code>EvaluationFunctionError</code> is raised based on the original exception.</p> Source code in <code>blackboxopt/optimization_loops/utils.py</code> <pre><code>def evaluation_function_wrapper(\n    evaluation_function: Callable[[EvaluationSpecification], Evaluation],\n    evaluation_specification: EvaluationSpecification,\n    objectives: List[Objective],\n    catch_exceptions_from_evaluation_function: bool,\n    logger: logging.Logger,\n) -&gt; Evaluation:\n    \"\"\"Wrapper for evaluation functions. The evaluation result returned by the\n    evaluation function is checked to contain all relevant objectives. An empty\n    evaluation with a stacktrace is reported to the optimizer in case an unhandled\n    Exception occurrs during the evaluation function call when\n    `catch_exceptions_from_evaluation_function` is set to `True`, otherwise an\n    `EvaluationFunctionError` is raised based on the original exception.\n    \"\"\"\n    try:\n        evaluation = evaluation_function(evaluation_specification)\n    except Exception as e:\n        if not catch_exceptions_from_evaluation_function:\n            raise EvaluationFunctionError(evaluation_specification) from e\n\n        stacktrace = traceback.format_exc()\n\n        logger.warning(\n            \"Reporting FAILURE due to unhandled error in evaluation function. See \"\n            + \"DEBUG log level output or evaluation.stacktrace for details. \"\n            + \"Alternatively, disable automated exception handling by setting \"\n            + \"catch_exceptions_from_evaluation_function=False to exit on errors.\"\n        )\n        logger.debug(stacktrace)\n\n        evaluation = evaluation_specification.create_evaluation(\n            stacktrace=stacktrace, objectives={o.name: None for o in objectives}\n        )\n\n    raise_on_unknown_or_incomplete(\n        exception=ObjectivesError,\n        known=[o.name for o in objectives],\n        reported=evaluation.objectives.keys(),\n    )\n\n    return evaluation\n</code></pre>"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils.init_max_evaluations_with_limit_logging","title":"<code>init_max_evaluations_with_limit_logging(timeout_s, logger, max_evaluations=None)</code>","text":"<p>[summary]</p> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>float</code> <p>[description]</p> required <code>logger</code> <code>Logger</code> <p>[description]</p> required <code>max_evaluations</code> <code>Optional[int]</code> <p>[description]</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>[description]</p> Source code in <code>blackboxopt/optimization_loops/utils.py</code> <pre><code>def init_max_evaluations_with_limit_logging(\n    timeout_s: float, logger: logging.Logger, max_evaluations: Optional[int] = None\n) -&gt; float:\n    \"\"\"[summary]\n\n    Args:\n        timeout_s: [description]\n        logger: [description]\n        max_evaluations: [description]\n\n    Returns:\n        [description]\n    \"\"\"\n    if max_evaluations:\n        logger.info(\n            \"Starting optimization run. Stops when complete or \"\n            + f\"{max_evaluations} evaluations reached.\"\n        )\n        return float(max_evaluations)\n\n    if timeout_s == float(\"inf\"):\n        logger.info(\"Starting optimization run. Stops when complete.\")\n    else:\n        timeout_pretty = datetime.timedelta(seconds=timeout_s)\n        logger.info(\n            \"Starting optimization run. Stops when complete or \"\n            + f\"{timeout_pretty} passed.\"\n        )\n\n    return float(\"inf\")\n</code></pre>"},{"location":"reference/optimizers/bohb/","title":"Bohb","text":""},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB","title":"<code> BOHB            (StagedIterationOptimizer)         </code>","text":"Source code in <code>blackboxopt/optimizers/bohb.py</code> <pre><code>class BOHB(StagedIterationOptimizer):\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objective: Objective,\n        min_fidelity: float,\n        max_fidelity: float,\n        num_iterations: int,\n        eta: float = 3.0,\n        top_n_percent: int = 15,\n        min_samples_in_model: Optional[int] = None,\n        num_samples: int = 64,\n        random_fraction: float = 1 / 3,\n        bandwidth_factor: float = 3.0,\n        min_bandwidth: float = 1e-3,\n        seed: Optional[int] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"BOHB Optimizer.\n\n        BOHB performs robust and efficient hyperparameter optimization\n        at scale by combining the speed of Hyperband searches with the\n        guidance and guarantees of convergence of Bayesian\n        Optimization. Instead of sampling new configurations at random,\n        BOHB uses kernel density estimators to select promising candidates.\n\n        For reference:\n        ```\n        @InProceedings{falkner-icml-18,\n            title =     {{BOHB}: Robust and Efficient Hyperparameter Optimization at\n                Scale},\n            author =    {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n            booktitle = {Proceedings of the 35th International Conference on Machine\n                Learning},\n            pages =     {1436--1445},\n            year =      {2018},\n        }\n        ```\n\n        Args:\n            search_space: [description]\n            objective: [description]\n            min_fidelity: The smallest fidelity value that is still meaningful.\n                Must be strictly greater than zero!\n            max_fidelity: The largest fidelity value used during the optimization.\n                Must not be smaller than `min_fidelity`.\n            num_iterations: The number of iterations that the optimizer will run.\n            eta: Scaling parameter to control the aggressiveness of Hyperband's racing.\n            top_n_percent: Determines the percentile of configurations that will be\n                used as training data for the kernel density estimator of the good\n                configuration, e.g if set to 10 the best 10% configurations will be\n                considered for training.\n            min_samples_in_model: Minimum number of datapoints needed to fit a model.\n            num_samples: Number of samples drawn to optimize EI via sampling.\n            random_fraction: Fraction of random configurations returned.\n            bandwidth_factor: Widens the bandwidth for contiuous parameters for\n                proposed points to optimize EI\n            min_bandwidth: to keep diversity, even when all (good) samples have the\n                same value for one of the parameters, a minimum bandwidth\n                (reasonable default: 1e-3) is used instead of zero.\n            seed: [description]\n            logger: [description]\n        \"\"\"\n        if min_samples_in_model is None:\n            min_samples_in_model = 3 * len(search_space)\n\n        self.min_fidelity = min_fidelity\n        self.max_fidelity = max_fidelity\n        self.eta = eta\n\n        self.config_sampler = BOHBSampler(\n            search_space=search_space,\n            objective=objective,\n            min_samples_in_model=min_samples_in_model,\n            top_n_percent=top_n_percent,\n            num_samples=num_samples,\n            random_fraction=random_fraction,\n            bandwidth_factor=bandwidth_factor,\n            min_bandwidth=min_bandwidth,\n            seed=seed,\n        )\n\n        super().__init__(\n            search_space=search_space,\n            objective=objective,\n            num_iterations=num_iterations,\n            seed=seed,\n            logger=logger,\n        )\n\n    def _create_new_iteration(self, iteration_index):\n        \"\"\"Optimizer specific way to create a new\n        `blackboxopt.optimizer.utils.staged_iteration.StagedIteration` object\n        \"\"\"\n        return create_hyperband_iteration(\n            iteration_index,\n            self.min_fidelity,\n            self.max_fidelity,\n            self.eta,\n            self.config_sampler,\n            self.objective,\n            self.logger,\n        )\n</code></pre>"},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB.__init__","title":"<code>__init__(self, search_space, objective, min_fidelity, max_fidelity, num_iterations, eta=3.0, top_n_percent=15, min_samples_in_model=None, num_samples=64, random_fraction=0.3333333333333333, bandwidth_factor=3.0, min_bandwidth=0.001, seed=None, logger=None)</code>  <code>special</code>","text":"<p>BOHB Optimizer.</p> <p>BOHB performs robust and efficient hyperparameter optimization at scale by combining the speed of Hyperband searches with the guidance and guarantees of convergence of Bayesian Optimization. Instead of sampling new configurations at random, BOHB uses kernel density estimators to select promising candidates.</p> <p>For reference: <pre><code>@InProceedings{falkner-icml-18,\n    title =     {{BOHB}: Robust and Efficient Hyperparameter Optimization at\n        Scale},\n    author =    {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n    booktitle = {Proceedings of the 35th International Conference on Machine\n        Learning},\n    pages =     {1436--1445},\n    year =      {2018},\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>[description]</p> required <code>objective</code> <code>Objective</code> <p>[description]</p> required <code>min_fidelity</code> <code>float</code> <p>The smallest fidelity value that is still meaningful. Must be strictly greater than zero!</p> required <code>max_fidelity</code> <code>float</code> <p>The largest fidelity value used during the optimization. Must not be smaller than <code>min_fidelity</code>.</p> required <code>num_iterations</code> <code>int</code> <p>The number of iterations that the optimizer will run.</p> required <code>eta</code> <code>float</code> <p>Scaling parameter to control the aggressiveness of Hyperband's racing.</p> <code>3.0</code> <code>top_n_percent</code> <code>int</code> <p>Determines the percentile of configurations that will be used as training data for the kernel density estimator of the good configuration, e.g if set to 10 the best 10% configurations will be considered for training.</p> <code>15</code> <code>min_samples_in_model</code> <code>Optional[int]</code> <p>Minimum number of datapoints needed to fit a model.</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>Number of samples drawn to optimize EI via sampling.</p> <code>64</code> <code>random_fraction</code> <code>float</code> <p>Fraction of random configurations returned.</p> <code>0.3333333333333333</code> <code>bandwidth_factor</code> <code>float</code> <p>Widens the bandwidth for contiuous parameters for proposed points to optimize EI</p> <code>3.0</code> <code>min_bandwidth</code> <code>float</code> <p>to keep diversity, even when all (good) samples have the same value for one of the parameters, a minimum bandwidth (reasonable default: 1e-3) is used instead of zero.</p> <code>0.001</code> <code>seed</code> <code>Optional[int]</code> <p>[description]</p> <code>None</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>[description]</p> <code>None</code> Source code in <code>blackboxopt/optimizers/bohb.py</code> <pre><code>def __init__(\n    self,\n    search_space: ParameterSpace,\n    objective: Objective,\n    min_fidelity: float,\n    max_fidelity: float,\n    num_iterations: int,\n    eta: float = 3.0,\n    top_n_percent: int = 15,\n    min_samples_in_model: Optional[int] = None,\n    num_samples: int = 64,\n    random_fraction: float = 1 / 3,\n    bandwidth_factor: float = 3.0,\n    min_bandwidth: float = 1e-3,\n    seed: Optional[int] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"BOHB Optimizer.\n\n    BOHB performs robust and efficient hyperparameter optimization\n    at scale by combining the speed of Hyperband searches with the\n    guidance and guarantees of convergence of Bayesian\n    Optimization. Instead of sampling new configurations at random,\n    BOHB uses kernel density estimators to select promising candidates.\n\n    For reference:\n    ```\n    @InProceedings{falkner-icml-18,\n        title =     {{BOHB}: Robust and Efficient Hyperparameter Optimization at\n            Scale},\n        author =    {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n        booktitle = {Proceedings of the 35th International Conference on Machine\n            Learning},\n        pages =     {1436--1445},\n        year =      {2018},\n    }\n    ```\n\n    Args:\n        search_space: [description]\n        objective: [description]\n        min_fidelity: The smallest fidelity value that is still meaningful.\n            Must be strictly greater than zero!\n        max_fidelity: The largest fidelity value used during the optimization.\n            Must not be smaller than `min_fidelity`.\n        num_iterations: The number of iterations that the optimizer will run.\n        eta: Scaling parameter to control the aggressiveness of Hyperband's racing.\n        top_n_percent: Determines the percentile of configurations that will be\n            used as training data for the kernel density estimator of the good\n            configuration, e.g if set to 10 the best 10% configurations will be\n            considered for training.\n        min_samples_in_model: Minimum number of datapoints needed to fit a model.\n        num_samples: Number of samples drawn to optimize EI via sampling.\n        random_fraction: Fraction of random configurations returned.\n        bandwidth_factor: Widens the bandwidth for contiuous parameters for\n            proposed points to optimize EI\n        min_bandwidth: to keep diversity, even when all (good) samples have the\n            same value for one of the parameters, a minimum bandwidth\n            (reasonable default: 1e-3) is used instead of zero.\n        seed: [description]\n        logger: [description]\n    \"\"\"\n    if min_samples_in_model is None:\n        min_samples_in_model = 3 * len(search_space)\n\n    self.min_fidelity = min_fidelity\n    self.max_fidelity = max_fidelity\n    self.eta = eta\n\n    self.config_sampler = BOHBSampler(\n        search_space=search_space,\n        objective=objective,\n        min_samples_in_model=min_samples_in_model,\n        top_n_percent=top_n_percent,\n        num_samples=num_samples,\n        random_fraction=random_fraction,\n        bandwidth_factor=bandwidth_factor,\n        min_bandwidth=min_bandwidth,\n        seed=seed,\n    )\n\n    super().__init__(\n        search_space=search_space,\n        objective=objective,\n        num_iterations=num_iterations,\n        seed=seed,\n        logger=logger,\n    )\n</code></pre>"},{"location":"reference/optimizers/botorch_base/","title":"Botorch base","text":""},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer","title":"<code> SingleObjectiveBOTorchOptimizer            (SingleObjectiveOptimizer)         </code>","text":"Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>class SingleObjectiveBOTorchOptimizer(SingleObjectiveOptimizer):\n    def __init__(\n        self,\n        search_space: ps.ParameterSpace,\n        objective: Objective,\n        model: Model,\n        acquisition_function_factory: Callable[[Model], AcquisitionFunction],\n        af_optimizer_kwargs=None,\n        num_initial_random_samples: int = 1,\n        max_pending_evaluations: Optional[int] = 1,\n        batch_shape: torch.Size = torch.Size(),\n        logger: Optional[logging.Logger] = None,\n        seed: Optional[int] = None,\n        torch_dtype: torch.dtype = torch.float64,\n    ):\n        \"\"\"Single objective BO optimizer that uses as a surrogate model the `model`\n        object provided by user.\n\n        The `model` is expected to be extended from BoTorch base model `Model` class,\n        and does not require to be a GP model.\n\n        Args:\n            search_space: The space in which to optimize.\n            objective: The objective to optimize.\n            model: Surrogate model of `Model` type.\n            acquisition_function_factory: Callable that produces an acquisition function\n                instance, could also be a compatible acquisition function class.\n                Only acquisition functions to be minimized are supported.\n                Providing a partially initialized class is possible with, e.g.\n                `functools.partial(UpperConfidenceBound, beta=6.0, maximize=False)`.\n            af_optimizer_kwargs: Settings for acquisition function optimizer,\n                see `botorch.optim.optimize_acqf` and in case the whole search space\n                is discrete: `botorch.optim.optimize_acqf_discrete`. The former can be\n                enforced by providing `raw_samples` or `num_restarts`, the latter by\n                providing `num_random_choices`.\n            num_initial_random_samples: Size of the initial space-filling design that\n                is used before starting BO. The points are sampled randomly in the\n                search space. If no random sampling is required, set it to 0.\n                When random sampling is enabled, but evaluations with missing objective\n                values are reported, more specifications are sampled until\n                `num_initial_random_samples` many valid evaluations were reported.\n            max_pending_evaluations: Maximum number of parallel evaluations. For\n                sequential BO use the default value of 1. If no limit is required,\n                set it to None.\n            batch_shape: Batch dimension(s) used for batched models.\n            logger: Custom logger.\n            seed: A seed to make the optimization reproducible.\n            torch_dtype: Torch data type used for storing the data. This needs to match\n                the dtype of the model used\n        \"\"\"\n        super().__init__(search_space=search_space, objective=objective, seed=seed)\n        self.num_initial_random = num_initial_random_samples\n        self.max_pending_evaluations = max_pending_evaluations\n        self.batch_shape = batch_shape\n        self.logger = logger or logging.getLogger(\"blackboxopt\")\n\n        self.torch_dtype = torch_dtype\n        self.X = torch.empty(\n            (*self.batch_shape, 0, len(search_space)), dtype=torch_dtype\n        )\n        self.losses = torch.empty((*self.batch_shape, 0, 1), dtype=torch_dtype)\n        self.pending_specifications: Dict[int, EvaluationSpecification] = {}\n        if seed is not None:\n            torch.manual_seed(seed=seed)\n\n        self.model = model\n        self.acquisition_function_factory = acquisition_function_factory\n        self.af_optimizer_kwargs = af_optimizer_kwargs\n\n    def _create_fantasy_model(self, model: Model) -&gt; Model:\n        \"\"\"Create model with the pending specifications and model based\n        outcomes added to the training data.\"\"\"\n\n        if not self.pending_specifications:\n            # nothing to do when there are no pending specs\n            return model\n\n        pending_X = torch.tensor(\n            np.array(\n                [\n                    self.search_space.to_numerical(e.configuration)\n                    for e in self.pending_specifications.values()\n                ]\n            ),\n            dtype=self.torch_dtype,\n        )\n\n        model = model.fantasize(pending_X, IIDNormalSampler(1), observation_noise=False)\n\n        if isinstance(model, ExactGP):\n            # ExactGP.fantasize extends model's X and Y with batch_size, even if\n            # originally not given -&gt; need to reshape these to their original\n            # representation\n            n_samples = model.train_targets.size(-1)\n            n_features = len(self.search_space)\n            model.train_inputs[0] = model.train_inputs[0].reshape(\n                torch.Size((*self.batch_shape, n_samples, n_features))\n            )\n            model.train_targets = model.train_targets.reshape(\n                torch.Size((*self.batch_shape, n_samples, 1))\n            )\n        return model\n\n    def _generate_evaluation_specification(self):\n        \"\"\"Optimize acquisition on fantasy model to pick next point.\"\"\"\n        fantasy_model = self._create_fantasy_model(self.model)\n        fantasy_model.eval()\n\n        af = self.acquisition_function_factory(fantasy_model)\n        if getattr(af, \"maximize\", False):\n            raise ValueError(\n                \"Only acquisition functions that need to be minimized are supported. \"\n                f\"The given {af.__class__.__name__} has maximize=True. \"\n                \"One potential fix is using functools.partial(\"\n                f\"{af.__class__.__name__}, maximize=False) as the \"\n                \"acquisition_function_factory init argument.\"\n            )\n\n        acquisition_function_optimizer = _acquisition_function_optimizer_factory(\n            search_space=self.search_space,\n            af_opt_kwargs=self.af_optimizer_kwargs,\n            torch_dtype=self.torch_dtype,\n        )\n        configuration, _ = acquisition_function_optimizer(af)\n\n        return EvaluationSpecification(\n            configuration=self.search_space.from_numerical(configuration[0].numpy()),\n        )\n\n    def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n        \"\"\"Call the optimizer specific function and append a unique integer id\n        to the specification.\n\n        Please refer to the docstring of\n        `blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification`\n        for a description of the method.\n        \"\"\"\n        if (\n            self.max_pending_evaluations\n            and len(self.pending_specifications) == self.max_pending_evaluations\n        ):\n            raise OptimizerNotReady\n\n        # Generate random samples until there are enough samples where at least one of\n        # the objective values is available\n        if self.num_initial_random &gt; 0 and (\n            sum(~torch.any(self.losses.isnan(), dim=1)) &lt; self.num_initial_random\n        ):\n            eval_spec = EvaluationSpecification(\n                configuration=self.search_space.sample(),\n                optimizer_info={\"model_based_pick\": False},\n            )\n        else:\n            eval_spec = self._generate_evaluation_specification()\n            eval_spec.optimizer_info[\"model_based_pick\"] = True\n\n        eval_id = self.X.size(-2) + len(self.pending_specifications)\n        eval_spec.optimizer_info[\"evaluation_id\"] = eval_id\n        self.pending_specifications[eval_id] = eval_spec\n        return eval_spec\n\n    def _remove_pending_specifications(\n        self, evaluations: Union[Evaluation, Iterable[Evaluation]]\n    ):\n        \"\"\"Find and remove the corresponding entries in `self.pending_specifications`.\n\n        Args:\n            evaluations: List of completed evaluations.\n        Raises:\n            ValueError: If an evaluation is reported with an ID that was not issued\n            by the optimizer, the method will fail.\n        \"\"\"\n        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n        for e in _evals:\n            if \"evaluation_id\" not in e.optimizer_info:\n                self.logger.debug(\"User provided EvaluationSpecification received.\")\n                continue\n\n            if e.optimizer_info[\"evaluation_id\"] not in self.pending_specifications:\n                msg = (\n                    \"Unknown evaluation_id reported. This could indicate that the \"\n                    \"evaluation has been reported before!\"\n                )\n                self.logger.error(msg)\n                raise ValueError(msg)\n\n            del self.pending_specifications[e.optimizer_info[\"evaluation_id\"]]\n\n    def _append_evaluations_to_data(\n        self, evaluations: Union[Evaluation, Iterable[Evaluation]]\n    ):\n        \"\"\"Convert the reported evaluation into its numerical representation\n        and append it to the training data.\n\n        Args:\n            evaluations: List of completed evaluations.\n        \"\"\"\n        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n        X, Y = to_numerical(\n            _evals,\n            self.search_space,\n            objectives=[self.objective],\n            batch_shape=self.batch_shape,\n            torch_dtype=self.torch_dtype,\n        )\n\n        # fill in NaNs originating from inactive parameters (conditional spaces support)\n        # botorch expect numerical representation of inputs to be within the unit\n        # hypercube, thus we can't use the default c=-1.0\n        X = impute_nans_with_constant(X, c=0.0)\n\n        self.logger.debug(f\"Next training configuration(s):{X}, {Y}\")\n\n        self.X = torch.cat([self.X, X], dim=-2)\n        self.losses = torch.cat([self.losses, Y], dim=-2)\n\n    def _update_internal_evaluation_data(\n        self, evaluations: Iterable[Evaluation]\n    ) -&gt; None:\n        \"\"\"Check validity of the evaluations and do optimizer agnostic bookkeeping.\"\"\"\n        call_functions_with_evaluations_and_collect_errors(\n            [\n                functools.partial(validate_objectives, objectives=[self.objective]),\n                self._remove_pending_specifications,\n                self._append_evaluations_to_data,\n            ],\n            sort_evaluations(evaluations),\n        )\n\n    def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n        \"\"\"A simple report method that conditions the model on data.\n        This likely needs to be overridden for more specific BO implementations.\n        \"\"\"\n        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n        self._update_internal_evaluation_data(_evals)\n        # Just for populating all relevant caches\n        self.model.posterior(self.X)\n\n        x_filtered, y_filtered = filter_y_nans(self.X, self.losses)\n\n        # The actual model update\n        # Ignore BotorchTensorDimensionWarning which is always reported to make the user\n        # aware that they are reponsible for the right input Tensors dimensionality.\n        with warnings.catch_warnings():\n            warnings.simplefilter(\n                action=\"ignore\", category=BotorchTensorDimensionWarning\n            )\n            self.model = self.model.condition_on_observations(x_filtered, y_filtered)\n\n    def predict_model_based_best(self) -&gt; Optional[Evaluation]:\n        \"\"\"Get the current configuration that is estimated to be the best (in terms of\n        optimal objective value) without waiting for a reported evaluation of that\n        configuration. Instead, the objective value estimation relies on BO's\n        underlying model.\n\n        This might return `None` in case there is no successfully evaluated\n        configuration yet (thus, the optimizer has not been given training data yet).\n\n        Returns:\n            blackboxopt.evaluation.Evaluation\n                The evaluated specification containing the estimated best configuration\n                or `None` in case no evaluations have been reported yet.\n        \"\"\"\n        return predict_model_based_best(\n            model=self.model,\n            objective=self.objective,\n            search_space=self.search_space,\n            torch_dtype=self.torch_dtype,\n        )\n</code></pre>"},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.__init__","title":"<code>__init__(self, search_space, objective, model, acquisition_function_factory, af_optimizer_kwargs=None, num_initial_random_samples=1, max_pending_evaluations=1, batch_shape=torch.Size([]), logger=None, seed=None, torch_dtype=torch.float64)</code>  <code>special</code>","text":"<p>Single objective BO optimizer that uses as a surrogate model the <code>model</code> object provided by user.</p> <p>The <code>model</code> is expected to be extended from BoTorch base model <code>Model</code> class, and does not require to be a GP model.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>The space in which to optimize.</p> required <code>objective</code> <code>Objective</code> <p>The objective to optimize.</p> required <code>model</code> <code>Model</code> <p>Surrogate model of <code>Model</code> type.</p> required <code>acquisition_function_factory</code> <code>Callable[[botorch.models.model.Model], botorch.acquisition.acquisition.AcquisitionFunction]</code> <p>Callable that produces an acquisition function instance, could also be a compatible acquisition function class. Only acquisition functions to be minimized are supported. Providing a partially initialized class is possible with, e.g. <code>functools.partial(UpperConfidenceBound, beta=6.0, maximize=False)</code>.</p> required <code>af_optimizer_kwargs</code> <p>Settings for acquisition function optimizer, see <code>botorch.optim.optimize_acqf</code> and in case the whole search space is discrete: <code>botorch.optim.optimize_acqf_discrete</code>. The former can be enforced by providing <code>raw_samples</code> or <code>num_restarts</code>, the latter by providing <code>num_random_choices</code>.</p> <code>None</code> <code>num_initial_random_samples</code> <code>int</code> <p>Size of the initial space-filling design that is used before starting BO. The points are sampled randomly in the search space. If no random sampling is required, set it to 0. When random sampling is enabled, but evaluations with missing objective values are reported, more specifications are sampled until <code>num_initial_random_samples</code> many valid evaluations were reported.</p> <code>1</code> <code>max_pending_evaluations</code> <code>Optional[int]</code> <p>Maximum number of parallel evaluations. For sequential BO use the default value of 1. If no limit is required, set it to None.</p> <code>1</code> <code>batch_shape</code> <code>Size</code> <p>Batch dimension(s) used for batched models.</p> <code>torch.Size([])</code> <code>logger</code> <code>Optional[logging.Logger]</code> <p>Custom logger.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>A seed to make the optimization reproducible.</p> <code>None</code> <code>torch_dtype</code> <code>dtype</code> <p>Torch data type used for storing the data. This needs to match the dtype of the model used</p> <code>torch.float64</code> Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>def __init__(\n    self,\n    search_space: ps.ParameterSpace,\n    objective: Objective,\n    model: Model,\n    acquisition_function_factory: Callable[[Model], AcquisitionFunction],\n    af_optimizer_kwargs=None,\n    num_initial_random_samples: int = 1,\n    max_pending_evaluations: Optional[int] = 1,\n    batch_shape: torch.Size = torch.Size(),\n    logger: Optional[logging.Logger] = None,\n    seed: Optional[int] = None,\n    torch_dtype: torch.dtype = torch.float64,\n):\n    \"\"\"Single objective BO optimizer that uses as a surrogate model the `model`\n    object provided by user.\n\n    The `model` is expected to be extended from BoTorch base model `Model` class,\n    and does not require to be a GP model.\n\n    Args:\n        search_space: The space in which to optimize.\n        objective: The objective to optimize.\n        model: Surrogate model of `Model` type.\n        acquisition_function_factory: Callable that produces an acquisition function\n            instance, could also be a compatible acquisition function class.\n            Only acquisition functions to be minimized are supported.\n            Providing a partially initialized class is possible with, e.g.\n            `functools.partial(UpperConfidenceBound, beta=6.0, maximize=False)`.\n        af_optimizer_kwargs: Settings for acquisition function optimizer,\n            see `botorch.optim.optimize_acqf` and in case the whole search space\n            is discrete: `botorch.optim.optimize_acqf_discrete`. The former can be\n            enforced by providing `raw_samples` or `num_restarts`, the latter by\n            providing `num_random_choices`.\n        num_initial_random_samples: Size of the initial space-filling design that\n            is used before starting BO. The points are sampled randomly in the\n            search space. If no random sampling is required, set it to 0.\n            When random sampling is enabled, but evaluations with missing objective\n            values are reported, more specifications are sampled until\n            `num_initial_random_samples` many valid evaluations were reported.\n        max_pending_evaluations: Maximum number of parallel evaluations. For\n            sequential BO use the default value of 1. If no limit is required,\n            set it to None.\n        batch_shape: Batch dimension(s) used for batched models.\n        logger: Custom logger.\n        seed: A seed to make the optimization reproducible.\n        torch_dtype: Torch data type used for storing the data. This needs to match\n            the dtype of the model used\n    \"\"\"\n    super().__init__(search_space=search_space, objective=objective, seed=seed)\n    self.num_initial_random = num_initial_random_samples\n    self.max_pending_evaluations = max_pending_evaluations\n    self.batch_shape = batch_shape\n    self.logger = logger or logging.getLogger(\"blackboxopt\")\n\n    self.torch_dtype = torch_dtype\n    self.X = torch.empty(\n        (*self.batch_shape, 0, len(search_space)), dtype=torch_dtype\n    )\n    self.losses = torch.empty((*self.batch_shape, 0, 1), dtype=torch_dtype)\n    self.pending_specifications: Dict[int, EvaluationSpecification] = {}\n    if seed is not None:\n        torch.manual_seed(seed=seed)\n\n    self.model = model\n    self.acquisition_function_factory = acquisition_function_factory\n    self.af_optimizer_kwargs = af_optimizer_kwargs\n</code></pre>"},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.generate_evaluation_specification","title":"<code>generate_evaluation_specification(self)</code>","text":"<p>Call the optimizer specific function and append a unique integer id to the specification.</p> <p>Please refer to the docstring of <code>blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification</code> for a description of the method.</p> Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n    \"\"\"Call the optimizer specific function and append a unique integer id\n    to the specification.\n\n    Please refer to the docstring of\n    `blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification`\n    for a description of the method.\n    \"\"\"\n    if (\n        self.max_pending_evaluations\n        and len(self.pending_specifications) == self.max_pending_evaluations\n    ):\n        raise OptimizerNotReady\n\n    # Generate random samples until there are enough samples where at least one of\n    # the objective values is available\n    if self.num_initial_random &gt; 0 and (\n        sum(~torch.any(self.losses.isnan(), dim=1)) &lt; self.num_initial_random\n    ):\n        eval_spec = EvaluationSpecification(\n            configuration=self.search_space.sample(),\n            optimizer_info={\"model_based_pick\": False},\n        )\n    else:\n        eval_spec = self._generate_evaluation_specification()\n        eval_spec.optimizer_info[\"model_based_pick\"] = True\n\n    eval_id = self.X.size(-2) + len(self.pending_specifications)\n    eval_spec.optimizer_info[\"evaluation_id\"] = eval_id\n    self.pending_specifications[eval_id] = eval_spec\n    return eval_spec\n</code></pre>"},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.predict_model_based_best","title":"<code>predict_model_based_best(self)</code>","text":"<p>Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model.</p> <p>This might return <code>None</code> in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet).</p> <p>Returns:</p> Type Description <code>Optional[blackboxopt.evaluation.Evaluation]</code> <p>blackboxopt.evaluation.Evaluation     The evaluated specification containing the estimated best configuration     or <code>None</code> in case no evaluations have been reported yet.</p> Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>def predict_model_based_best(self) -&gt; Optional[Evaluation]:\n    \"\"\"Get the current configuration that is estimated to be the best (in terms of\n    optimal objective value) without waiting for a reported evaluation of that\n    configuration. Instead, the objective value estimation relies on BO's\n    underlying model.\n\n    This might return `None` in case there is no successfully evaluated\n    configuration yet (thus, the optimizer has not been given training data yet).\n\n    Returns:\n        blackboxopt.evaluation.Evaluation\n            The evaluated specification containing the estimated best configuration\n            or `None` in case no evaluations have been reported yet.\n    \"\"\"\n    return predict_model_based_best(\n        model=self.model,\n        objective=self.objective,\n        search_space=self.search_space,\n        torch_dtype=self.torch_dtype,\n    )\n</code></pre>"},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.report","title":"<code>report(self, evaluations)</code>","text":"<p>A simple report method that conditions the model on data. This likely needs to be overridden for more specific BO implementations.</p> Source code in <code>blackboxopt/optimizers/botorch_base.py</code> <pre><code>def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n    \"\"\"A simple report method that conditions the model on data.\n    This likely needs to be overridden for more specific BO implementations.\n    \"\"\"\n    _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n    self._update_internal_evaluation_data(_evals)\n    # Just for populating all relevant caches\n    self.model.posterior(self.X)\n\n    x_filtered, y_filtered = filter_y_nans(self.X, self.losses)\n\n    # The actual model update\n    # Ignore BotorchTensorDimensionWarning which is always reported to make the user\n    # aware that they are reponsible for the right input Tensors dimensionality.\n    with warnings.catch_warnings():\n        warnings.simplefilter(\n            action=\"ignore\", category=BotorchTensorDimensionWarning\n        )\n        self.model = self.model.condition_on_observations(x_filtered, y_filtered)\n</code></pre>"},{"location":"reference/optimizers/botorch_utils/","title":"Botorch utils","text":""},{"location":"reference/optimizers/botorch_utils/#blackboxopt.optimizers.botorch_utils.filter_y_nans","title":"<code>filter_y_nans(x, y)</code>","text":"<p>Filter rows jointly for <code>x</code> and <code>y</code>, where <code>y</code> is <code>NaN</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>n x d</code> or <code>1 x n x d</code>.</p> required <code>y</code> <code>Tensor</code> <p>Input tensor of shape <code>n x m</code> or <code>1 x n x m</code>.</p> required <p>Returns:</p> Type Description <code>- x_f</code> <p>Filtered <code>x</code>. - y_f: Filtered <code>y</code>.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If input is 3D (batched representation) with first dimension not <code>1</code> (multiple batches).</p> Source code in <code>blackboxopt/optimizers/botorch_utils.py</code> <pre><code>def filter_y_nans(\n    x: torch.Tensor, y: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Filter rows jointly for `x` and `y`, where `y` is `NaN`.\n\n    Args:\n        x: Input tensor of shape `n x d` or `1 x n x d`.\n        y: Input tensor of shape `n x m` or `1 x n x m`.\n\n    Returns:\n        - x_f: Filtered `x`.\n        - y_f: Filtered `y`.\n\n    Raises:\n        ValueError: If input is 3D (batched representation) with first dimension not\n            `1` (multiple batches).\n    \"\"\"\n    if (len(x.shape) == 3 and x.shape[0] &gt; 1) or (len(y.shape) == 3 and y.shape[0] &gt; 1):\n        raise ValueError(\"Multiple batches are not supported for now.\")\n\n    x_f = x.clone()\n    y_f = y.clone()\n\n    # filter rows jointly where y is NaN\n    x_f = x_f[~torch.any(y_f.isnan(), dim=-1)]\n    y_f = y_f[~torch.any(y_f.isnan(), dim=-1)]\n\n    # cast n x d back to 1 x n x d if originally batch case\n    if len(x.shape) == 3:\n        x_f = x_f.reshape(torch.Size((1,)) + x_f.shape)\n    if len(y.shape) == 3:\n        y_f = y_f.reshape(torch.Size((1,)) + y_f.shape)\n\n    return x_f, y_f\n</code></pre>"},{"location":"reference/optimizers/botorch_utils/#blackboxopt.optimizers.botorch_utils.impute_nans_with_constant","title":"<code>impute_nans_with_constant(x, c=-1.0)</code>","text":"<p>Impute <code>NaN</code> values with given constant value.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape <code>n x d</code> or <code>b x n x d</code>.</p> required <code>c</code> <code>float</code> <p>Constant used as fill value to replace <code>NaNs</code>.</p> <code>-1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <ul> <li>x_i - <code>x</code> where all <code>NaN</code>s are replaced with given constant.</li> </ul> Source code in <code>blackboxopt/optimizers/botorch_utils.py</code> <pre><code>def impute_nans_with_constant(x: torch.Tensor, c: float = -1.0) -&gt; torch.Tensor:\n    \"\"\"Impute `NaN` values with given constant value.\n\n    Args:\n        x: Input tensor of shape `n x d` or `b x n x d`.\n        c: Constant used as fill value to replace `NaNs`.\n\n    Returns:\n        - x_i - `x` where all `NaN`s are replaced with given constant.\n    \"\"\"\n    if x.numel() == 0:  # empty tensor, nothing to impute\n        return x\n    x_i = x.clone()\n\n    # cast n x d to 1 x n x d (cover non-batch case)\n    if len(x.shape) == 2:\n        x_i = x_i.reshape(torch.Size((1,)) + x_i.shape)\n\n    for b in range(x_i.shape[0]):\n        x_1 = x_i[b, :, :]\n        x_1 = torch.tensor(\n            SimpleImputer(\n                missing_values=np.nan,\n                strategy=\"constant\",\n                fill_value=c,\n                keep_empty_features=True,\n            ).fit_transform(x_1),\n            dtype=x.dtype,\n        )\n        x_i[b, :, :] = x_1\n\n    # cast 1 x n x d back to n x d if originally non-batch\n    if len(x.shape) == 2:\n        x_i = x_i.reshape(x.shape)\n    return x_i\n</code></pre>"},{"location":"reference/optimizers/botorch_utils/#blackboxopt.optimizers.botorch_utils.predict_model_based_best","title":"<code>predict_model_based_best(model, search_space, objective, torch_dtype)</code>","text":"<p>Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model.</p> <p>This might return <code>None</code> in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to use for predicting the best.</p> required <code>search_space</code> <code>ParameterSpace</code> <p>Space to convert between numerical and original configurations.</p> required <code>objective</code> <code>Objective</code> <p>Objective to convert the model based loss prediction to the target.</p> required <p>Returns:</p> Type Description <code>Optional[blackboxopt.evaluation.Evaluation]</code> <p>blackboxopt.evaluation.Evaluation     The evaluated specification containing the estimated best configuration     or <code>None</code> in case no evaluations have been reported yet.</p> Source code in <code>blackboxopt/optimizers/botorch_utils.py</code> <pre><code>def predict_model_based_best(\n    model: botorch.models.model.Model,\n    search_space: ps.ParameterSpace,\n    objective: Objective,\n    torch_dtype: torch.dtype,\n) -&gt; Optional[Evaluation]:\n    \"\"\"Get the current configuration that is estimated to be the best (in terms of\n    optimal objective value) without waiting for a reported evaluation of that\n    configuration. Instead, the objective value estimation relies on BO's\n    underlying model.\n\n    This might return `None` in case there is no successfully evaluated\n    configuration yet (thus, the optimizer has not been given training data yet).\n\n    Args:\n        model: The model to use for predicting the best.\n        search_space: Space to convert between numerical and original configurations.\n        objective: Objective to convert the model based loss prediction to the target.\n\n    Returns:\n        blackboxopt.evaluation.Evaluation\n            The evaluated specification containing the estimated best configuration\n            or `None` in case no evaluations have been reported yet.\n    \"\"\"\n    if model.train_inputs[0].numel() == 0:\n        return None\n\n    def posterior_mean(x):\n        # function to be optimized: posterior mean\n        # scipy's minimize expects the following interface:\n        #  - input: 1-D array with shape (n,)\n        #  - output: float\n        mean = model.posterior(torch.from_numpy(np.atleast_2d(x))).mean\n        return mean.item()\n\n    # prepare initial random samples and bounds for scipy's minimize\n    n_init_samples = 10\n    init_points = np.asarray(\n        [\n            search_space.to_numerical(search_space.sample())\n            for _ in range(n_init_samples)\n        ]\n    )\n\n    # use scipy's minimize to find optimum of the posterior mean\n    optimized_points = [\n        sci_opt.minimize(\n            fun=posterior_mean,\n            constraints=None,\n            jac=False,\n            x0=x,\n            args=(),\n            # The numerical representation always lives on the unit hypercube\n            bounds=torch.Tensor([[0, 1]] * len(search_space)).to(dtype=torch_dtype),\n            method=\"L-BFGS-B\",\n            options=None,\n        )\n        for x in init_points\n    ]\n\n    f_optimized = np.array([np.atleast_1d(p.fun) for p in optimized_points]).flatten()\n    # get indexes of optimum value (with a tolerance)\n    inds = np.argwhere(np.isclose(f_optimized, np.min(f_optimized)))\n    # randomly select one index if there are multiple\n    ind = np.random.choice(inds.flatten())\n\n    # create Evaluation from the best estimated configuration\n    best_x = optimized_points[ind].x\n    best_y = posterior_mean(best_x)\n    return Evaluation(\n        configuration=search_space.from_numerical(best_x),\n        objectives={\n            objective.name: -1 * best_y if objective.greater_is_better else best_y\n        },\n    )\n</code></pre>"},{"location":"reference/optimizers/botorch_utils/#blackboxopt.optimizers.botorch_utils.to_numerical","title":"<code>to_numerical(evaluations, search_space, objectives, constraint_names=None, batch_shape=torch.Size([]), torch_dtype=torch.float32)</code>","text":"<p>Convert evaluations to one <code>(#batch, #evaluations, #parameters)</code> tensor containing the numerical representations of the configurations and one <code>(#batch, #evaluations, 1)</code> tensor containing the loss representation of the evaluations' objective value (flips the sign for objective value if <code>objective.greater_is_better=True</code>) and optionally constraints value.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations</code> <code>Iterable[blackboxopt.evaluation.Evaluation]</code> <p>List of evaluations that were collected during optimization.</p> required <code>search_space</code> <code>ParameterSpace</code> <p>Search space used during optimization.</p> required <code>objectives</code> <code>Sequence[blackboxopt.base.Objective]</code> <p>Objectives that were used for optimization.</p> required <code>constraint_names</code> <code>Optional[List[str]]</code> <p>Name of constraints that are used for optimization.</p> <code>None</code> <code>batch_shape</code> <code>Size</code> <p>Batch dimension(s) used for batched models.</p> <code>torch.Size([])</code> <code>torch_dtype</code> <code>dtype</code> <p>Type of returned tensors.</p> <code>torch.float32</code> <p>Returns:</p> Type Description <code>- X</code> <p>Numerical representation of the configurations - Y: Numerical representation of the objective values and optionally constraints</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If one of configurations is not valid w.r.t. search space.</p> <code>ValueError</code> <p>If one of configurations includes parameters that are not part of the search space.</p> <code>ConstraintError</code> <p>If one of the constraint names is not defined in evaluations.</p> Source code in <code>blackboxopt/optimizers/botorch_utils.py</code> <pre><code>def to_numerical(\n    evaluations: Iterable[Evaluation],\n    search_space: ps.ParameterSpace,\n    objectives: Sequence[Objective],\n    constraint_names: Optional[List[str]] = None,\n    batch_shape: torch.Size = torch.Size(),\n    torch_dtype: torch.dtype = torch.float32,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Convert evaluations to one `(#batch, #evaluations, #parameters)` tensor\n    containing the numerical representations of the configurations and\n    one `(#batch, #evaluations, 1)` tensor containing the loss representation of\n    the evaluations' objective value (flips the sign for objective value\n    if `objective.greater_is_better=True`) and optionally constraints value.\n\n    Args:\n        evaluations: List of evaluations that were collected during optimization.\n        search_space: Search space used during optimization.\n        objectives: Objectives that were used for optimization.\n        constraint_names: Name of constraints that are used for optimization.\n        batch_shape: Batch dimension(s) used for batched models.\n        torch_dtype: Type of returned tensors.\n\n    Returns:\n        - X: Numerical representation of the configurations\n        - Y: Numerical representation of the objective values and optionally constraints\n\n    Raises:\n        ValueError: If one of configurations is not valid w.r.t. search space.\n        ValueError: If one of configurations includes parameters that are not part of\n            the search space.\n        ConstraintError: If one of the constraint names is not defined in evaluations.\n    \"\"\"\n    # validate configuration values and dimensions\n    parameter_names = search_space.get_parameter_names() + list(\n        search_space.get_constant_names()\n    )\n    for e in evaluations:\n        with warnings.catch_warnings():\n            # we already raise error if search space not valid, thus can ignore warnings\n            warnings.filterwarnings(\n                \"ignore\", category=RuntimeWarning, message=\"Parameter\"\n            )\n            if not search_space.check_validity(e.configuration):\n                raise ValueError(\n                    f\"The provided configuration {e.configuration} is not valid.\"\n                )\n        if not set(parameter_names) &gt;= set(e.configuration.keys()):\n            raise ValueError(\n                f\"Mismatch in parameter names from search space {parameter_names} and \"\n                + f\"configuration {e.configuration}\"\n            )\n\n    X = torch.tensor(\n        np.array([search_space.to_numerical(e.configuration) for e in evaluations]),\n        dtype=torch_dtype,\n    )\n    X = X.reshape(*batch_shape + X.shape)\n\n    Y = torch.Tensor(\n        [\n            get_loss_vector(\n                known_objectives=objectives, reported_objectives=e.objectives\n            )\n            for e in evaluations\n        ]\n    ).to(dtype=torch_dtype)\n\n    if constraint_names is not None:\n        try:\n            Y_constraints = torch.tensor(\n                np.array(\n                    [[e.constraints[c] for c in constraint_names] for e in evaluations],\n                    dtype=float,\n                ),\n                dtype=torch_dtype,\n            )\n            Y = torch.cat((Y, Y_constraints), dim=1)\n        except KeyError as e:\n            raise ConstraintsError(\n                f\"Constraint name {e} is not defined in input evaluations.\"\n            ) from e\n        except TypeError as e:\n            raise ConstraintsError(\n                f\"Constraint name(s) {constraint_names} are not defined in input \"\n                + \"evaluations.\"\n            ) from e\n\n    Y = Y.reshape(*batch_shape + Y.shape)\n\n    return X, Y\n</code></pre>"},{"location":"reference/optimizers/hyperband/","title":"Hyperband","text":""},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband.Hyperband","title":"<code> Hyperband            (StagedIterationOptimizer)         </code>","text":"Source code in <code>blackboxopt/optimizers/hyperband.py</code> <pre><code>class Hyperband(StagedIterationOptimizer):\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objective: Objective,\n        min_fidelity: float,\n        max_fidelity: float,\n        num_iterations: int,\n        eta: float = 3.0,\n        seed: Optional[int] = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Implementation of Hyperband as proposed in\n\n        Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., &amp; Talwalkar, A. (2016).\n        Hyperband: A novel bandit-based approach to hyperparameter optimization.\n        arXiv preprint arXiv:1603.06560.\n\n        Args:\n            search_space: [description]\n            objective: [description]\n            min_fidelity: The smallest fidelity value that is still meaningful.\n                Must be strictly greater than zero!\n            max_fidelity: The largest fidelity value used during the optimization.\n                Must not be smaller than `min_fidelity`\n            num_iterations: [description]\n            eta: Scaling parameter to control the aggressiveness of Hyperband's racing.\n            seed: [description]\n            logger: [description]\n        \"\"\"\n        self.config_sampler = RandomSearchSampler(search_space)\n        self.min_fidelity = min_fidelity\n        self.max_fidelity = max_fidelity\n        self.eta = eta\n\n        super().__init__(\n            search_space=search_space,\n            objective=objective,\n            num_iterations=num_iterations,\n            seed=seed,\n            logger=logger,\n        )\n\n    def _create_new_iteration(self, iteration_index: int) -&gt; StagedIteration:\n        \"\"\"Optimizer specific way to create a new\n        `blackboxopt.optimizer.staged.iteration.StagedIteration` object\n        \"\"\"\n        return create_hyperband_iteration(\n            iteration_index,\n            self.min_fidelity,\n            self.max_fidelity,\n            self.eta,\n            self.config_sampler,\n            self.objective,\n            self.logger,\n        )\n</code></pre>"},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband.Hyperband.__init__","title":"<code>__init__(self, search_space, objective, min_fidelity, max_fidelity, num_iterations, eta=3.0, seed=None, logger=None)</code>  <code>special</code>","text":"<p>Implementation of Hyperband as proposed in</p> <p>Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., &amp; Talwalkar, A. (2016). Hyperband: A novel bandit-based approach to hyperparameter optimization. arXiv preprint arXiv:1603.06560.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>[description]</p> required <code>objective</code> <code>Objective</code> <p>[description]</p> required <code>min_fidelity</code> <code>float</code> <p>The smallest fidelity value that is still meaningful. Must be strictly greater than zero!</p> required <code>max_fidelity</code> <code>float</code> <p>The largest fidelity value used during the optimization. Must not be smaller than <code>min_fidelity</code></p> required <code>num_iterations</code> <code>int</code> <p>[description]</p> required <code>eta</code> <code>float</code> <p>Scaling parameter to control the aggressiveness of Hyperband's racing.</p> <code>3.0</code> <code>seed</code> <code>Optional[int]</code> <p>[description]</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>[description]</p> <code>None</code> Source code in <code>blackboxopt/optimizers/hyperband.py</code> <pre><code>def __init__(\n    self,\n    search_space: ParameterSpace,\n    objective: Objective,\n    min_fidelity: float,\n    max_fidelity: float,\n    num_iterations: int,\n    eta: float = 3.0,\n    seed: Optional[int] = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"Implementation of Hyperband as proposed in\n\n    Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., &amp; Talwalkar, A. (2016).\n    Hyperband: A novel bandit-based approach to hyperparameter optimization.\n    arXiv preprint arXiv:1603.06560.\n\n    Args:\n        search_space: [description]\n        objective: [description]\n        min_fidelity: The smallest fidelity value that is still meaningful.\n            Must be strictly greater than zero!\n        max_fidelity: The largest fidelity value used during the optimization.\n            Must not be smaller than `min_fidelity`\n        num_iterations: [description]\n        eta: Scaling parameter to control the aggressiveness of Hyperband's racing.\n        seed: [description]\n        logger: [description]\n    \"\"\"\n    self.config_sampler = RandomSearchSampler(search_space)\n    self.min_fidelity = min_fidelity\n    self.max_fidelity = max_fidelity\n    self.eta = eta\n\n    super().__init__(\n        search_space=search_space,\n        objective=objective,\n        num_iterations=num_iterations,\n        seed=seed,\n        logger=logger,\n    )\n</code></pre>"},{"location":"reference/optimizers/random_search/","title":"Random search","text":""},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search.RandomSearch","title":"<code> RandomSearch            (MultiObjectiveOptimizer)         </code>","text":"Source code in <code>blackboxopt/optimizers/random_search.py</code> <pre><code>class RandomSearch(MultiObjectiveOptimizer):\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objectives: List[Objective],\n        max_steps: int,\n        seed: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Randomly sample up to `max_steps` configurations from the given search space.\n\n        Args:\n            search_space: Space to search\n            objectives: The objectives of the optimization.\n            max_steps: Max number of evaluation specifications the optimizer generates\n                before raising `OptimizationComplete`\n            seed: Optional number to seed the random number generator with.\n                Defaults to None.\n        \"\"\"\n        super().__init__(search_space=search_space, objectives=objectives, seed=seed)\n\n        self.max_steps: int = max_steps\n        self.n_steps: int = 0\n\n    def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n        \"\"\"[summary]\n\n        Raises:\n            OptimizationComplete: Raised if the optimizer's `max_steps` are reached.\n\n        Returns:\n            [description]\n        \"\"\"\n        if self.n_steps &gt;= self.max_steps:\n            raise OptimizationComplete()\n\n        eval_spec = EvaluationSpecification(\n            configuration=self.search_space.sample(),\n            settings={},\n            optimizer_info={\"step\": self.n_steps},\n        )\n        self.n_steps += 1\n\n        return eval_spec\n</code></pre>"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search.RandomSearch.__init__","title":"<code>__init__(self, search_space, objectives, max_steps, seed=None)</code>  <code>special</code>","text":"<p>Randomly sample up to <code>max_steps</code> configurations from the given search space.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>Space to search</p> required <code>objectives</code> <code>List[blackboxopt.base.Objective]</code> <p>The objectives of the optimization.</p> required <code>max_steps</code> <code>int</code> <p>Max number of evaluation specifications the optimizer generates before raising <code>OptimizationComplete</code></p> required <code>seed</code> <code>Optional[int]</code> <p>Optional number to seed the random number generator with. Defaults to None.</p> <code>None</code> Source code in <code>blackboxopt/optimizers/random_search.py</code> <pre><code>def __init__(\n    self,\n    search_space: ParameterSpace,\n    objectives: List[Objective],\n    max_steps: int,\n    seed: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Randomly sample up to `max_steps` configurations from the given search space.\n\n    Args:\n        search_space: Space to search\n        objectives: The objectives of the optimization.\n        max_steps: Max number of evaluation specifications the optimizer generates\n            before raising `OptimizationComplete`\n        seed: Optional number to seed the random number generator with.\n            Defaults to None.\n    \"\"\"\n    super().__init__(search_space=search_space, objectives=objectives, seed=seed)\n\n    self.max_steps: int = max_steps\n    self.n_steps: int = 0\n</code></pre>"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search.RandomSearch.generate_evaluation_specification","title":"<code>generate_evaluation_specification(self)</code>","text":"<p>[summary]</p> <p>Exceptions:</p> Type Description <code>OptimizationComplete</code> <p>Raised if the optimizer's <code>max_steps</code> are reached.</p> <p>Returns:</p> Type Description <code>EvaluationSpecification</code> <p>[description]</p> Source code in <code>blackboxopt/optimizers/random_search.py</code> <pre><code>def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n    \"\"\"[summary]\n\n    Raises:\n        OptimizationComplete: Raised if the optimizer's `max_steps` are reached.\n\n    Returns:\n        [description]\n    \"\"\"\n    if self.n_steps &gt;= self.max_steps:\n        raise OptimizationComplete()\n\n    eval_spec = EvaluationSpecification(\n        configuration=self.search_space.sample(),\n        settings={},\n        optimizer_info={\"step\": self.n_steps},\n    )\n    self.n_steps += 1\n\n    return eval_spec\n</code></pre>"},{"location":"reference/optimizers/space_filling/","title":"Space filling","text":""},{"location":"reference/optimizers/space_filling/#blackboxopt.optimizers.space_filling.SpaceFilling","title":"<code> SpaceFilling            (MultiObjectiveOptimizer)         </code>","text":"<p>Sobol sequence based, space filling optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>The search space to optimize</p> required <code>objectives</code> <code>List[blackboxopt.base.Objective]</code> <p>The objectives of the optimization</p> required <code>seed</code> <code>Optional[int]</code> <p>The sobol sequence is Owen scrambled and can be seeded for reproducibility</p> <code>None</code> Source code in <code>blackboxopt/optimizers/space_filling.py</code> <pre><code>class SpaceFilling(MultiObjectiveOptimizer):\n    \"\"\"Sobol sequence based, space filling optimizer.\n\n    Args:\n        search_space: The search space to optimize\n        objectives: The objectives of the optimization\n        seed: The sobol sequence is Owen scrambled and can be seeded for reproducibility\n    \"\"\"\n\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objectives: List[Objective],\n        seed: Optional[int] = None,\n    ) -&gt; None:\n        super().__init__(search_space=search_space, objectives=objectives, seed=seed)\n        self.sobol = Sobol(d=len(self.search_space), scramble=True, seed=seed)\n\n    def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n        vector = self.sobol.random().flatten()\n        configuration = self.search_space.from_numerical(vector)\n        return EvaluationSpecification(configuration=configuration)\n</code></pre>"},{"location":"reference/optimizers/space_filling/#blackboxopt.optimizers.space_filling.SpaceFilling.generate_evaluation_specification","title":"<code>generate_evaluation_specification(self)</code>","text":"<p>Get next configuration and settings to evaluate.</p> <p>Exceptions:</p> Type Description <code>OptimizationComplete</code> <p>When the optimization run is finished, e.g. when the budget has been exhausted.</p> <code>OptimizerNotReady</code> <p>When the optimizer is not ready to propose a new evaluation specification.</p> Source code in <code>blackboxopt/optimizers/space_filling.py</code> <pre><code>def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n    vector = self.sobol.random().flatten()\n    configuration = self.search_space.from_numerical(vector)\n    return EvaluationSpecification(configuration=configuration)\n</code></pre>"},{"location":"reference/optimizers/testing/","title":"Testing","text":"<p>Tests that can be imported and used to test optimizer implementations against this packages blackbox optimizer interface.</p>"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.handles_conditional_space","title":"<code>handles_conditional_space(optimizer_class, optimizer_kwargs, seed=None, n_max_evaluations=10)</code>","text":"<p>Check if optimizer handles conditional i.e. hierarchical search spaces.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_class</code> <code>Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]]</code> <p>Optimizer to test.</p> required <code>optimizer_kwargs</code> <code>dict</code> <p>Expected to contain additional arguments for initializing the optimizer. (<code>search_space</code> and <code>objective(s)</code> are set automatically by the test.)</p> required <code>seed</code> <code>Optional[int]</code> <p>(optional) custom seed</p> <code>None</code> <code>n_max_evaluations</code> <code>int</code> <p>Maximum number of evaluation to try</p> <code>10</code> Source code in <code>blackboxopt/optimizers/testing.py</code> <pre><code>def handles_conditional_space(\n    optimizer_class: Union[\n        Type[SingleObjectiveOptimizer], Type[MultiObjectiveOptimizer]\n    ],\n    optimizer_kwargs: dict,\n    seed: Optional[int] = None,\n    n_max_evaluations: int = 10,\n):\n    \"\"\"Check if optimizer handles conditional i.e. hierarchical search spaces.\n\n    Args:\n        optimizer_class: Optimizer to test.\n        optimizer_kwargs: Expected to contain additional arguments for initializing\n            the optimizer. (`search_space` and `objective(s)` are set automatically\n            by the test.)\n        seed: (optional) custom seed\n        n_max_evaluations: Maximum number of evaluation to try\n    \"\"\"\n    space = ps.ParameterSpace()\n    space.add(ps.CategoricalParameter(\"optimizer\", (\"adam\", \"sgd\")))\n    space.add(ps.ContinuousParameter(\"lr\", (0.0001, 0.1), transformation=\"log\"))\n    space.add(\n        ps.ContinuousParameter(\"momentum\", (0.0, 1.0)),\n        lambda optimizer: optimizer == \"sgd\",\n    )\n    space.seed(seed)\n\n    opt = _initialize_optimizer(\n        optimizer_class, optimizer_kwargs, space=space, seed=seed\n    )\n\n    for _ in range(n_max_evaluations):\n        es = opt.generate_evaluation_specification()\n        objectives = {\n            \"loss\": es.configuration.get(\"momentum\", 1.0) * es.configuration[\"lr\"] ** 2\n        }\n        if isinstance(opt, MultiObjectiveOptimizer):\n            objectives[\"score\"] = -1.0 * es.configuration[\"lr\"] ** 2\n        opt.report(\n            es.create_evaluation(\n                objectives=objectives, constraints={\"constraint\": 10.0}\n            )\n        )\n</code></pre>"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.handles_reporting_evaluations_list","title":"<code>handles_reporting_evaluations_list(optimizer_class, optimizer_kwargs, seed=None)</code>","text":"<p>Check if optimizer's report method can process an iterable of evaluations.</p> <p>All optimizers should be able to allow reporting batches of evaluations. It's up to the optimizer's implementation, if evaluations in a batch are processed one by one like if they were reported individually, or if a batch is handled differently.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_class</code> <code>Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]]</code> <p>Optimizer to test.</p> required <code>optimizer_kwargs</code> <code>dict</code> <p>Expected to contain additional arguments for initializing the optimizer. (<code>search_space</code> and <code>objective(s)</code> are set automatically by the test.)</p> required <code>seed</code> <code>Optional[int]</code> <p>(optional) custom seed</p> <code>None</code> Source code in <code>blackboxopt/optimizers/testing.py</code> <pre><code>def handles_reporting_evaluations_list(\n    optimizer_class: Union[\n        Type[SingleObjectiveOptimizer], Type[MultiObjectiveOptimizer]\n    ],\n    optimizer_kwargs: dict,\n    seed: Optional[int] = None,\n):\n    \"\"\"Check if optimizer's report method can process an iterable of evaluations.\n\n    All optimizers should be able to allow reporting batches of evaluations. It's up to\n    the optimizer's implementation, if evaluations in a batch are processed\n    one by one like if they were reported individually, or if a batch is handled\n    differently.\n\n    Args:\n        optimizer_class: Optimizer to test.\n        optimizer_kwargs: Expected to contain additional arguments for initializing\n            the optimizer. (`search_space` and `objective(s)` are set automatically\n            by the test.)\n        seed: (optional) custom seed\n    \"\"\"\n    opt = _initialize_optimizer(optimizer_class, optimizer_kwargs, seed=seed)\n    evaluations = []\n    for i in range(3):\n        es = opt.generate_evaluation_specification()\n\n        objectives = {\"loss\": 0.42 * i}\n        if isinstance(opt, MultiObjectiveOptimizer):\n            objectives[\"score\"] = float(i)\n\n        evaluation = es.create_evaluation(\n            objectives=objectives, constraints={\"constraint\": 10.0 * i}\n        )\n        evaluations.append(evaluation)\n\n    opt.report(evaluations)\n</code></pre>"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.is_deterministic_when_reporting_shuffled_evaluations","title":"<code>is_deterministic_when_reporting_shuffled_evaluations(optimizer_class, optimizer_kwargs, seed=None)</code>","text":"<p>Check if determinism isn't affected by the order of initially reported data.</p> <p>Repeatedly initialize the optimizer with the same parameter space and a fixed seed. Report a set of initial evaluations in randomized order as initial data. Start optimizing and check if the generated configurations for all optimizers are equal.</p> <p>By doing multiple evaluations, this tests covers effects that become visible after a while, e.g. only after stages got completed in staged iteration samplers.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_class</code> <code>Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]]</code> <p>Optimizer to test.</p> required <code>optimizer_kwargs</code> <code>dict</code> <p>Expected to contain additional arguments for initializing the optimizer. (<code>search_space</code> and <code>objective(s)</code> are set automatically by the test.)</p> required <code>seed</code> <code>Optional[int]</code> <p>(optional) custom seed</p> <code>None</code> Source code in <code>blackboxopt/optimizers/testing.py</code> <pre><code>def is_deterministic_when_reporting_shuffled_evaluations(\n    optimizer_class: Union[\n        Type[SingleObjectiveOptimizer], Type[MultiObjectiveOptimizer]\n    ],\n    optimizer_kwargs: dict,\n    seed: Optional[int] = None,\n):\n    \"\"\"Check if determinism isn't affected by the order of initially reported data.\n\n    Repeatedly initialize the optimizer with the same parameter space and a fixed seed.\n    Report a set of initial evaluations in randomized order as initial data. Start\n    optimizing and check if the generated configurations for all optimizers are equal.\n\n    By doing multiple evaluations, this tests covers effects that become visible after\n    a while, e.g. only after stages got completed in staged iteration samplers.\n\n    Args:\n        optimizer_class: Optimizer to test.\n        optimizer_kwargs: Expected to contain additional arguments for initializing\n            the optimizer. (`search_space` and `objective(s)` are set automatically\n            by the test.)\n        seed: (optional) custom seed\n    \"\"\"\n    if seed is None:\n        seed = 0\n\n    space = ps.ParameterSpace()\n    space.add(ps.ContinuousParameter(\"p1\", (0, 1)))\n    space.seed(seed)\n\n    def _run_experiment_1d(es):\n        x = es.configuration[\"p1\"]\n        _x = np.copy(np.atleast_2d(x))\n        params = np.array([0.75, 0.0, -10.0, 0.0, 0.0])\n        y = np.polyval(params, _x)\n        return float(np.squeeze(y))\n\n    runs: Dict[int, Dict] = {0: {}, 1: {}}\n    for run_idx, run in runs.items():\n        run[\"evaluations\"] = []\n        opt = _initialize_optimizer(\n            optimizer_class, optimizer_kwargs, space=space, seed=seed\n        )\n\n        # Report initial data in different order\n        eval_specs = [opt.generate_evaluation_specification() for _ in range(5)]\n        run[\"initial_evaluations\"] = [\n            es.create_evaluation(\n                objectives={\"loss\": _run_experiment_1d(es)},\n                constraints={\"constraint\": 10.0},\n            )\n            for es in eval_specs\n        ]\n        if isinstance(opt, MultiObjectiveOptimizer):\n            for e in run[\"initial_evaluations\"]:\n                e.objectives[\"score\"] = -1.0 * e.objectives[\"loss\"] ** 2\n\n        shuffle_rng = random.Random(run_idx)\n        shuffle_rng.shuffle(run[\"initial_evaluations\"])\n        opt.report(run[\"initial_evaluations\"])\n\n        # Start optimizing\n        for _ in range(5):\n            es = opt.generate_evaluation_specification()\n\n            objectives = {\"loss\": _run_experiment_1d(es)}\n            if isinstance(opt, MultiObjectiveOptimizer):\n                objectives[\"score\"] = -1.0 * objectives[\"loss\"] ** 2\n            evaluation = es.create_evaluation(\n                objectives=objectives, constraints={\"constraint\": 10.0}\n            )\n\n            opt.report(evaluation)\n            run[\"evaluations\"].append(evaluation)\n\n    initial_configs_run_0 = [e.configuration for e in runs[0][\"initial_evaluations\"]]\n    initial_configs_run_1 = [e.configuration for e in runs[1][\"initial_evaluations\"]]\n\n    configs_run_0_as_floats = [e.configuration[\"p1\"] for e in runs[0][\"evaluations\"]]\n    configs_run_1_as_floats = [e.configuration[\"p1\"] for e in runs[1][\"evaluations\"]]\n\n    assert initial_configs_run_0 != initial_configs_run_1\n    np.testing.assert_almost_equal(\n        configs_run_0_as_floats, configs_run_1_as_floats, decimal=3\n    )\n</code></pre>"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.is_deterministic_with_fixed_seed_and_larger_space","title":"<code>is_deterministic_with_fixed_seed_and_larger_space(optimizer_class, optimizer_kwargs, seed=None)</code>","text":"<p>Check if optimizer is deterministic.</p> <p>Initialize the optimizer twice with the same parameter space and a fixed seed. For each optimizer run optimization loop <code>n_evaluations</code> times, namely, get an evaluation specification and report a placeholder result back. The list of configurations should be equal for both optimizers.</p> <p>This tests covers multiple parameter types by using a mixed search space.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_class</code> <code>Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]]</code> <p>Optimizer to test.</p> required <code>optimizer_kwargs</code> <code>dict</code> <p>Expected to contain additional arguments for initializing the optimizer. (<code>search_space</code> and <code>objective(s)</code> are set automatically by the test.)</p> required <code>seed</code> <code>Optional[int]</code> <p>(optional) custom seed</p> <code>None</code> Source code in <code>blackboxopt/optimizers/testing.py</code> <pre><code>def is_deterministic_with_fixed_seed_and_larger_space(\n    optimizer_class: Union[\n        Type[SingleObjectiveOptimizer], Type[MultiObjectiveOptimizer]\n    ],\n    optimizer_kwargs: dict,\n    seed: Optional[int] = None,\n):\n    \"\"\"Check if optimizer is deterministic.\n\n    Initialize the optimizer twice with the same parameter space and a fixed seed.\n    For each optimizer run optimization loop `n_evaluations` times, namely,\n    get an evaluation specification and report a placeholder result back.\n    The list of configurations should be equal for both optimizers.\n\n    This tests covers multiple parameter types by using a mixed search space.\n\n    Args:\n        optimizer_class: Optimizer to test.\n        optimizer_kwargs: Expected to contain additional arguments for initializing\n            the optimizer. (`search_space` and `objective(s)` are set automatically\n            by the test.)\n        seed: (optional) custom seed\n    \"\"\"\n    if seed is None:\n        seed = 42\n\n    n_evaluations = 5\n    losses = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n    run_0_configs: List[Evaluation] = []\n    run_1_configs: List[Evaluation] = []\n\n    for run_configs in [run_0_configs, run_1_configs]:\n        opt = _initialize_optimizer(optimizer_class, optimizer_kwargs, seed=seed)\n\n        for i in range(n_evaluations):\n            es = opt.generate_evaluation_specification()\n\n            objectives = {\"loss\": losses[i]}\n            if isinstance(opt, MultiObjectiveOptimizer):\n                objectives[\"score\"] = -1.0 * losses[i] ** 2\n            evaluation = es.create_evaluation(\n                objectives=objectives, constraints={\"constraint\": 10.0}\n            )\n\n            opt.report(evaluation)\n\n            run_configs.append(evaluation.configuration)\n\n    assert len(run_0_configs) == n_evaluations\n    assert run_0_configs == run_1_configs\n</code></pre>"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.optimize_single_parameter_sequentially_for_n_max_evaluations","title":"<code>optimize_single_parameter_sequentially_for_n_max_evaluations(optimizer_class, optimizer_kwargs, seed=None, n_max_evaluations=20)</code>","text":"<p>[summary]</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_class</code> <code>Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]]</code> <p>[description]</p> required <code>optimizer_kwargs</code> <code>dict</code> <p>[description]</p> required <code>n_max_evaluations</code> <code>int</code> <p>[description]</p> <code>20</code> <code>seed</code> <code>Optional[int]</code> <p>(optional) custom seed</p> <code>None</code> <p>Returns:</p> Type Description <p>[description]</p> Source code in <code>blackboxopt/optimizers/testing.py</code> <pre><code>def optimize_single_parameter_sequentially_for_n_max_evaluations(\n    optimizer_class: Union[\n        Type[SingleObjectiveOptimizer], Type[MultiObjectiveOptimizer]\n    ],\n    optimizer_kwargs: dict,\n    seed: Optional[int] = None,\n    n_max_evaluations: int = 20,\n):\n    \"\"\"[summary]\n\n    Args:\n        optimizer_class: [description]\n        optimizer_kwargs: [description]\n        n_max_evaluations: [description]\n        seed: (optional) custom seed\n\n    Returns:\n        [description]\n    \"\"\"\n\n    def quadratic_function(p1):\n        return p1**2\n\n    assert issubclass(optimizer_class, Optimizer), (\n        \"The default test suite is only applicable for implementations of \"\n        \"blackboxopt.base.Optimizer\"\n    )\n\n    optimizer = _initialize_optimizer(optimizer_class, optimizer_kwargs, seed=seed)\n\n    eval_spec = optimizer.generate_evaluation_specification()\n\n    if issubclass(optimizer_class, MultiObjectiveOptimizer):\n        evaluation = eval_spec.create_evaluation(\n            objectives={\"loss\": None, \"score\": None},\n            constraints={\"constraint\": 10.0},\n        )\n    else:\n        evaluation = eval_spec.create_evaluation(\n            objectives={\"loss\": None}, constraints={\"constraint\": 10.0}\n        )\n    optimizer.report(evaluation)\n\n    for _ in range(n_max_evaluations):\n        try:\n            eval_spec = optimizer.generate_evaluation_specification()\n        except OptimizationComplete:\n            break\n\n        loss = quadratic_function(p1=eval_spec.configuration[\"p1\"])\n        if issubclass(optimizer_class, MultiObjectiveOptimizer):\n            evaluation_result = {\"loss\": loss, \"score\": -loss}\n        else:\n            evaluation_result = {\"loss\": loss}\n\n        evaluation = eval_spec.create_evaluation(\n            objectives=evaluation_result, constraints={\"constraint\": 10.0}\n        )\n        optimizer.report(evaluation)\n</code></pre>"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.raises_evaluation_error_when_reporting_unknown_objective","title":"<code>raises_evaluation_error_when_reporting_unknown_objective(optimizer_class, optimizer_kwargs, seed=None)</code>","text":"<p>Check if optimizer's report method raises exception in case objective is unknown.</p> <p>Also make sure that the faulty evaluations (and only those) are included in the exception.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_class</code> <code>Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]]</code> <p>Optimizer to test.</p> required <code>optimizer_kwargs</code> <code>dict</code> <p>Expected to contain additional arguments for initializing the optimizer. (<code>search_space</code> and <code>objective(s)</code> are set automatically by the test.)</p> required <code>seed</code> <code>Optional[int]</code> <p>(optional) custom seed</p> <code>None</code> Source code in <code>blackboxopt/optimizers/testing.py</code> <pre><code>def raises_evaluation_error_when_reporting_unknown_objective(\n    optimizer_class: Union[\n        Type[SingleObjectiveOptimizer], Type[MultiObjectiveOptimizer]\n    ],\n    optimizer_kwargs: dict,\n    seed: Optional[int] = None,\n):\n    \"\"\"Check if optimizer's report method raises exception in case objective is unknown.\n\n    Also make sure that the faulty evaluations (and only those) are included in the\n    exception.\n\n    Args:\n        optimizer_class: Optimizer to test.\n        optimizer_kwargs: Expected to contain additional arguments for initializing\n            the optimizer. (`search_space` and `objective(s)` are set automatically\n            by the test.)\n        seed: (optional) custom seed\n    \"\"\"\n    opt = _initialize_optimizer(optimizer_class, optimizer_kwargs, seed=seed)\n    es_1 = opt.generate_evaluation_specification()\n    es_2 = opt.generate_evaluation_specification()\n    es_3 = opt.generate_evaluation_specification()\n\n    # NOTE: The following is not using pytest.raises because this would add pytest as\n    #       a regular dependency to blackboxopt.\n    try:\n        evaluation_1 = es_1.create_evaluation(\n            objectives={\"loss\": 1}, constraints={\"constraint\": 10.0}\n        )\n        evaluation_2 = es_2.create_evaluation(\n            objectives={\"unknown_objective\": 2}, constraints={\"constraint\": 10.0}\n        )\n        evaluation_3 = es_3.create_evaluation(\n            objectives={\"loss\": 4}, constraints={\"constraint\": 10.0}\n        )\n        evaluations = [evaluation_1, evaluation_2, evaluation_3]\n        if isinstance(opt, MultiObjectiveOptimizer):\n            for e in evaluations:\n                e.objectives[\"score\"] = 0.0\n        opt.report(evaluations)\n\n        raise AssertionError(\n            f\"Optimizer {optimizer_class} did not raise an ObjectivesError when a \"\n            + \"result including an unknown objective name was reported.\"\n        )\n\n    except EvaluationsError as exception:\n        invalid_evaluations = [e for e, _ in exception.evaluations_with_errors]\n        assert len(invalid_evaluations) == 1\n        assert evaluation_2 in invalid_evaluations\n</code></pre>"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.respects_fixed_parameter","title":"<code>respects_fixed_parameter(optimizer_class, optimizer_kwargs, seed=None)</code>","text":"<p>Check if optimizer's generated evaluation specifications contain the values a parameter in the search space was fixed to.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_class</code> <code>Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]]</code> <p>Optimizer to test.</p> required <code>optimizer_kwargs</code> <code>dict</code> <p>Expected to contain additional arguments for initializing the optimizer. (<code>search_space</code> and <code>objective(s)</code> are set automatically by the test.)</p> required <code>seed</code> <code>Optional[int]</code> <p>(optional) custom seed</p> <code>None</code> Source code in <code>blackboxopt/optimizers/testing.py</code> <pre><code>def respects_fixed_parameter(\n    optimizer_class: Union[\n        Type[SingleObjectiveOptimizer], Type[MultiObjectiveOptimizer]\n    ],\n    optimizer_kwargs: dict,\n    seed: Optional[int] = None,\n):\n    \"\"\"Check if optimizer's generated evaluation specifications contain the values\n    a parameter in the search space was fixed to.\n\n    Args:\n        optimizer_class: Optimizer to test.\n        optimizer_kwargs: Expected to contain additional arguments for initializing\n            the optimizer. (`search_space` and `objective(s)` are set automatically\n            by the test.)\n        seed: (optional) custom seed\n    \"\"\"\n    space = ps.ParameterSpace()\n    space.add(ps.ContinuousParameter(\"my_fixed_param\", (-10.0, 200.0)))\n    space.add(ps.ContinuousParameter(\"x\", (-2.0, 2.0)))\n    space.seed(seed)\n\n    fixed_value = 1.0\n    space.fix(my_fixed_param=fixed_value)\n    opt = _initialize_optimizer(\n        optimizer_class, optimizer_kwargs, space=space, seed=seed\n    )\n    for _ in range(5):\n        es = opt.generate_evaluation_specification()\n        assert es.configuration[\"my_fixed_param\"] == fixed_value\n\n        objectives = {\"loss\": es.configuration[\"x\"] ** 2}\n        if isinstance(opt, MultiObjectiveOptimizer):\n            objectives[\"score\"] = -objectives[\"loss\"]\n\n        opt.report(\n            es.create_evaluation(\n                objectives=objectives,\n                constraints={\"constraint\": 10.0},\n            )\n        )\n</code></pre>"},{"location":"reference/optimizers/staged/bohb/","title":"Bohb","text":""},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler","title":"<code> Sampler            (StagedIterationConfigurationSampler)         </code>","text":"Source code in <code>blackboxopt/optimizers/staged/bohb.py</code> <pre><code>class Sampler(StagedIterationConfigurationSampler):\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objective: Objective,\n        min_samples_in_model: int,\n        top_n_percent: int,\n        num_samples: int,\n        random_fraction: float,\n        bandwidth_factor: float,\n        min_bandwidth: float,\n        seed: Optional[int] = None,\n        logger=None,\n    ):\n        \"\"\"Fits for each given fidelity a kernel density estimator on the best N percent\n        of the evaluated configurations on this fidelity.\n\n        Args:\n            search_space: ConfigurationSpace/ ParameterSpace object.\n            objective: The objective of the optimization.\n            min_samples_in_model: Minimum number of datapoints needed to fit a model.\n            top_n_percent: Determines the percentile of configurations that will be used\n                as training data for the kernel density estimator of the good\n                configuration, e.g if set to 10 the best 10% configurations will be\n                considered for training.\n            num_samples: Number of samples drawn to optimize EI via sampling.\n            random_fraction: Fraction of random configurations returned\n            bandwidth_factor: Widens the bandwidth for contiuous parameters for\n                proposed points to optimize EI\n            min_bandwidth: To keep diversity, even when all (good) samples have the\n                same value for one of the parameters, a minimum bandwidth\n                (reasonable default: 1e-3) is used instead of zero.\n            seed: A seed to make the sampler reproducible.\n            logger: [description]\n\n        Raises:\n            RuntimeError: [description]\n        \"\"\"\n        self.logger = logging.getLogger(\"blackboxopt\") if logger is None else logger\n\n        self.objective = objective\n        self.min_samples_in_model = min_samples_in_model\n        self.top_n_percent = top_n_percent\n        self.search_space = search_space\n        self.bw_factor = bandwidth_factor\n        self.min_bandwidth = min_bandwidth\n        self.seed = seed\n        self._rng = np.random.default_rng(self.seed)\n\n        if self.min_samples_in_model &lt; len(search_space) + 1:\n            self.min_samples_in_model = len(search_space) + 1\n            self.logger.warning(\n                \"Invalid min_samples_in_model value. \"\n                + f\"Setting it to {self.min_samples_in_model}\"\n            )\n\n        self.num_samples = num_samples\n        self.random_fraction = random_fraction\n\n        self.kde_vartypes = \"\"\n\n        vartypes: List[Union[float, int]] = []\n        for hp in search_space:  # type: ignore\n            hp = hp[\"parameter\"]\n            if isinstance(hp, (ps.ContinuousParameter, ps.IntegerParameter)):\n                self.kde_vartypes += \"c\"\n                vartypes.append(0)\n\n            elif isinstance(hp, ps.CategoricalParameter):\n                self.kde_vartypes += \"u\"\n                vartypes.append(hp.num_values)\n\n            elif isinstance(hp, ps.OrdinalParameter):\n                self.kde_vartypes += \"o\"\n                vartypes.append(-hp.num_values)\n            else:\n                raise RuntimeError(f\"This version on BOHB does not support {type(hp)}!\")\n\n        self.vartypes = np.array(vartypes, dtype=int)\n\n        self.configs: Dict[float, List[np.ndarray]] = dict()\n        self.losses: Dict[float, List[float]] = dict()\n        self.kde_models: Dict[float, dict] = dict()\n\n    def sample_configuration(self) -&gt; Tuple[dict, dict]:\n        \"\"\"[summary]\n\n        Returns:\n            [description]\n        \"\"\"\n        self.logger.debug(\"start sampling a new configuration.\")\n\n        # Sample from prior, if no model is available or with given probability\n        if len(self.kde_models) == 0 or self._rng.random() &lt; self.random_fraction:\n            return self.search_space.sample(), {\"model_based_pick\": False}\n\n        best = np.inf\n        best_vector = None\n\n        try:\n            # sample from largest fidelity\n            fidelity = max(self.kde_models.keys())\n\n            good = self.kde_models[fidelity][\"good\"].pdf\n            bad = self.kde_models[fidelity][\"bad\"].pdf\n\n            def minimize_me(x):\n                return max(1e-32, bad(x)) / max(good(x), 1e-32)\n\n            kde_good = self.kde_models[fidelity][\"good\"]\n            kde_bad = self.kde_models[fidelity][\"bad\"]\n\n            for i in range(self.num_samples):\n                idx = self._rng.integers(0, len(kde_good.data))\n                datum = kde_good.data[idx]\n                vector = sample_around_values(\n                    datum,\n                    kde_good.bw,\n                    self.vartypes,\n                    self.min_bandwidth,\n                    self.bw_factor,\n                    rng=self._rng,\n                )\n                if vector is None:\n                    continue\n\n                # Statsmodels KDE estimators relies on seeding through numpy's global\n                # state. We do this close to the evaluation of the PDF (`good`, `bad`)\n                # to increase robustness for multi threading.\n                # As we seed in a loop, we need to change it each iteration to not get\n                # the same random numbers each time.\n                # We also reset the np.random's global state, in case the user relies\n                # on it in other parts of the code and to not hide other determinism\n                # issues.\n                # TODO: Check github issue if there was progress and the seeding can be\n                # removed: https://github.com/statsmodels/statsmodels/issues/306\n                cached_rng_state = None\n                if self.seed:\n                    cached_rng_state = np.random.get_state()\n                    np.random.seed(self.seed + i)\n\n                val = minimize_me(vector)\n\n                if cached_rng_state:\n                    np.random.set_state(cached_rng_state)\n\n                if not np.isfinite(val):\n                    self.logger.warning(\n                        \"sampled vector: %s has EI value %s\" % (vector, val)\n                    )\n                    self.logger.warning(\n                        \"data in the KDEs:\\n%s\\n%s\" % (kde_good.data, kde_bad.data)\n                    )\n                    self.logger.warning(\n                        \"bandwidth of the KDEs:\\n%s\\n%s\" % (kde_good.bw, kde_bad.bw)\n                    )\n\n                    # right now, this happens because a KDE does not contain all values\n                    # for a categorical parameter this cannot be fixed with the\n                    # statsmodels KDE, so for now, we are just going to evaluate this\n                    # one if the good_kde has a finite value, i.e. there is no config\n                    # with that value in the bad kde, so it shouldn't be terrible.\n                    if np.isfinite(good(vector)) and best_vector is not None:\n                        best_vector = vector\n                    continue\n\n                if val &lt; best:\n                    best = val\n                    best_vector = convert_from_statsmodels_kde_representation(\n                        vector, self.vartypes\n                    )\n\n            if best_vector is None:\n                self.logger.debug(\n                    f\"Sampling based optimization with {self.num_samples} samples did \"\n                    + \"not find any finite/numerical acquisition function value \"\n                    + \"-&gt; using random configuration\"\n                )\n                return self.search_space.sample(), {\"model_based_pick\": False}\n            else:\n                self.logger.debug(\n                    \"best_vector: {}, {}, {}, {}\".format(\n                        best_vector, best, good(best_vector), bad(best_vector)\n                    )\n                )\n                return (\n                    self.search_space.from_numerical(best_vector),\n                    {\"model_based_pick\": True},\n                )\n\n        except Exception:\n            self.logger.debug(\n                \"Sample base optimization failed. Falling back to a random sample.\"\n            )\n            return self.search_space.sample(), {\"model_based_pick\": False}\n\n    def digest_evaluation(self, evaluation: Evaluation):\n        \"\"\"[summary]\n\n        Args:\n            evaluation: [description]\n        \"\"\"\n        objective_value = evaluation.objectives[self.objective.name]\n        if objective_value is None:\n            loss = np.inf\n        else:\n            loss = (\n                -objective_value\n                if self.objective.greater_is_better\n                else objective_value\n            )\n        config_vector = self.search_space.to_numerical(evaluation.configuration)\n        config_vector = convert_to_statsmodels_kde_representation(\n            config_vector, self.vartypes\n        )\n\n        fidelity = evaluation.settings[\"fidelity\"]\n\n        if fidelity not in self.configs.keys():\n            self.configs[fidelity] = []\n            self.losses[fidelity] = []\n\n        self.configs[fidelity].append(config_vector)\n        self.losses[fidelity].append(loss)\n\n        if bool(self.kde_models.keys()) and max(self.kde_models.keys()) &gt; fidelity:\n            return\n\n        if np.isfinite(self.losses[fidelity]).sum() &lt;= self.min_samples_in_model - 1:\n            n_runs_finite_loss = np.isfinite(self.losses[fidelity]).sum()\n            self.logger.debug(\n                f\"Only {n_runs_finite_loss} run(s) with a finite loss for fidelity \"\n                + f\"{fidelity} available, need more than \"\n                + f\"{self.min_samples_in_model + 1} -&gt; can't build model!\"\n            )\n            return\n\n        train_configs = np.array(self.configs[fidelity])\n        train_losses = np.array(self.losses[fidelity])\n\n        n_good = max(\n            self.min_samples_in_model,\n            (self.top_n_percent * train_configs.shape[0]) // 100,\n        )\n\n        n_bad = max(\n            self.min_samples_in_model,\n            ((100 - self.top_n_percent) * train_configs.shape[0]) // 100,\n        )\n\n        # Refit KDE for the current fidelity\n        idx = np.argsort(train_losses)\n\n        train_data_good = impute_conditional_data(\n            train_configs[idx[:n_good]], self.vartypes, rng=self._rng\n        )\n        train_data_bad = impute_conditional_data(\n            train_configs[idx[n_good : n_good + n_bad]], self.vartypes, rng=self._rng\n        )\n\n        if train_data_good.shape[0] &lt;= train_data_good.shape[1]:\n            return\n        if train_data_bad.shape[0] &lt;= train_data_bad.shape[1]:\n            return\n\n        # more expensive crossvalidation method\n        # bw_estimation = 'cv_ls'\n        # quick rule of thumb\n        bw_estimation = \"normal_reference\"\n\n        bad_kde = sm.nonparametric.KDEMultivariate(\n            data=train_data_bad,\n            var_type=self.kde_vartypes,\n            bw=bw_estimation,\n        )\n        good_kde = sm.nonparametric.KDEMultivariate(\n            data=train_data_good,\n            var_type=self.kde_vartypes,\n            bw=bw_estimation,\n        )\n\n        bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n        good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n\n        self.kde_models[fidelity] = {\"good\": good_kde, \"bad\": bad_kde}\n\n        # update probs for the categorical parameters for later sampling\n        self.logger.debug(\n            f\"done building a new model for fidelity {fidelity} based on \"\n            + f\"{n_good}/{n_bad} split\\nBest loss for this fidelity: \"\n            + f\"{np.min(train_losses)}\\n\"\n            + (\"=\" * 40)\n        )\n</code></pre>"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler.__init__","title":"<code>__init__(self, search_space, objective, min_samples_in_model, top_n_percent, num_samples, random_fraction, bandwidth_factor, min_bandwidth, seed=None, logger=None)</code>  <code>special</code>","text":"<p>Fits for each given fidelity a kernel density estimator on the best N percent of the evaluated configurations on this fidelity.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>ConfigurationSpace/ ParameterSpace object.</p> required <code>objective</code> <code>Objective</code> <p>The objective of the optimization.</p> required <code>min_samples_in_model</code> <code>int</code> <p>Minimum number of datapoints needed to fit a model.</p> required <code>top_n_percent</code> <code>int</code> <p>Determines the percentile of configurations that will be used as training data for the kernel density estimator of the good configuration, e.g if set to 10 the best 10% configurations will be considered for training.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples drawn to optimize EI via sampling.</p> required <code>random_fraction</code> <code>float</code> <p>Fraction of random configurations returned</p> required <code>bandwidth_factor</code> <code>float</code> <p>Widens the bandwidth for contiuous parameters for proposed points to optimize EI</p> required <code>min_bandwidth</code> <code>float</code> <p>To keep diversity, even when all (good) samples have the same value for one of the parameters, a minimum bandwidth (reasonable default: 1e-3) is used instead of zero.</p> required <code>seed</code> <code>Optional[int]</code> <p>A seed to make the sampler reproducible.</p> <code>None</code> <code>logger</code> <p>[description]</p> <code>None</code> <p>Exceptions:</p> Type Description <code>RuntimeError</code> <p>[description]</p> Source code in <code>blackboxopt/optimizers/staged/bohb.py</code> <pre><code>def __init__(\n    self,\n    search_space: ParameterSpace,\n    objective: Objective,\n    min_samples_in_model: int,\n    top_n_percent: int,\n    num_samples: int,\n    random_fraction: float,\n    bandwidth_factor: float,\n    min_bandwidth: float,\n    seed: Optional[int] = None,\n    logger=None,\n):\n    \"\"\"Fits for each given fidelity a kernel density estimator on the best N percent\n    of the evaluated configurations on this fidelity.\n\n    Args:\n        search_space: ConfigurationSpace/ ParameterSpace object.\n        objective: The objective of the optimization.\n        min_samples_in_model: Minimum number of datapoints needed to fit a model.\n        top_n_percent: Determines the percentile of configurations that will be used\n            as training data for the kernel density estimator of the good\n            configuration, e.g if set to 10 the best 10% configurations will be\n            considered for training.\n        num_samples: Number of samples drawn to optimize EI via sampling.\n        random_fraction: Fraction of random configurations returned\n        bandwidth_factor: Widens the bandwidth for contiuous parameters for\n            proposed points to optimize EI\n        min_bandwidth: To keep diversity, even when all (good) samples have the\n            same value for one of the parameters, a minimum bandwidth\n            (reasonable default: 1e-3) is used instead of zero.\n        seed: A seed to make the sampler reproducible.\n        logger: [description]\n\n    Raises:\n        RuntimeError: [description]\n    \"\"\"\n    self.logger = logging.getLogger(\"blackboxopt\") if logger is None else logger\n\n    self.objective = objective\n    self.min_samples_in_model = min_samples_in_model\n    self.top_n_percent = top_n_percent\n    self.search_space = search_space\n    self.bw_factor = bandwidth_factor\n    self.min_bandwidth = min_bandwidth\n    self.seed = seed\n    self._rng = np.random.default_rng(self.seed)\n\n    if self.min_samples_in_model &lt; len(search_space) + 1:\n        self.min_samples_in_model = len(search_space) + 1\n        self.logger.warning(\n            \"Invalid min_samples_in_model value. \"\n            + f\"Setting it to {self.min_samples_in_model}\"\n        )\n\n    self.num_samples = num_samples\n    self.random_fraction = random_fraction\n\n    self.kde_vartypes = \"\"\n\n    vartypes: List[Union[float, int]] = []\n    for hp in search_space:  # type: ignore\n        hp = hp[\"parameter\"]\n        if isinstance(hp, (ps.ContinuousParameter, ps.IntegerParameter)):\n            self.kde_vartypes += \"c\"\n            vartypes.append(0)\n\n        elif isinstance(hp, ps.CategoricalParameter):\n            self.kde_vartypes += \"u\"\n            vartypes.append(hp.num_values)\n\n        elif isinstance(hp, ps.OrdinalParameter):\n            self.kde_vartypes += \"o\"\n            vartypes.append(-hp.num_values)\n        else:\n            raise RuntimeError(f\"This version on BOHB does not support {type(hp)}!\")\n\n    self.vartypes = np.array(vartypes, dtype=int)\n\n    self.configs: Dict[float, List[np.ndarray]] = dict()\n    self.losses: Dict[float, List[float]] = dict()\n    self.kde_models: Dict[float, dict] = dict()\n</code></pre>"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler.digest_evaluation","title":"<code>digest_evaluation(self, evaluation)</code>","text":"<p>[summary]</p> <p>Parameters:</p> Name Type Description Default <code>evaluation</code> <code>Evaluation</code> <p>[description]</p> required Source code in <code>blackboxopt/optimizers/staged/bohb.py</code> <pre><code>def digest_evaluation(self, evaluation: Evaluation):\n    \"\"\"[summary]\n\n    Args:\n        evaluation: [description]\n    \"\"\"\n    objective_value = evaluation.objectives[self.objective.name]\n    if objective_value is None:\n        loss = np.inf\n    else:\n        loss = (\n            -objective_value\n            if self.objective.greater_is_better\n            else objective_value\n        )\n    config_vector = self.search_space.to_numerical(evaluation.configuration)\n    config_vector = convert_to_statsmodels_kde_representation(\n        config_vector, self.vartypes\n    )\n\n    fidelity = evaluation.settings[\"fidelity\"]\n\n    if fidelity not in self.configs.keys():\n        self.configs[fidelity] = []\n        self.losses[fidelity] = []\n\n    self.configs[fidelity].append(config_vector)\n    self.losses[fidelity].append(loss)\n\n    if bool(self.kde_models.keys()) and max(self.kde_models.keys()) &gt; fidelity:\n        return\n\n    if np.isfinite(self.losses[fidelity]).sum() &lt;= self.min_samples_in_model - 1:\n        n_runs_finite_loss = np.isfinite(self.losses[fidelity]).sum()\n        self.logger.debug(\n            f\"Only {n_runs_finite_loss} run(s) with a finite loss for fidelity \"\n            + f\"{fidelity} available, need more than \"\n            + f\"{self.min_samples_in_model + 1} -&gt; can't build model!\"\n        )\n        return\n\n    train_configs = np.array(self.configs[fidelity])\n    train_losses = np.array(self.losses[fidelity])\n\n    n_good = max(\n        self.min_samples_in_model,\n        (self.top_n_percent * train_configs.shape[0]) // 100,\n    )\n\n    n_bad = max(\n        self.min_samples_in_model,\n        ((100 - self.top_n_percent) * train_configs.shape[0]) // 100,\n    )\n\n    # Refit KDE for the current fidelity\n    idx = np.argsort(train_losses)\n\n    train_data_good = impute_conditional_data(\n        train_configs[idx[:n_good]], self.vartypes, rng=self._rng\n    )\n    train_data_bad = impute_conditional_data(\n        train_configs[idx[n_good : n_good + n_bad]], self.vartypes, rng=self._rng\n    )\n\n    if train_data_good.shape[0] &lt;= train_data_good.shape[1]:\n        return\n    if train_data_bad.shape[0] &lt;= train_data_bad.shape[1]:\n        return\n\n    # more expensive crossvalidation method\n    # bw_estimation = 'cv_ls'\n    # quick rule of thumb\n    bw_estimation = \"normal_reference\"\n\n    bad_kde = sm.nonparametric.KDEMultivariate(\n        data=train_data_bad,\n        var_type=self.kde_vartypes,\n        bw=bw_estimation,\n    )\n    good_kde = sm.nonparametric.KDEMultivariate(\n        data=train_data_good,\n        var_type=self.kde_vartypes,\n        bw=bw_estimation,\n    )\n\n    bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n    good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n\n    self.kde_models[fidelity] = {\"good\": good_kde, \"bad\": bad_kde}\n\n    # update probs for the categorical parameters for later sampling\n    self.logger.debug(\n        f\"done building a new model for fidelity {fidelity} based on \"\n        + f\"{n_good}/{n_bad} split\\nBest loss for this fidelity: \"\n        + f\"{np.min(train_losses)}\\n\"\n        + (\"=\" * 40)\n    )\n</code></pre>"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler.sample_configuration","title":"<code>sample_configuration(self)</code>","text":"<p>[summary]</p> <p>Returns:</p> Type Description <code>Tuple[dict, dict]</code> <p>[description]</p> Source code in <code>blackboxopt/optimizers/staged/bohb.py</code> <pre><code>def sample_configuration(self) -&gt; Tuple[dict, dict]:\n    \"\"\"[summary]\n\n    Returns:\n        [description]\n    \"\"\"\n    self.logger.debug(\"start sampling a new configuration.\")\n\n    # Sample from prior, if no model is available or with given probability\n    if len(self.kde_models) == 0 or self._rng.random() &lt; self.random_fraction:\n        return self.search_space.sample(), {\"model_based_pick\": False}\n\n    best = np.inf\n    best_vector = None\n\n    try:\n        # sample from largest fidelity\n        fidelity = max(self.kde_models.keys())\n\n        good = self.kde_models[fidelity][\"good\"].pdf\n        bad = self.kde_models[fidelity][\"bad\"].pdf\n\n        def minimize_me(x):\n            return max(1e-32, bad(x)) / max(good(x), 1e-32)\n\n        kde_good = self.kde_models[fidelity][\"good\"]\n        kde_bad = self.kde_models[fidelity][\"bad\"]\n\n        for i in range(self.num_samples):\n            idx = self._rng.integers(0, len(kde_good.data))\n            datum = kde_good.data[idx]\n            vector = sample_around_values(\n                datum,\n                kde_good.bw,\n                self.vartypes,\n                self.min_bandwidth,\n                self.bw_factor,\n                rng=self._rng,\n            )\n            if vector is None:\n                continue\n\n            # Statsmodels KDE estimators relies on seeding through numpy's global\n            # state. We do this close to the evaluation of the PDF (`good`, `bad`)\n            # to increase robustness for multi threading.\n            # As we seed in a loop, we need to change it each iteration to not get\n            # the same random numbers each time.\n            # We also reset the np.random's global state, in case the user relies\n            # on it in other parts of the code and to not hide other determinism\n            # issues.\n            # TODO: Check github issue if there was progress and the seeding can be\n            # removed: https://github.com/statsmodels/statsmodels/issues/306\n            cached_rng_state = None\n            if self.seed:\n                cached_rng_state = np.random.get_state()\n                np.random.seed(self.seed + i)\n\n            val = minimize_me(vector)\n\n            if cached_rng_state:\n                np.random.set_state(cached_rng_state)\n\n            if not np.isfinite(val):\n                self.logger.warning(\n                    \"sampled vector: %s has EI value %s\" % (vector, val)\n                )\n                self.logger.warning(\n                    \"data in the KDEs:\\n%s\\n%s\" % (kde_good.data, kde_bad.data)\n                )\n                self.logger.warning(\n                    \"bandwidth of the KDEs:\\n%s\\n%s\" % (kde_good.bw, kde_bad.bw)\n                )\n\n                # right now, this happens because a KDE does not contain all values\n                # for a categorical parameter this cannot be fixed with the\n                # statsmodels KDE, so for now, we are just going to evaluate this\n                # one if the good_kde has a finite value, i.e. there is no config\n                # with that value in the bad kde, so it shouldn't be terrible.\n                if np.isfinite(good(vector)) and best_vector is not None:\n                    best_vector = vector\n                continue\n\n            if val &lt; best:\n                best = val\n                best_vector = convert_from_statsmodels_kde_representation(\n                    vector, self.vartypes\n                )\n\n        if best_vector is None:\n            self.logger.debug(\n                f\"Sampling based optimization with {self.num_samples} samples did \"\n                + \"not find any finite/numerical acquisition function value \"\n                + \"-&gt; using random configuration\"\n            )\n            return self.search_space.sample(), {\"model_based_pick\": False}\n        else:\n            self.logger.debug(\n                \"best_vector: {}, {}, {}, {}\".format(\n                    best_vector, best, good(best_vector), bad(best_vector)\n                )\n            )\n            return (\n                self.search_space.from_numerical(best_vector),\n                {\"model_based_pick\": True},\n            )\n\n    except Exception:\n        self.logger.debug(\n            \"Sample base optimization failed. Falling back to a random sample.\"\n        )\n        return self.search_space.sample(), {\"model_based_pick\": False}\n</code></pre>"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.convert_from_statsmodels_kde_representation","title":"<code>convert_from_statsmodels_kde_representation(array, vartypes)</code>","text":"<p>Convert numerical representation for categoricals and ordinals back into the unit hypercube.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numerical representation of the configurations following the statsmodels convention for categorical and ordinal values being integers.</p> required <code>vartypes</code> <code>Union[list, numpy.ndarray]</code> <p>Encoding of the types of the variables: 0 mean continuous, &gt;0 means categorical with as many different values, and &lt;0 means ordinal with as many values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Numerical representation consistent with a numerical representation in the hypercube.</p> Source code in <code>blackboxopt/optimizers/staged/bohb.py</code> <pre><code>def convert_from_statsmodels_kde_representation(\n    array: np.ndarray, vartypes: Union[list, np.ndarray]\n) -&gt; np.ndarray:\n    \"\"\"Convert numerical representation for categoricals and ordinals back into the unit\n    hypercube.\n\n    Args:\n        array: Numerical representation of the configurations following the statsmodels\n            convention for categorical and ordinal values being integers.\n        vartypes: Encoding of the types of the variables: 0 mean continuous, &gt;0 means\n            categorical with as many different values, and &lt;0 means ordinal with as many\n            values.\n\n    Returns:\n        Numerical representation consistent with a numerical representation in the\n        hypercube.\n    \"\"\"\n    processed_vector = np.copy(array)\n\n    for i in range(len(processed_vector)):\n        if vartypes[i] != 0:\n            num_values = abs(vartypes[i])\n            processed_vector[i] = (processed_vector[i] + 0.5) / num_values\n\n    return processed_vector\n</code></pre>"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.convert_to_statsmodels_kde_representation","title":"<code>convert_to_statsmodels_kde_representation(array, vartypes)</code>","text":"<p>Convert numerical representation for categoricals and ordinals to integers.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numerical representation of the configurations with categorical and ordinal values mapped into the unit hypercube.</p> required <code>vartypes</code> <code>Union[list, numpy.ndarray]</code> <p>Encoding of the types of the variables: 0 mean continuous, &gt;0 means categorical with as many different values, and &lt;0 means ordinal with as many values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Numerical representation consistent with the statsmodels package.</p> Source code in <code>blackboxopt/optimizers/staged/bohb.py</code> <pre><code>def convert_to_statsmodels_kde_representation(\n    array: np.ndarray, vartypes: Union[list, np.ndarray]\n) -&gt; np.ndarray:\n    \"\"\"Convert numerical representation for categoricals and ordinals to integers.\n    Args:\n        array: Numerical representation of the configurations with categorical and\n            ordinal values mapped into the unit hypercube.\n        vartypes: Encoding of the types of the variables: 0 mean continuous, &gt;0 means\n            categorical with as many different values, and &lt;0 means ordinal with as many\n            values.\n\n    Returns:\n        Numerical representation consistent with the statsmodels package.\n    \"\"\"\n    processed_vector = np.copy(array)\n\n    for i in range(len(processed_vector)):\n        if vartypes[i] == 0:\n            continue\n        num_values = abs(vartypes[i])\n        processed_vector[i] = np.around((processed_vector[i] * num_values) - 0.5)\n\n    return processed_vector\n</code></pre>"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.impute_conditional_data","title":"<code>impute_conditional_data(array, vartypes, rng=None)</code>","text":"<p>Impute NaNs in numerical representation with observed values or prior samples.</p> <p>This method is needed to use the <code>statsmodels</code> KDE, which doesn't handle missing values out of the box.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Numerical representation of the configurations which can include NaN values for inactive variables.</p> required <code>vartypes</code> <code>Union[list, numpy.ndarray]</code> <p>Encoding of the types of the variables: 0 mean continuous, &gt;0 means categorical with as many different values, and &lt;0 means ordinal with as many values.</p> required <code>rng</code> <code>Optional[numpy.random._generator.Generator]</code> <p>A random number generator to make the imputation reproducible.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numerical representation where all NaNs have been replaced with observed values or prior samples.</p> Source code in <code>blackboxopt/optimizers/staged/bohb.py</code> <pre><code>def impute_conditional_data(\n    array: np.ndarray,\n    vartypes: Union[list, np.ndarray],\n    rng: Optional[np.random.Generator] = None,\n) -&gt; np.ndarray:\n    \"\"\"Impute NaNs in numerical representation with observed values or prior samples.\n\n    This method is needed to use the `statsmodels` KDE, which doesn't handle missing\n    values out of the box.\n\n    Args:\n        array: Numerical representation of the configurations which can include NaN\n            values for inactive variables.\n        vartypes: Encoding of the types of the variables: 0 mean continuous, &gt;0 means\n            categorical with as many different values, and &lt;0 means ordinal with as many\n            values.\n        rng: A random number generator to make the imputation reproducible.\n    Returns:\n        Numerical representation where all NaNs have been replaced with observed values\n        or prior samples.\n    \"\"\"\n    rng = np.random.default_rng(rng)\n\n    return_array = np.empty_like(array)\n\n    for i in range(array.shape[0]):\n        datum = np.copy(array[i])\n        nan_indices = np.argwhere(np.isnan(datum)).flatten()\n\n        while np.any(nan_indices):\n            nan_idx = nan_indices[0]\n            valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n\n            if len(valid_indices) &gt; 0:\n                # pick one of them at random and overwrite all NaN values\n                row_idx = rng.choice(valid_indices)\n                datum[nan_indices] = array[row_idx, nan_indices]\n\n            else:\n                # no point in the data has this value activated, so fill it with a valid\n                # but random value\n                t = vartypes[nan_idx]\n                if t == 0:\n                    datum[nan_idx] = rng.random()\n                elif t &gt; 0:\n                    datum[nan_idx] = rng.integers(t)\n                elif t &lt; 0:\n                    datum[nan_idx] = rng.integers(-t)\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n        return_array[i, :] = datum\n    return return_array\n</code></pre>"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.sample_around_values","title":"<code>sample_around_values(datum, bandwidths, vartypes, min_bandwidth, bw_factor, rng=None)</code>","text":"<p>Sample numerical representation close to a given datum.</p> <p>This is specific to the KDE in statsmodels and their kernel for the different variable types.</p> <p>Parameters:</p> Name Type Description Default <code>datum</code> <code>ndarray</code> <p>Numerical representation of a configuration that is used as the 'center' for sampling.</p> required <code>bandwidths</code> <code>ndarray</code> <p>Bandwidth of the corresponding kernels in each dimension.</p> required <code>vartypes</code> <code>Union[list, numpy.ndarray]</code> <p>Encoding of the types of the variables: 0 mean continuous, &gt;0 means categorical with as many different values.</p> required <code>min_bandwidth</code> <code>float</code> <p>Smallest allowed bandwidth. Ensures diversity even if all samples agree on a value in a dimension.</p> required <code>bw_factor</code> <code>float</code> <p>To increase diversity, the bandwidth is actually multiplied by this factor before sampling.</p> required <code>rng</code> <code>Optional[numpy.random._generator.Generator]</code> <p>A random number generator to make the sampling reproducible.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[numpy.ndarray]</code> <p>Numerical representation of a configuration close to the provided datum.</p> Source code in <code>blackboxopt/optimizers/staged/bohb.py</code> <pre><code>def sample_around_values(\n    datum: np.ndarray,\n    bandwidths: np.ndarray,\n    vartypes: Union[list, np.ndarray],\n    min_bandwidth: float,\n    bw_factor: float,\n    rng: Optional[np.random.Generator] = None,\n) -&gt; Optional[np.ndarray]:\n    \"\"\"Sample numerical representation close to a given datum.\n\n    This is specific to the KDE in statsmodels and their kernel for the different\n    variable types.\n\n    Args:\n        datum: Numerical representation of a configuration that is used as the 'center'\n            for sampling.\n        bandwidths: Bandwidth of the corresponding kernels in each dimension.\n        vartypes: Encoding of the types of the variables: 0 mean continuous, &gt;0 means\n            categorical with as many different values.\n        min_bandwidth: Smallest allowed bandwidth. Ensures diversity even if all\n            samples agree on a value in a dimension.\n        bw_factor: To increase diversity, the bandwidth is actually multiplied by this\n            factor before sampling.\n        rng: A random number generator to make the sampling reproducible.\n\n    Returns:\n        Numerical representation of a configuration close to the provided datum.\n    \"\"\"\n    rng = np.random.default_rng(rng)\n\n    vector = []\n    for m, bw, t in zip(datum, bandwidths, vartypes):\n        bw = max(bw, min_bandwidth)\n        if t == 0:\n            bw = bw_factor * bw\n            try:\n                v = sps.truncnorm.rvs(\n                    -m / bw, (1 - m) / bw, loc=m, scale=bw, random_state=rng\n                )\n            except Exception:\n                return None\n        elif t &gt; 0:\n            v = m if rng.random() &lt; (1 - bw) else rng.integers(t)\n        else:\n            bw = min(0.9999, bw)  # bandwidth has to be less the one for this kernel!\n            diffs = np.abs(np.arange(-t) - m)\n            probs = 0.5 * (1 - bw) * (bw**diffs)\n            idx = diffs == 0\n            probs[idx] = (idx * (1 - bw))[idx]\n            probs /= probs.sum()\n            v = rng.choice(-t, p=probs)\n        vector.append(v)\n    return np.array(vector)\n</code></pre>"},{"location":"reference/optimizers/staged/configuration_sampler/","title":"Configuration sampler","text":""},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.RandomSearchSampler","title":"<code> RandomSearchSampler            (StagedIterationConfigurationSampler)         </code>","text":"Source code in <code>blackboxopt/optimizers/staged/configuration_sampler.py</code> <pre><code>class RandomSearchSampler(StagedIterationConfigurationSampler):\n    def __init__(self, search_space: ParameterSpace):\n        self.search_space = search_space\n\n    def sample_configuration(self) -&gt; Tuple[dict, dict]:\n        return self.search_space.sample(), {}\n\n    def digest_evaluation(self, evaluation: Evaluation):\n        \"\"\"Random Search is stateless and does nothing with finished evaluations.\"\"\"\n</code></pre>"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.RandomSearchSampler.digest_evaluation","title":"<code>digest_evaluation(self, evaluation)</code>","text":"<p>Random Search is stateless and does nothing with finished evaluations.</p> Source code in <code>blackboxopt/optimizers/staged/configuration_sampler.py</code> <pre><code>def digest_evaluation(self, evaluation: Evaluation):\n    \"\"\"Random Search is stateless and does nothing with finished evaluations.\"\"\"\n</code></pre>"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.RandomSearchSampler.sample_configuration","title":"<code>sample_configuration(self)</code>","text":"<p>Pick the next configuration.</p> <p>Returns:</p> Type Description <code>Tuple[dict, dict]</code> <p>The configuration to be evaluated, Additional information that will be added to the <code>optimizer_info</code> dict.</p> Source code in <code>blackboxopt/optimizers/staged/configuration_sampler.py</code> <pre><code>def sample_configuration(self) -&gt; Tuple[dict, dict]:\n    return self.search_space.sample(), {}\n</code></pre>"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.StagedIterationConfigurationSampler","title":"<code> StagedIterationConfigurationSampler        </code>","text":"<p>Base class for sampling new configurations inside a StagedIterationOpitimzer.</p> Source code in <code>blackboxopt/optimizers/staged/configuration_sampler.py</code> <pre><code>class StagedIterationConfigurationSampler:\n    \"\"\"Base class for sampling new configurations inside a StagedIterationOpitimzer.\"\"\"\n\n    @abc.abstractmethod\n    def sample_configuration(self) -&gt; Tuple[dict, dict]:\n        \"\"\"Pick the next configuration.\n\n        Returns:\n            The configuration to be evaluated,\n            Additional information that will be added to the `optimizer_info` dict.\n        \"\"\"\n\n    @abc.abstractmethod\n    def digest_evaluation(self, evaluation: Evaluation):\n        \"\"\"Register the result of an evaluation.\"\"\"\n</code></pre>"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.StagedIterationConfigurationSampler.digest_evaluation","title":"<code>digest_evaluation(self, evaluation)</code>","text":"<p>Register the result of an evaluation.</p> Source code in <code>blackboxopt/optimizers/staged/configuration_sampler.py</code> <pre><code>@abc.abstractmethod\ndef digest_evaluation(self, evaluation: Evaluation):\n    \"\"\"Register the result of an evaluation.\"\"\"\n</code></pre>"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.StagedIterationConfigurationSampler.sample_configuration","title":"<code>sample_configuration(self)</code>","text":"<p>Pick the next configuration.</p> <p>Returns:</p> Type Description <code>Tuple[dict, dict]</code> <p>The configuration to be evaluated, Additional information that will be added to the <code>optimizer_info</code> dict.</p> Source code in <code>blackboxopt/optimizers/staged/configuration_sampler.py</code> <pre><code>@abc.abstractmethod\ndef sample_configuration(self) -&gt; Tuple[dict, dict]:\n    \"\"\"Pick the next configuration.\n\n    Returns:\n        The configuration to be evaluated,\n        Additional information that will be added to the `optimizer_info` dict.\n    \"\"\"\n</code></pre>"},{"location":"reference/optimizers/staged/hyperband/","title":"Hyperband","text":""},{"location":"reference/optimizers/staged/hyperband/#blackboxopt.optimizers.staged.hyperband.create_hyperband_iteration","title":"<code>create_hyperband_iteration(iteration_index, min_fidelity, max_fidelity, eta, config_sampler, objective, logger)</code>","text":"<p>Optimizer specific way to create a new <code>blackboxopt.optimizer.staged.iteration.StagedIteration</code> object</p> Source code in <code>blackboxopt/optimizers/staged/hyperband.py</code> <pre><code>def create_hyperband_iteration(\n    iteration_index: int,\n    min_fidelity: float,\n    max_fidelity: float,\n    eta: float,\n    config_sampler: StagedIterationConfigurationSampler,\n    objective: Objective,\n    logger: logging.Logger,\n) -&gt; StagedIteration:\n    \"\"\"Optimizer specific way to create a new\n    `blackboxopt.optimizer.staged.iteration.StagedIteration` object\n    \"\"\"\n    # 's_max + 1' in the paper\n    max_num_stages = 1 + int(math.floor(math.log(max_fidelity / min_fidelity, eta)))\n    # 's+1' in the paper\n    num_stages = max_num_stages - (iteration_index % (max_num_stages))\n    num_configs_first_stage = int(\n        math.ceil((max_num_stages / num_stages) * eta ** (num_stages - 1))\n    )\n    num_configs_per_stage = [\n        int(num_configs_first_stage // (eta**i)) for i in range(num_stages)\n    ]\n    fidelities_per_stage = [\n        max_fidelity / eta**i for i in range(num_stages - 1, -1, -1)\n    ]\n    # Hyperband simple draws random configurations, and there is no additional\n    # information that needs to be stored\n    return StagedIteration(\n        iteration_index,\n        num_configs_per_stage,\n        fidelities_per_stage,\n        config_sampler,\n        greedy_promotion,\n        objective,\n        logger=logger,\n    )\n</code></pre>"},{"location":"reference/optimizers/staged/iteration/","title":"Iteration","text":""},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.Datum","title":"<code> Datum        </code>  <code>dataclass</code>","text":"<p>Small container for bookkeeping only.</p> Source code in <code>blackboxopt/optimizers/staged/iteration.py</code> <pre><code>@dataclass\nclass Datum:\n    \"\"\"Small container for bookkeeping only.\"\"\"\n\n    config_key: Tuple[int, int, int]\n    status: str\n    loss: float = float(\"NaN\")\n</code></pre>"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration","title":"<code> StagedIteration        </code>","text":"Source code in <code>blackboxopt/optimizers/staged/iteration.py</code> <pre><code>class StagedIteration:\n    def __init__(\n        self,\n        iteration: int,\n        num_configs: List[int],\n        fidelities: List[float],\n        config_sampler: StagedIterationConfigurationSampler,\n        config_promotion_function: Callable,\n        objective: Objective,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Base class for iterations that compare configurations at different\n        fidelities and race them as in SuccessiveHalving or Hyperband.\n\n        Args:\n            iteration: Index of this iteration.\n            num_configs: Number of configurations in each stage.\n            fidelities: The fidelity for each stage. Must have the same length as\n                `num_configs'.\n            config_sampler: Configuration Sampler object that suggests a new\n                configuration for evaluation given a fidelity.\n            config_promotion_function: Function that decides which configurations are\n                promoted. Check\n                `blackboxopt.optimizers.utils.staged_iteration.greedy_promotion` for\n                the signature.\n            objective: The objective of the optimization.\n            logger: A standard logger to which some debug output might be written.\n        \"\"\"\n        assert len(fidelities) == len(\n            num_configs\n        ), \"Please specify the number of configuration and the fidelities.\"\n        self.logger = logging.getLogger(\"blackboxopt\") if logger is None else logger\n        self.iteration = iteration\n        self.fidelities = fidelities\n        self.num_configs = num_configs\n        self.config_sampler = config_sampler\n        self.config_promotion_function = config_promotion_function\n        self.objective = objective\n        self.current_stage = 0\n        self.evaluation_data: List[List[Datum]] = [[]]\n        self.eval_specs: Dict[Tuple[int, int, int], EvaluationSpecification] = {}\n        self.pending_evaluations: Dict[UUID, int] = {}\n        self.finished = False\n\n    def generate_evaluation_specification(self) -&gt; Optional[EvaluationSpecification]:\n        \"\"\"Pick the next evaluation specification with a budget i.e. fidelity to run.\n\n        Returns:\n            [description]\n        \"\"\"\n        if self.finished:\n            return None\n\n        # try to find a queued entry first\n        for i, d in enumerate(self.evaluation_data[self.current_stage]):\n            if d.status == \"QUEUED\":\n                es = copy.deepcopy(self.eval_specs[d.config_key])\n                es.settings[\"fidelity\"] = self.fidelities[self.current_stage]\n                d.status = \"RUNNING\"\n                self.pending_evaluations[es.optimizer_info[\"id\"]] = i\n                return es\n\n        # sample a new configuration if there are empty slots to be filled\n        if (\n            len(self.evaluation_data[self.current_stage])\n            &lt; self.num_configs[self.current_stage]\n        ):\n            conf_key = (\n                self.iteration,\n                self.current_stage,\n                len(self.evaluation_data[self.current_stage]),\n            )\n            conf, opt_info = self.config_sampler.sample_configuration()\n            opt_info.update({\"configuration_key\": conf_key, \"id\": str(uuid4())})\n            self.eval_specs[conf_key] = EvaluationSpecification(\n                configuration=conf, settings={}, optimizer_info=opt_info\n            )\n            self.evaluation_data[self.current_stage].append(Datum(conf_key, \"QUEUED\"))\n            # To understand recursion, you first must understand recursion :)\n            return self.generate_evaluation_specification()\n\n        # at this point there are pending evaluations and this iteration has to wait\n        return None\n\n    def digest_evaluation(\n        self, evaluation_specificiation_id: UUID, evaluation: Evaluation\n    ):\n        \"\"\"Registers the result of an evaluation.\n\n        Args:\n            evaluation_specificiation_id: [description]\n            evaluation: [description]\n        \"\"\"\n        self.config_sampler.digest_evaluation(evaluation)\n        i = self.pending_evaluations.pop(evaluation_specificiation_id)\n        d = self.evaluation_data[self.current_stage][i]\n        d.status = \"FINISHED\" if not evaluation.all_objectives_none else \"CRASHED\"\n        objective_value = evaluation.objectives[self.objective.name]\n        if objective_value is not None:\n            d.loss = (\n                -objective_value\n                if self.objective.greater_is_better\n                else objective_value\n            )\n\n        # quick check if all configurations have finished yet\n        if len(self.evaluation_data[self.current_stage]) == self.num_configs[\n            self.current_stage\n        ] and all(\n            [\n                e.status in [\"FINISHED\", \"CRASHED\"]\n                for e in self.evaluation_data[self.current_stage]\n            ]\n        ):\n            self._progress_to_next_stage()\n\n    def _progress_to_next_stage(self):\n        \"\"\"Implements logic to promote configurations to the next stage.\"\"\"\n        # filter out crashed configurations\n        data = [\n            d for d in self.evaluation_data[self.current_stage] if np.isfinite(d.loss)\n        ]\n        self.current_stage += 1\n        if self.current_stage == len(self.num_configs):\n            self.finished = True\n            return\n\n        config_keys = self.config_promotion_function(\n            data, self.num_configs[self.current_stage]\n        )\n        self.logger.debug(\n            \"Iteration %i: Advancing configurations %s to stage %i.\",\n            self.iteration,\n            str(config_keys),\n            self.current_stage,\n        )\n        self.evaluation_data.append(\n            [Datum(config_key, \"QUEUED\") for config_key in config_keys]\n        )\n</code></pre>"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration.__init__","title":"<code>__init__(self, iteration, num_configs, fidelities, config_sampler, config_promotion_function, objective, logger=None)</code>  <code>special</code>","text":"<p>Base class for iterations that compare configurations at different fidelities and race them as in SuccessiveHalving or Hyperband.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>Index of this iteration.</p> required <code>num_configs</code> <code>List[int]</code> <p>Number of configurations in each stage.</p> required <code>fidelities</code> <code>List[float]</code> <p>The fidelity for each stage. Must have the same length as `num_configs'.</p> required <code>config_sampler</code> <code>StagedIterationConfigurationSampler</code> <p>Configuration Sampler object that suggests a new configuration for evaluation given a fidelity.</p> required <code>config_promotion_function</code> <code>Callable</code> <p>Function that decides which configurations are promoted. Check <code>blackboxopt.optimizers.utils.staged_iteration.greedy_promotion</code> for the signature.</p> required <code>objective</code> <code>Objective</code> <p>The objective of the optimization.</p> required <code>logger</code> <code>Logger</code> <p>A standard logger to which some debug output might be written.</p> <code>None</code> Source code in <code>blackboxopt/optimizers/staged/iteration.py</code> <pre><code>def __init__(\n    self,\n    iteration: int,\n    num_configs: List[int],\n    fidelities: List[float],\n    config_sampler: StagedIterationConfigurationSampler,\n    config_promotion_function: Callable,\n    objective: Objective,\n    logger: logging.Logger = None,\n):\n    \"\"\"Base class for iterations that compare configurations at different\n    fidelities and race them as in SuccessiveHalving or Hyperband.\n\n    Args:\n        iteration: Index of this iteration.\n        num_configs: Number of configurations in each stage.\n        fidelities: The fidelity for each stage. Must have the same length as\n            `num_configs'.\n        config_sampler: Configuration Sampler object that suggests a new\n            configuration for evaluation given a fidelity.\n        config_promotion_function: Function that decides which configurations are\n            promoted. Check\n            `blackboxopt.optimizers.utils.staged_iteration.greedy_promotion` for\n            the signature.\n        objective: The objective of the optimization.\n        logger: A standard logger to which some debug output might be written.\n    \"\"\"\n    assert len(fidelities) == len(\n        num_configs\n    ), \"Please specify the number of configuration and the fidelities.\"\n    self.logger = logging.getLogger(\"blackboxopt\") if logger is None else logger\n    self.iteration = iteration\n    self.fidelities = fidelities\n    self.num_configs = num_configs\n    self.config_sampler = config_sampler\n    self.config_promotion_function = config_promotion_function\n    self.objective = objective\n    self.current_stage = 0\n    self.evaluation_data: List[List[Datum]] = [[]]\n    self.eval_specs: Dict[Tuple[int, int, int], EvaluationSpecification] = {}\n    self.pending_evaluations: Dict[UUID, int] = {}\n    self.finished = False\n</code></pre>"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration.digest_evaluation","title":"<code>digest_evaluation(self, evaluation_specificiation_id, evaluation)</code>","text":"<p>Registers the result of an evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>evaluation_specificiation_id</code> <code>UUID</code> <p>[description]</p> required <code>evaluation</code> <code>Evaluation</code> <p>[description]</p> required Source code in <code>blackboxopt/optimizers/staged/iteration.py</code> <pre><code>def digest_evaluation(\n    self, evaluation_specificiation_id: UUID, evaluation: Evaluation\n):\n    \"\"\"Registers the result of an evaluation.\n\n    Args:\n        evaluation_specificiation_id: [description]\n        evaluation: [description]\n    \"\"\"\n    self.config_sampler.digest_evaluation(evaluation)\n    i = self.pending_evaluations.pop(evaluation_specificiation_id)\n    d = self.evaluation_data[self.current_stage][i]\n    d.status = \"FINISHED\" if not evaluation.all_objectives_none else \"CRASHED\"\n    objective_value = evaluation.objectives[self.objective.name]\n    if objective_value is not None:\n        d.loss = (\n            -objective_value\n            if self.objective.greater_is_better\n            else objective_value\n        )\n\n    # quick check if all configurations have finished yet\n    if len(self.evaluation_data[self.current_stage]) == self.num_configs[\n        self.current_stage\n    ] and all(\n        [\n            e.status in [\"FINISHED\", \"CRASHED\"]\n            for e in self.evaluation_data[self.current_stage]\n        ]\n    ):\n        self._progress_to_next_stage()\n</code></pre>"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration.generate_evaluation_specification","title":"<code>generate_evaluation_specification(self)</code>","text":"<p>Pick the next evaluation specification with a budget i.e. fidelity to run.</p> <p>Returns:</p> Type Description <code>Optional[blackboxopt.evaluation.EvaluationSpecification]</code> <p>[description]</p> Source code in <code>blackboxopt/optimizers/staged/iteration.py</code> <pre><code>def generate_evaluation_specification(self) -&gt; Optional[EvaluationSpecification]:\n    \"\"\"Pick the next evaluation specification with a budget i.e. fidelity to run.\n\n    Returns:\n        [description]\n    \"\"\"\n    if self.finished:\n        return None\n\n    # try to find a queued entry first\n    for i, d in enumerate(self.evaluation_data[self.current_stage]):\n        if d.status == \"QUEUED\":\n            es = copy.deepcopy(self.eval_specs[d.config_key])\n            es.settings[\"fidelity\"] = self.fidelities[self.current_stage]\n            d.status = \"RUNNING\"\n            self.pending_evaluations[es.optimizer_info[\"id\"]] = i\n            return es\n\n    # sample a new configuration if there are empty slots to be filled\n    if (\n        len(self.evaluation_data[self.current_stage])\n        &lt; self.num_configs[self.current_stage]\n    ):\n        conf_key = (\n            self.iteration,\n            self.current_stage,\n            len(self.evaluation_data[self.current_stage]),\n        )\n        conf, opt_info = self.config_sampler.sample_configuration()\n        opt_info.update({\"configuration_key\": conf_key, \"id\": str(uuid4())})\n        self.eval_specs[conf_key] = EvaluationSpecification(\n            configuration=conf, settings={}, optimizer_info=opt_info\n        )\n        self.evaluation_data[self.current_stage].append(Datum(conf_key, \"QUEUED\"))\n        # To understand recursion, you first must understand recursion :)\n        return self.generate_evaluation_specification()\n\n    # at this point there are pending evaluations and this iteration has to wait\n    return None\n</code></pre>"},{"location":"reference/optimizers/staged/optimizer/","title":"Optimizer","text":""},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer","title":"<code> StagedIterationOptimizer            (SingleObjectiveOptimizer)         </code>","text":"Source code in <code>blackboxopt/optimizers/staged/optimizer.py</code> <pre><code>class StagedIterationOptimizer(SingleObjectiveOptimizer):\n    def __init__(\n        self,\n        search_space: ParameterSpace,\n        objective: Objective,\n        num_iterations: int,\n        seed: Optional[int] = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Base class for optimizers using iterations that compare configurations at\n        different fidelities and race them in stages, like Hyperband or BOHB.\n\n        Args:\n            search_space: [description]\n            objective: [description]\n            num_iterations: The number of iterations that the optimizer will run.\n            seed: [description]\n            logger: [description]\n        \"\"\"\n        super().__init__(search_space=search_space, objective=objective, seed=seed)\n        self.logger = logging.getLogger(\"blackboxopt\") if logger is None else logger\n        self.num_iterations = num_iterations\n        self.iterations: List[StagedIteration] = []\n        self.evaluation_uuid_to_iteration: Dict[str, int] = {}\n        self.pending_configurations: Dict[str, EvaluationSpecification] = {}\n\n    def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n        _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n        call_functions_with_evaluations_and_collect_errors(\n            [super().report, _validate_optimizer_info_id, self._report],\n            _evals,\n        )\n\n    def _report(self, evaluation: Evaluation) -&gt; None:\n        evaluation_specification_id = evaluation.optimizer_info.get(\"id\")\n        self.pending_configurations.pop(str(evaluation_specification_id))\n        idx = self.evaluation_uuid_to_iteration.pop(str(evaluation_specification_id))\n        self.iterations[idx].digest_evaluation(evaluation_specification_id, evaluation)\n\n    def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n        \"\"\"Get next configuration and settings to evaluate.\n\n        Raises:\n            OptimizationComplete: When the optimization run is finished, e.g. when the\n                budget has been exhausted.\n            OptimizerNotReady: When the optimizer is not ready to propose a new\n                evaluation specification.\n        \"\"\"\n        # check if any of the already active iterations returns a configuration and\n        # simply return that\n        for idx, iteration in enumerate(self.iterations):\n            es = iteration.generate_evaluation_specification()\n\n            if es is not None:\n                self.evaluation_uuid_to_iteration[str(es.optimizer_info[\"id\"])] = idx\n                self.pending_configurations[str(es.optimizer_info[\"id\"])] = es\n                return es\n\n        # if that didn't work, check if there another iteration can be started and then\n        # ask it for a configuration\n        if len(self.iterations) &lt; self.num_iterations:\n            self.iterations.append(self._create_new_iteration(len(self.iterations)))\n            es = self.iterations[-1].generate_evaluation_specification()\n            self.evaluation_uuid_to_iteration[str(es.optimizer_info[\"id\"])] = (\n                len(self.iterations) - 1\n            )\n            self.pending_configurations[str(es.optimizer_info[\"id\"])] = es\n            return es\n\n        # check if the optimization is already complete or whether the optimizer is\n        # waiting for evaluation results -&gt; raise corresponding error\n        if all([iteration.finished for iteration in self.iterations]):\n            raise OptimizationComplete\n\n        raise OptimizerNotReady\n\n    @abc.abstractmethod\n    def _create_new_iteration(self, iteration_index):\n        \"\"\"Optimizer specific way to create a new\n        `blackboxopt.optimizer.utils.staged_iteration.StagedIteration` object\n        \"\"\"\n</code></pre>"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer.__init__","title":"<code>__init__(self, search_space, objective, num_iterations, seed=None, logger=None)</code>  <code>special</code>","text":"<p>Base class for optimizers using iterations that compare configurations at different fidelities and race them in stages, like Hyperband or BOHB.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>ParameterSpace</code> <p>[description]</p> required <code>objective</code> <code>Objective</code> <p>[description]</p> required <code>num_iterations</code> <code>int</code> <p>The number of iterations that the optimizer will run.</p> required <code>seed</code> <code>Optional[int]</code> <p>[description]</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>[description]</p> <code>None</code> Source code in <code>blackboxopt/optimizers/staged/optimizer.py</code> <pre><code>def __init__(\n    self,\n    search_space: ParameterSpace,\n    objective: Objective,\n    num_iterations: int,\n    seed: Optional[int] = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"Base class for optimizers using iterations that compare configurations at\n    different fidelities and race them in stages, like Hyperband or BOHB.\n\n    Args:\n        search_space: [description]\n        objective: [description]\n        num_iterations: The number of iterations that the optimizer will run.\n        seed: [description]\n        logger: [description]\n    \"\"\"\n    super().__init__(search_space=search_space, objective=objective, seed=seed)\n    self.logger = logging.getLogger(\"blackboxopt\") if logger is None else logger\n    self.num_iterations = num_iterations\n    self.iterations: List[StagedIteration] = []\n    self.evaluation_uuid_to_iteration: Dict[str, int] = {}\n    self.pending_configurations: Dict[str, EvaluationSpecification] = {}\n</code></pre>"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer.generate_evaluation_specification","title":"<code>generate_evaluation_specification(self)</code>","text":"<p>Get next configuration and settings to evaluate.</p> <p>Exceptions:</p> Type Description <code>OptimizationComplete</code> <p>When the optimization run is finished, e.g. when the budget has been exhausted.</p> <code>OptimizerNotReady</code> <p>When the optimizer is not ready to propose a new evaluation specification.</p> Source code in <code>blackboxopt/optimizers/staged/optimizer.py</code> <pre><code>def generate_evaluation_specification(self) -&gt; EvaluationSpecification:\n    \"\"\"Get next configuration and settings to evaluate.\n\n    Raises:\n        OptimizationComplete: When the optimization run is finished, e.g. when the\n            budget has been exhausted.\n        OptimizerNotReady: When the optimizer is not ready to propose a new\n            evaluation specification.\n    \"\"\"\n    # check if any of the already active iterations returns a configuration and\n    # simply return that\n    for idx, iteration in enumerate(self.iterations):\n        es = iteration.generate_evaluation_specification()\n\n        if es is not None:\n            self.evaluation_uuid_to_iteration[str(es.optimizer_info[\"id\"])] = idx\n            self.pending_configurations[str(es.optimizer_info[\"id\"])] = es\n            return es\n\n    # if that didn't work, check if there another iteration can be started and then\n    # ask it for a configuration\n    if len(self.iterations) &lt; self.num_iterations:\n        self.iterations.append(self._create_new_iteration(len(self.iterations)))\n        es = self.iterations[-1].generate_evaluation_specification()\n        self.evaluation_uuid_to_iteration[str(es.optimizer_info[\"id\"])] = (\n            len(self.iterations) - 1\n        )\n        self.pending_configurations[str(es.optimizer_info[\"id\"])] = es\n        return es\n\n    # check if the optimization is already complete or whether the optimizer is\n    # waiting for evaluation results -&gt; raise corresponding error\n    if all([iteration.finished for iteration in self.iterations]):\n        raise OptimizationComplete\n\n    raise OptimizerNotReady\n</code></pre>"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer.report","title":"<code>report(self, evaluations)</code>","text":"<p>Report one or multiple evaluations to the optimizer.</p> <p>All valid evaluations are processed. Faulty evaluations are not processed, instead an <code>EvaluationsError</code> is raised, which includes the problematic evaluations with their respective Exceptions in the <code>evaluations_with_errors</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations</code> <code>Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]]</code> <p>A single evaluated evaluation specifications, or an iterable of many.</p> required <p>Exceptions:</p> Type Description <code>EvaluationsError</code> <p>Raised when an evaluation could not be processed.</p> Source code in <code>blackboxopt/optimizers/staged/optimizer.py</code> <pre><code>def report(self, evaluations: Union[Evaluation, Iterable[Evaluation]]) -&gt; None:\n    _evals = [evaluations] if isinstance(evaluations, Evaluation) else evaluations\n\n    call_functions_with_evaluations_and_collect_errors(\n        [super().report, _validate_optimizer_info_id, self._report],\n        _evals,\n    )\n</code></pre>"},{"location":"reference/optimizers/staged/utils/","title":"Utils","text":""},{"location":"reference/optimizers/staged/utils/#blackboxopt.optimizers.staged.utils.best_evaluation_at_highest_fidelity","title":"<code>best_evaluation_at_highest_fidelity(evaluations, objective)</code>","text":"<p>From given list of evaluations, get the best in terms of minimal loss at the highest fidelity.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations</code> <code>List[blackboxopt.evaluation.Evaluation]</code> <p>[description]</p> required <code>objective</code> <code>Objective</code> <p>[description]</p> required <p>Returns:</p> Type Description <code>Optional[blackboxopt.evaluation.Evaluation]</code> <p>[description]</p> Source code in <code>blackboxopt/optimizers/staged/utils.py</code> <pre><code>def best_evaluation_at_highest_fidelity(\n    evaluations: List[Evaluation],\n    objective: Objective,\n) -&gt; Optional[Evaluation]:\n    \"\"\"From given list of evaluations, get the best in terms of minimal loss at the\n    highest fidelity.\n\n    Args:\n        evaluations: [description]\n        objective: [description]\n\n    Returns:\n        [description]\n    \"\"\"\n    if not evaluations:\n        return None\n\n    successes = [\n        evaluation\n        for evaluation in evaluations\n        if evaluation.objectives[objective.name] is not None\n    ]\n    if not successes:\n        return None\n\n    successful_fidelities = [\n        evaluation.settings[\"fidelity\"] for evaluation in successes\n    ]\n    if not successful_fidelities:\n        return None\n\n    max_successful_fidelities = max(successful_fidelities)\n    successful_max_fidelity_evaluations = [\n        evaluation\n        for evaluation in successes\n        if evaluation.settings[\"fidelity\"] == max_successful_fidelities\n    ]\n\n    if not successful_max_fidelity_evaluations:\n        return None\n\n    sort_function = max if objective.greater_is_better else min\n    best_evaluation = sort_function(\n        successful_max_fidelity_evaluations, key=lambda e: e.objectives[objective.name]\n    )\n\n    return best_evaluation\n</code></pre>"},{"location":"reference/optimizers/staged/utils/#blackboxopt.optimizers.staged.utils.greedy_promotion","title":"<code>greedy_promotion(data, num_configs)</code>","text":"<p>Promotes the best configurations to the next stage solely relying on the current loss.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[blackboxopt.optimizers.staged.iteration.Datum]</code> <p>List with all successful evaluations for this stage. All failed configurations have already been removed.</p> required <code>num_configs</code> <code>int</code> <p>Maximum number of configurations to be promoted.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of the config_keys to be evaluated on the next higher fidelity. These must only include config_keys found in <code>data</code> und must also be of at most length <code>num_configs</code>. If fewer ids are returned, the remaining configurations for the next stage will be sampled using the <code>config_sample_function</code> of the staged_iteration.</p> Source code in <code>blackboxopt/optimizers/staged/utils.py</code> <pre><code>def greedy_promotion(data: List[Datum], num_configs: int) -&gt; list:\n    \"\"\"Promotes the best configurations to the next stage solely relying on the current\n    loss.\n\n    Args:\n        data: List with all successful evaluations for this stage. All failed\n            configurations have already been removed.\n        num_configs: Maximum number of configurations to be promoted.\n\n    Returns:\n        List of the config_keys to be evaluated on the next higher fidelity. These must\n        only include config_keys found in `data` und must also be of at most length\n        `num_configs`. If fewer ids are returned, the remaining configurations for the\n        next stage will be sampled using the `config_sample_function` of the\n        staged_iteration.\n    \"\"\"\n    losses = [d.loss for d in data]\n    ranks = np.argsort(np.argsort(losses))\n    n = min(num_configs, len(data))\n    return [datum.config_key for rank, datum in zip(ranks, data) if rank &lt; n]\n</code></pre>"},{"location":"reference/visualizations/utils/","title":"Utils","text":""},{"location":"reference/visualizations/utils/#blackboxopt.visualizations.utils.get_incumbent_objective_over_time_single_fidelity","title":"<code>get_incumbent_objective_over_time_single_fidelity(objective, objective_values, times, fidelities, target_fidelity)</code>","text":"<p>Filter for results with given target fidelity and generate incumbent trace.</p> Source code in <code>blackboxopt/visualizations/utils.py</code> <pre><code>def get_incumbent_objective_over_time_single_fidelity(\n    objective: Objective,\n    objective_values: np.ndarray,\n    times: np.ndarray,\n    fidelities: np.ndarray,\n    target_fidelity: float,\n):\n    \"\"\"Filter for results with given target fidelity and generate incumbent trace.\"\"\"\n    # filter out fidelity and take min/max of objective_values\n    idx = np.logical_and(fidelities == target_fidelity, np.isfinite(objective_values))\n    _times = times[idx]\n    if objective.greater_is_better:\n        _objective_values = np.maximum.accumulate(objective_values[idx])\n    else:\n        _objective_values = np.minimum.accumulate(objective_values[idx])\n    # get unique objective values and sort their indices (to be in chronological order)\n    _, idx = np.unique(_objective_values, return_index=True)\n    idx.sort()\n    # find objective_values\n    _objective_values = _objective_values[idx]\n    _times = _times[idx]\n    # add steps where a new incumbent was found\n    _times = np.repeat(_times, 2)[1:]\n    _objective_values = np.repeat(_objective_values, 2)[:-1]\n    # append best value for largest time to extend the lines\n    _times = np.concatenate([_times, np.nanmax(times, keepdims=True)])\n    _objective_values = np.concatenate([_objective_values, _objective_values[-1:]])\n    return _times, _objective_values\n</code></pre>"},{"location":"reference/visualizations/utils/#blackboxopt.visualizations.utils.patch_plotly_io_to_html","title":"<code>patch_plotly_io_to_html(method)</code>","text":"<p>Patch <code>plotly.io.to_html</code> with additional javascript to improve usability.</p> <p>Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets fixed.</p> <p>Injects <code>&lt;script&gt;</code>-tag with content from <code>to_html_patch.js</code> at the end of the HTML output. But only, if the chart title starts with \"[BBO]\" (to minimize side effects, if the user uses <code>plotly.io</code> for something else).</p> <p><code>plotly.io.to_html</code> is also internally used for <code>figure.show()</code> and <code>figure.to_html()</code>, so this is covered, too.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Callable</code> <p>Original <code>plotly.io.to_html</code> method.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Patched method.</p> Source code in <code>blackboxopt/visualizations/utils.py</code> <pre><code>def patch_plotly_io_to_html(method: Callable) -&gt; Callable:\n    \"\"\"Patch `plotly.io.to_html` with additional javascript to improve usability.\n\n    Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets\n    fixed.\n\n    Injects `&lt;script&gt;`-tag with content from `to_html_patch.js` at the end of the HTML\n    output. But only, if the chart title starts with \"[BBO]\" (to minimize side\n    effects, if the user uses `plotly.io` for something else).\n\n    `plotly.io.to_html` is also internally used for `figure.show()` and\n    `figure.to_html()`, so this is covered, too.\n\n    Args:\n        method: Original `plotly.io.to_html` method.\n\n    Returns:\n        Patched method.\n    \"\"\"\n\n    @wraps(method)\n    def wrapped(*args, **kwargs):\n        html = method(*args, **kwargs)\n\n        # Test if title text contains \"[BBO]\"\n        if html.find('\"title\":{\"text\":\"[BBO]') &lt; 0:\n            return html\n\n        js = importlib.resources.read_text(\n            blackboxopt.visualizations, \"to_html_patch.js\"\n        )\n        html_to_inject = f\"&lt;script&gt;{js}&lt;/script&gt;\"\n        insert_idx = html.rfind(\"&lt;/body&gt;\")\n        if insert_idx &gt;= 0:\n            # Full html page got rendered, inject &lt;script&gt; before &lt;\\body&gt;\n            html = html[:insert_idx] + html_to_inject + html[insert_idx:]\n        else:\n            # Only chart part got rendered: append &lt;script&gt; at the end\n            html = html + html_to_inject\n\n        return html\n\n    return wrapped\n</code></pre>"},{"location":"reference/visualizations/visualizer/","title":"Visualizer","text":""},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.create_hover_information","title":"<code>create_hover_information(sections)</code>","text":"<p>Create a hovertemplate which is used to render hover hints in plotly charts.</p> <p>The data for the chart hovertext has to be provided as <code>custom_data</code> attribute to the chart and can be e.g. a list of column names.</p> <p>One oddness is, that in the template the columns can't be referenced by name, but only by index. That's why it is important to have the same ordering in the template as in the <code>custom_data</code> and the reason why this is done together in one function.</p> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>dict</code> <p>Sections to render. The kyeys will show up as the section titles, values are expected to be a list of column names to be rendered under the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] }</p> required <p>Returns:</p> Type Description <code>Tuple[str, List]</code> <p>(plotly hover template, data column names)</p> Source code in <code>blackboxopt/visualizations/visualizer.py</code> <pre><code>def create_hover_information(sections: dict) -&gt; Tuple[str, List]:\n    \"\"\"\n    Create a [hovertemplate](https://plotly.com/python/reference/pie/#pie-hovertemplate)\n    which is used to render hover hints in plotly charts.\n\n    The data for the chart hovertext has to be provided as `custom_data` attribute to\n    the chart and can be e.g. a list of column names.\n\n    One oddness is, that in the template the columns can't be referenced by name, but\n    only by index. That's why it is important to have the same ordering in the template\n    as in the `custom_data` and the reason why this is done together in one function.\n\n    Args:\n        sections: Sections to render. The kyeys will show up as the section titles,\n            values are expected to be a list of column names to be rendered under\n            the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] }\n\n    Returns:\n        (plotly hover template, data column names)\n    \"\"\"\n    template = \"\"\n    idx = 0\n    for section, columns in sections.items():\n        template += f\"&lt;br&gt;&lt;b&gt;{section.replace('_', ' ').title()}&lt;/b&gt;&lt;br&gt;\"\n        for column in columns:\n            template += f\"{column}: %{{customdata[{idx}]}}&lt;br&gt;\"\n            idx += 1\n    template += \"&lt;extra&gt;&lt;/extra&gt;\"\n\n    data_columns: list = sum(sections.values(), [])\n\n    return template, data_columns\n</code></pre>"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.evaluations_to_df","title":"<code>evaluations_to_df(evaluations)</code>","text":"<p>Convert evaluations into multi index dataframe.</p> <p>The evaluations will be casted to dictionaries which will be normalized. The keys of the dicts will be used as secondary column index. Evaluations with one or more missing objective-value will be dropped.</p> <p>Examples:</p> <pre><code>Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...)\n</code></pre> <p>Will be transformed into:</p> <p>|    objectives   | stacktrace | ... |  &lt;- \"group\" index | loss_1 | loss_2 | stacktrace | ... |  &lt;- \"field\" index | ------ | ------ | ---------- | --- | |    1.0 |   -0.0 | None       | ... |</p> Source code in <code>blackboxopt/visualizations/visualizer.py</code> <pre><code>def evaluations_to_df(evaluations: List[Evaluation]) -&gt; pd.DataFrame:\n    \"\"\"Convert evaluations into multi index dataframe.\n\n    The evaluations will be casted to dictionaries which will be normalized.\n    The keys of the dicts will be used as secondary column index. Evaluations\n    with one or more missing objective-value will be dropped.\n\n    Example:\n\n    ```\n    Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...)\n    ```\n\n    Will be transformed into:\n\n    |    objectives   | stacktrace | ... |  &lt;- \"group\" index\n    | loss_1 | loss_2 | stacktrace | ... |  &lt;- \"field\" index\n    | ------ | ------ | ---------- | --- |\n    |    1.0 |   -0.0 | None       | ... |\n    \"\"\"\n    if not evaluations or len(evaluations) == 0:\n        raise NoSuccessfulEvaluationsError\n\n    # Filter out e.g. EvaluationSpecifications which might be passed into\n    evaluations = [e for e in evaluations if isinstance(e, Evaluation)]\n\n    # Transform to dicts, filter out evaluations with missing objectives\n    evaluation_dicts = [e.__dict__ for e in evaluations if not e.any_objective_none]\n\n    if len(evaluation_dicts) == 0:\n        raise NoSuccessfulEvaluationsError\n\n    df = pd.DataFrame(evaluation_dicts)\n\n    # Flatten json/dict columns into single multi-index dataframe\n    dfs_expanded = []\n    for column in df.columns:\n        # Normalize json columns keep original column for non-json columns\n        try:\n            df_temp = pd.json_normalize(df[column], errors=\"ignore\", max_level=0)\n        except AttributeError:\n            df_temp = df[[column]]\n\n        # Use keys of dicts as second level of column index\n        df_temp.columns = pd.MultiIndex.from_product(\n            [[column], df_temp.columns], names=[\"group\", \"field\"]\n        )\n        # Drop empty columns\n        df_temp = df_temp.dropna(axis=1, how=\"all\")\n\n        dfs_expanded.append(df_temp)\n\n    df = pd.concat(dfs_expanded, join=\"outer\", axis=1)\n\n    # Parse datetime columns\n    date_columns = [c for c in df.columns if \"unixtime\" in str(c)]\n    df[date_columns] = df[date_columns].apply(pd.to_datetime, unit=\"s\")\n\n    # Calculate duration in seconds\n    df[\"duration\", \"duration\"] = (\n        df[\"finished_unixtime\", \"finished_unixtime\"]\n        - df[\"created_unixtime\", \"created_unixtime\"]\n    )\n\n    return df\n</code></pre>"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.hypervolume_over_iterations","title":"<code>hypervolume_over_iterations(evaluations_per_optimizer, objectives, reference_point, percentiles=None, hex_colors=None)</code>","text":"<p>Visualize the hypervolume over iterations.</p> <p>In case multiple studies per optimizer are provided, a central tendency as well as variability is visualized.</p> <p>Parameters:</p> Name Type Description Default <code>evaluations_per_optimizer</code> <code>Dict[str, List[List[blackboxopt.evaluation.Evaluation]]]</code> <p>For each key i.e. optimizer, a list of studies which each contain a list of evaluations for the respective study corresponding to the number of iterations.</p> required <code>objectives</code> <code>Sequence[blackboxopt.base.Objective]</code> <p>The objectives to which the reported objective values correspond.</p> required <code>reference_point</code> <code>List[float]</code> <p>The hypervolume reference point.</p> required <code>percentiles</code> <code>Optional[Tuple[float, float, float]]</code> <p>When provided (e.g. <code>(25, 50, 75)</code>) the median is used as the measure of central tendency, while the area between the 25 and 75 percentiles is shaded. In case no percentiles are given, the mean is used as the central tendency and an area indicating the standard error of the mean is shaded.</p> <code>None</code> <code>hex_colors</code> <code>Optional[List[str]]</code> <p>A list of hex color code strings. Defaults to plotly express' Dark24</p> <code>None</code> <p>Returns:</p> Type Description <p>Plotly figure with hypervolume over iterations and a trace per optimizer.</p> Source code in <code>blackboxopt/visualizations/visualizer.py</code> <pre><code>def hypervolume_over_iterations(\n    evaluations_per_optimizer: Dict[str, List[List[Evaluation]]],\n    objectives: Sequence[Objective],\n    reference_point: List[float],\n    percentiles: Optional[Tuple[float, float, float]] = None,\n    hex_colors: Optional[List[str]] = None,\n):\n    \"\"\"Visualize the hypervolume over iterations.\n\n    In case multiple studies per optimizer are provided, a central tendency as well as\n    variability is visualized.\n\n    Args:\n        evaluations_per_optimizer: For each key i.e. optimizer, a list of studies which\n            each contain a list of evaluations for the respective study corresponding to\n            the number of iterations.\n        objectives: The objectives to which the reported objective values correspond.\n        reference_point: The hypervolume reference point.\n        percentiles: When provided (e.g. `(25, 50, 75)`) the median is used as the\n            measure of central tendency, while the area between the 25 and 75\n            percentiles is shaded. In case no percentiles are given, the mean is used as\n            the central tendency and an area indicating the standard error of the mean\n            is shaded.\n        hex_colors: A list of hex color code strings. Defaults to plotly express' Dark24\n\n    Returns:\n        Plotly figure with hypervolume over iterations and a trace per optimizer.\n    \"\"\"\n    if hex_colors is None:\n        hex_colors = px.colors.qualitative.Dark24\n    hex_color_iterator = iter(hex_colors)\n\n    plotly_data = []\n    for optimizer, studies in evaluations_per_optimizer.items():\n        hv_per_study = []\n        for evaluations in studies:\n            iteration_steps = len(evaluations)\n            hvs = [\n                compute_hypervolume(\n                    evaluations[: (step + 1)], objectives, reference_point\n                )\n                for step in range(iteration_steps)\n            ]\n            hv_per_study.append(hvs)\n\n        if percentiles is not None:\n            lower = np.percentile(hv_per_study, percentiles[0], axis=0)\n            central = np.percentile(hv_per_study, percentiles[1], axis=0)\n            upper = np.percentile(hv_per_study, percentiles[2], axis=0)\n        else:\n            central = np.mean(hv_per_study, axis=0)\n            sem = sps.sem(hv_per_study, axis=0)\n            lower = central - sem\n            upper = central + sem\n\n        x_plotted = np.arange(len(central))\n\n        r, g, b = plotly.colors.hex_to_rgb(next(hex_color_iterator))\n        color_line = f\"rgb({r}, {g}, {b})\"\n        color_fill = f\"rgba({r}, {g}, {b}, 0.3)\"\n\n        plotly_data.extend(\n            [\n                go.Scatter(\n                    name=optimizer,\n                    x=x_plotted,\n                    y=central,\n                    mode=\"lines\",\n                    legendgroup=optimizer,\n                    showlegend=True,\n                    line=dict(color=color_line, simplify=True),\n                ),\n                go.Scatter(\n                    x=x_plotted,\n                    y=lower,\n                    mode=\"lines\",\n                    marker=dict(color=color_line),\n                    line=dict(width=0, simplify=True),\n                    legendgroup=optimizer,\n                    showlegend=False,\n                    hoverinfo=\"skip\",\n                ),\n                go.Scatter(\n                    x=x_plotted,\n                    y=upper,\n                    mode=\"lines\",\n                    marker=dict(color=color_line),\n                    line=dict(width=0, simplify=True),\n                    legendgroup=optimizer,\n                    showlegend=False,\n                    hoverinfo=\"skip\",\n                    fillcolor=color_fill,\n                    fill=\"tonexty\",\n                ),\n            ]\n        )\n\n    fig = go.Figure(plotly_data)\n    return fig\n</code></pre>"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.parallel_coordinate_plot_parameters","title":"<code>parallel_coordinate_plot_parameters(evaluations, columns=None, color_by=None)</code>","text":"<p>Create an interactive parallel coordinate plot.</p> <p>Useful to investigate relationships in a higher dimensional search space and the     optimization's objective(s).</p> <p>Parameters:</p> Name Type Description Default <code>evaluations</code> <code>List[blackboxopt.evaluation.Evaluation]</code> <p>Evaluations to plot.</p> required <code>columns</code> <code>Optional[List[str]]</code> <p>Names of columns to show. Can contain parameter names, objective names and settings keys. If <code>None</code>, all parameters, objectives and settings are displayed.</p> <code>None</code> <code>color_by</code> <code>Optional[str]</code> <p>Parameter name, objective name or settings key. The corresponding column will be shown at the very right, it's value will be used for the color scale. If <code>None</code>, all lines have the same color.</p> <code>None</code> <p>Returns:</p> Type Description <p>Plotly figure</p> <p>Raised</p> <p>In case <code>evaluations</code> does not contain at least</p> <p>one successful evaluation (an evaluation with objective value != <code>None</code>).</p> Source code in <code>blackboxopt/visualizations/visualizer.py</code> <pre><code>def parallel_coordinate_plot_parameters(\n    evaluations: List[Evaluation],\n    columns: Optional[List[str]] = None,\n    color_by: Optional[str] = None,\n):\n    \"\"\"Create an interactive parallel coordinate plot.\n\n    Useful to investigate relationships in a higher dimensional search space and the\n        optimization's objective(s).\n\n    Args:\n        evaluations: Evaluations to plot.\n        columns: Names of columns to show. Can contain parameter names, objective names\n            and settings keys. If `None`, all parameters, objectives and settings are\n            displayed.\n        color_by: Parameter name, objective name or settings key. The corresponding\n            column will be shown at the very right, it's value will be used for the\n            color scale. If `None`, all lines have the same color.\n\n    Returns:\n        Plotly figure\n\n    Raised:\n        NoSuccessfulEvaluationsError: In case `evaluations` does not contain at least\n            one successful evaluation (an evaluation with objective value != `None`).\n    \"\"\"\n    if not evaluations:\n        raise NoSuccessfulEvaluationsError\n\n    # Prepare dataframe for visualization\n    df = evaluations_to_df(evaluations)\n\n    # Drop unused columns and indices\n    if \"settings\" in df.columns:\n        df = df[[\"configuration\", \"settings\", \"objectives\"]]\n        settings_cols = df[\"settings\"].columns.to_list()\n    else:\n        df = df[[\"configuration\", \"objectives\"]]\n        settings_cols = []\n    objective_cols = df[\"objectives\"].columns.to_list()\n    df = df.droplevel(0, axis=1)\n\n    # If no columns are specified, use all:\n    if not columns:\n        columns = df.columns.to_list()\n\n    if color_by and color_by not in columns:\n        raise ValueError(\n            f\"Unknown column name in color_by='{color_by}'. Please make sure, that this\"\n            + \"column name is correct and one of the visible columns.\"\n        )\n\n    ambigious_columns = [k for k, v in Counter(df[columns].columns).items() if v &gt; 1]\n    if ambigious_columns:\n        raise ValueError(\n            \"All columns to plot must have a unique name, but those are ambigious: \"\n            + f\"{ambigious_columns}. Either rename parameters/settings/objective to \"\n            + \"be unique or provide only the unambigious ones as `columns` argument.\"\n        )\n\n    # Prepare a coordinate (vertical line) for every column\n    coordinates = []\n    colored_coordinate = {}\n    for column in columns:\n        coordinate: Dict[str, Any] = {}\n\n        if column in objective_cols:\n            coordinate[\"label\"] = f\"&lt;b&gt;Objective: {column}&lt;/b&gt;\"\n        elif column in settings_cols:\n            coordinate[\"label\"] = f\"Setting: {column}\"\n        else:\n            coordinate[\"label\"] = column\n\n        parameter_type = df[column].dtype.name\n        if parameter_type.startswith(\"float\") or parameter_type.startswith(\"int\"):\n            # Handling floats and integers the same, because unfortunately it's hard to\n            # use integers only for ticks and still be robust regarding a large range\n            # of values.\n            coordinate[\"values\"] = df[column]\n        elif parameter_type in [\"object\", \"bool\"]:\n            # Encode categorical values to integers. Unfortunately, ordinal parameters\n            # loose there ordering, as there is no information about the order in the\n            # evaluations.\n            # The string conversion is necessary for unhashable entries, e.g. of\n            # type List, which can't be casted to categories.\n            df[column] = df[column].astype(str).astype(\"category\")\n            categories = df[column].cat.categories.to_list()\n            encoded_categories = list(range(len(categories)))\n            df[column] = df[column].cat.rename_categories(encoded_categories)\n            # Use integer encodings for scale and category values as tick labels\n            coordinate[\"ticktext\"] = categories\n            coordinate[\"tickvals\"] = encoded_categories\n            coordinate[\"values\"] = df[column].astype(\"str\")\n        else:\n            warnings.warn(\n                f\"Ignoring column with unknown type: {column}&lt;{parameter_type}&gt;\"\n            )\n            continue\n\n        if column == color_by:\n            colored_coordinate = coordinate\n        else:\n            coordinates.append(coordinate)\n\n    # Append colored coordinate to the end (right)\n    if colored_coordinate:\n        coordinates.append(colored_coordinate)\n\n    # Plot\n    return go.Figure(\n        data=go.Parcoords(\n            line=dict(\n                # Color lines by objective value\n                color=df[color_by] if color_by else None,\n                colorscale=px.colors.diverging.Tealrose,\n                showscale=True,\n                # Use colorbar as kind of colored extension to the axis\n                colorbar=dict(\n                    thickness=16, x=1, xpad=0, ypad=1, tickmode=\"array\", tickvals=[]\n                ),\n            ),\n            dimensions=coordinates,\n        ),\n        layout=dict(title=\"[BBO] Parallel coordinates plot\"),\n    )\n</code></pre>"}]}