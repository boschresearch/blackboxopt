{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Blackbox Optimization The blackboxopt Python package contains blackbox optimization algorithms with a common interface, along with useful helpers like parallel optimization loops, analysis and visualization tools. Key Features Common Interface The blackboxopt base classes along with the EvaluationSpecification and Evaluation data classes specify a unified interface for different blackbox optimization method implementations. In addition to these interfaces, a standard pytest compatible testsuite is available to ensure functional compatibility of an optimizer implementation with the blackboxopt framework. Optimizers Aside from random search and a Sobol sequence based space filling method, the main ones in this package are Hyperband, BOHB and a BoTorch based Bayesian optimization base implementation. BOHB is provided as a cleaner replacement of the former implementation in HpBandSter . Optimization Loops As part of the blackboxopt.optimization_loops module compatible implementations for optimization loops are avilable bot for local, serial execution as well as for distributed optimization via dask.distributed . Visualizations Interactive visualizations like objective value over time or duration for single objective optimization, as well as an objectives pair plot with a highlighted pareto front for multi objective optimization is available as part of the blackboxopt.visualizations module. Getting Started The following example outlines how a quadratic function can be optimized with random search in a distributed manner. # Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import parameterspace as ps try : import dask.distributed as dd except ImportError : raise ImportError ( \"Unable to import Dask Distributed specific dependencies. \" + \"Make sure to install blackboxopt[dask]\" ) from blackboxopt import Evaluation , EvaluationSpecification , Objective from blackboxopt.optimization_loops.dask_distributed import ( run_optimization_loop , ) from blackboxopt.optimizers.random_search import RandomSearch def evaluation_function ( eval_spec : EvaluationSpecification ) -> Evaluation : return eval_spec . create_evaluation ( objectives = { \"loss\" : eval_spec . configuration [ \"p1\" ] ** 2 }, user_info = { \"weather\" : \"sunny\" }, ) if __name__ == \"__main__\" : space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( - 1.0 , 1.0 ))) optimizer = RandomSearch ( space , [ Objective ( \"loss\" , greater_is_better = False )], max_steps = 1000 , ) evaluations = run_optimization_loop ( optimizer , evaluation_function , dd . Client (), max_evaluations = 100 ) n_successes = len ([ e for e in evaluations if not e . all_objectives_none ]) print ( f \"Successfully evaluated { n_successes } / { len ( evaluations ) } \" ) License blackboxopt is open-sourced under the Apache-2.0 license. See the LICENSE file for details. For a list of other open source components included in blackboxopt , see the file 3rd-party-licenses.txt .","title":"Overview"},{"location":"#blackbox-optimization","text":"The blackboxopt Python package contains blackbox optimization algorithms with a common interface, along with useful helpers like parallel optimization loops, analysis and visualization tools.","title":"Blackbox Optimization"},{"location":"#key-features","text":"","title":"Key Features"},{"location":"#common-interface","text":"The blackboxopt base classes along with the EvaluationSpecification and Evaluation data classes specify a unified interface for different blackbox optimization method implementations. In addition to these interfaces, a standard pytest compatible testsuite is available to ensure functional compatibility of an optimizer implementation with the blackboxopt framework.","title":"Common Interface"},{"location":"#optimizers","text":"Aside from random search and a Sobol sequence based space filling method, the main ones in this package are Hyperband, BOHB and a BoTorch based Bayesian optimization base implementation. BOHB is provided as a cleaner replacement of the former implementation in HpBandSter .","title":"Optimizers"},{"location":"#optimization-loops","text":"As part of the blackboxopt.optimization_loops module compatible implementations for optimization loops are avilable bot for local, serial execution as well as for distributed optimization via dask.distributed .","title":"Optimization Loops"},{"location":"#visualizations","text":"Interactive visualizations like objective value over time or duration for single objective optimization, as well as an objectives pair plot with a highlighted pareto front for multi objective optimization is available as part of the blackboxopt.visualizations module.","title":"Visualizations"},{"location":"#getting-started","text":"The following example outlines how a quadratic function can be optimized with random search in a distributed manner. # Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import parameterspace as ps try : import dask.distributed as dd except ImportError : raise ImportError ( \"Unable to import Dask Distributed specific dependencies. \" + \"Make sure to install blackboxopt[dask]\" ) from blackboxopt import Evaluation , EvaluationSpecification , Objective from blackboxopt.optimization_loops.dask_distributed import ( run_optimization_loop , ) from blackboxopt.optimizers.random_search import RandomSearch def evaluation_function ( eval_spec : EvaluationSpecification ) -> Evaluation : return eval_spec . create_evaluation ( objectives = { \"loss\" : eval_spec . configuration [ \"p1\" ] ** 2 }, user_info = { \"weather\" : \"sunny\" }, ) if __name__ == \"__main__\" : space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( - 1.0 , 1.0 ))) optimizer = RandomSearch ( space , [ Objective ( \"loss\" , greater_is_better = False )], max_steps = 1000 , ) evaluations = run_optimization_loop ( optimizer , evaluation_function , dd . Client (), max_evaluations = 100 ) n_successes = len ([ e for e in evaluations if not e . all_objectives_none ]) print ( f \"Successfully evaluated { n_successes } / { len ( evaluations ) } \" )","title":"Getting Started"},{"location":"#license","text":"blackboxopt is open-sourced under the Apache-2.0 license. See the LICENSE file for details. For a list of other open source components included in blackboxopt , see the file 3rd-party-licenses.txt .","title":"License"},{"location":"examples/dask-distributed/","text":"Dask Distributed # Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import parameterspace as ps try : import dask.distributed as dd except ImportError : raise ImportError ( \"Unable to import Dask Distributed specific dependencies. \" + \"Make sure to install blackboxopt[dask]\" ) from blackboxopt import Evaluation , EvaluationSpecification , Objective from blackboxopt.optimization_loops.dask_distributed import ( run_optimization_loop , ) from blackboxopt.optimizers.random_search import RandomSearch def evaluation_function ( eval_spec : EvaluationSpecification ) -> Evaluation : return eval_spec . create_evaluation ( objectives = { \"loss\" : eval_spec . configuration [ \"p1\" ] ** 2 }, user_info = { \"weather\" : \"sunny\" }, ) if __name__ == \"__main__\" : space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( - 1.0 , 1.0 ))) optimizer = RandomSearch ( space , [ Objective ( \"loss\" , greater_is_better = False )], max_steps = 1000 , ) evaluations = run_optimization_loop ( optimizer , evaluation_function , dd . Client (), max_evaluations = 100 ) n_successes = len ([ e for e in evaluations if not e . all_objectives_none ]) print ( f \"Successfully evaluated { n_successes } / { len ( evaluations ) } \" )","title":"Dask Distributed"},{"location":"examples/dask-distributed/#dask-distributed","text":"# Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import parameterspace as ps try : import dask.distributed as dd except ImportError : raise ImportError ( \"Unable to import Dask Distributed specific dependencies. \" + \"Make sure to install blackboxopt[dask]\" ) from blackboxopt import Evaluation , EvaluationSpecification , Objective from blackboxopt.optimization_loops.dask_distributed import ( run_optimization_loop , ) from blackboxopt.optimizers.random_search import RandomSearch def evaluation_function ( eval_spec : EvaluationSpecification ) -> Evaluation : return eval_spec . create_evaluation ( objectives = { \"loss\" : eval_spec . configuration [ \"p1\" ] ** 2 }, user_info = { \"weather\" : \"sunny\" }, ) if __name__ == \"__main__\" : space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( - 1.0 , 1.0 ))) optimizer = RandomSearch ( space , [ Objective ( \"loss\" , greater_is_better = False )], max_steps = 1000 , ) evaluations = run_optimization_loop ( optimizer , evaluation_function , dd . Client (), max_evaluations = 100 ) n_successes = len ([ e for e in evaluations if not e . all_objectives_none ]) print ( f \"Successfully evaluated { n_successes } / { len ( evaluations ) } \" )","title":"Dask Distributed"},{"location":"examples/multi-objective-multi-param/","text":"Mixed Space & Multi Objective Aside from continuous parameters, different types are supported by parameterspace . In the following example we use continuous, integer and categorical parameters. Also, we are having a glance at optimizing multiple objectives at the same time. # Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import logging import time import numpy as np import parameterspace as ps from sklearn.datasets import load_diabetes from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split import blackboxopt as bbo from blackboxopt.optimization_loops.sequential import run_optimization_loop from blackboxopt.optimizers.space_filling import SpaceFilling # Load a sklearn sample dataset X_TRAIN , X_VALIDATE , Y_TRAIN , Y_VALIDATE = train_test_split ( * load_diabetes ( return_X_y = True ) ) # Set up search space with multiple ML model hyperparameters of different types SPACE = ps . ParameterSpace () SPACE . add ( ps . IntegerParameter ( \"n_estimators\" , bounds = ( 256 , 2048 ), transformation = \"log\" )) SPACE . add ( ps . IntegerParameter ( \"min_samples_leaf\" , bounds = ( 1 , 32 ), transformation = \"log\" )) SPACE . add ( ps . ContinuousParameter ( \"max_samples\" , bounds = ( 0.1 , 1 ))) SPACE . add ( ps . ContinuousParameter ( \"max_features\" , bounds = ( 0.1 , 1 ))) SPACE . add ( ps . IntegerParameter ( \"max_depth\" , bounds = ( 1 , 128 ))) SPACE . add ( ps . CategoricalParameter ( \"criterion\" , values = ( \"squared_error\" , \"poisson\" ))) def evaluation_function ( eval_spec : bbo . EvaluationSpecification , ) -> bbo . Evaluation : \"\"\"Train and evaluate a random forest with given parameter configuration.\"\"\" regr = RandomForestRegressor ( n_estimators = eval_spec . configuration [ \"n_estimators\" ], max_samples = eval_spec . configuration [ \"max_samples\" ], max_features = eval_spec . configuration [ \"max_features\" ], max_depth = eval_spec . configuration [ \"max_depth\" ], min_samples_leaf = eval_spec . configuration [ \"min_samples_leaf\" ], criterion = eval_spec . configuration [ \"criterion\" ], ) start = time . time () regr . fit ( X_TRAIN , Y_TRAIN ) fit_duration = time . time () - start y_pred = regr . predict ( X_VALIDATE ) objectives = { \"R\u00b2\" : r2_score ( Y_VALIDATE , y_pred ), \"Fit Duration\" : fit_duration , \"Max Error\" : np . abs ( Y_VALIDATE - y_pred ) . max (), } evaluation = eval_spec . create_evaluation ( objectives = objectives ) return evaluation def main (): logger = bbo . init_logger ( logging . INFO ) # Create an optimization run based on a parameterspace and optimizer choice optimizer = SpaceFilling ( search_space = SPACE , objectives = [ bbo . Objective ( \"R\u00b2\" , greater_is_better = True ), bbo . Objective ( \"Max Error\" , greater_is_better = False ), bbo . Objective ( \"Fit Duration\" , greater_is_better = False ), ], ) # Fetch new configurations to evaluate until the optimization is done or # a given timeout is reached evaluations = run_optimization_loop ( optimizer = optimizer , evaluation_function = evaluation_function , timeout_s = 60.0 , ) logger . info ( f \"Evaluated { len ( evaluations ) } specifications\" ) pareto_front = bbo . utils . filter_pareto_efficient ( evaluations , optimizer . objectives ) logger . info ( f \" { len ( pareto_front ) } evaluation(s) are pareto efficient\" ) if __name__ == \"__main__\" : main ()","title":"Mixed Space & Multi Objective"},{"location":"examples/multi-objective-multi-param/#mixed-space-multi-objective","text":"Aside from continuous parameters, different types are supported by parameterspace . In the following example we use continuous, integer and categorical parameters. Also, we are having a glance at optimizing multiple objectives at the same time. # Copyright (c) 2020 - for information on the respective copyright owner # see the NOTICE file and/or the repository https://github.com/boschresearch/blackboxopt # # SPDX-License-Identifier: Apache-2.0 import logging import time import numpy as np import parameterspace as ps from sklearn.datasets import load_diabetes from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split import blackboxopt as bbo from blackboxopt.optimization_loops.sequential import run_optimization_loop from blackboxopt.optimizers.space_filling import SpaceFilling # Load a sklearn sample dataset X_TRAIN , X_VALIDATE , Y_TRAIN , Y_VALIDATE = train_test_split ( * load_diabetes ( return_X_y = True ) ) # Set up search space with multiple ML model hyperparameters of different types SPACE = ps . ParameterSpace () SPACE . add ( ps . IntegerParameter ( \"n_estimators\" , bounds = ( 256 , 2048 ), transformation = \"log\" )) SPACE . add ( ps . IntegerParameter ( \"min_samples_leaf\" , bounds = ( 1 , 32 ), transformation = \"log\" )) SPACE . add ( ps . ContinuousParameter ( \"max_samples\" , bounds = ( 0.1 , 1 ))) SPACE . add ( ps . ContinuousParameter ( \"max_features\" , bounds = ( 0.1 , 1 ))) SPACE . add ( ps . IntegerParameter ( \"max_depth\" , bounds = ( 1 , 128 ))) SPACE . add ( ps . CategoricalParameter ( \"criterion\" , values = ( \"squared_error\" , \"poisson\" ))) def evaluation_function ( eval_spec : bbo . EvaluationSpecification , ) -> bbo . Evaluation : \"\"\"Train and evaluate a random forest with given parameter configuration.\"\"\" regr = RandomForestRegressor ( n_estimators = eval_spec . configuration [ \"n_estimators\" ], max_samples = eval_spec . configuration [ \"max_samples\" ], max_features = eval_spec . configuration [ \"max_features\" ], max_depth = eval_spec . configuration [ \"max_depth\" ], min_samples_leaf = eval_spec . configuration [ \"min_samples_leaf\" ], criterion = eval_spec . configuration [ \"criterion\" ], ) start = time . time () regr . fit ( X_TRAIN , Y_TRAIN ) fit_duration = time . time () - start y_pred = regr . predict ( X_VALIDATE ) objectives = { \"R\u00b2\" : r2_score ( Y_VALIDATE , y_pred ), \"Fit Duration\" : fit_duration , \"Max Error\" : np . abs ( Y_VALIDATE - y_pred ) . max (), } evaluation = eval_spec . create_evaluation ( objectives = objectives ) return evaluation def main (): logger = bbo . init_logger ( logging . INFO ) # Create an optimization run based on a parameterspace and optimizer choice optimizer = SpaceFilling ( search_space = SPACE , objectives = [ bbo . Objective ( \"R\u00b2\" , greater_is_better = True ), bbo . Objective ( \"Max Error\" , greater_is_better = False ), bbo . Objective ( \"Fit Duration\" , greater_is_better = False ), ], ) # Fetch new configurations to evaluate until the optimization is done or # a given timeout is reached evaluations = run_optimization_loop ( optimizer = optimizer , evaluation_function = evaluation_function , timeout_s = 60.0 , ) logger . info ( f \"Evaluated { len ( evaluations ) } specifications\" ) pareto_front = bbo . utils . filter_pareto_efficient ( evaluations , optimizer . objectives ) logger . info ( f \" { len ( pareto_front ) } evaluation(s) are pareto efficient\" ) if __name__ == \"__main__\" : main ()","title":"Mixed Space &amp; Multi Objective"},{"location":"examples/overview/","text":"Examples - Overview To get all dependencies for the examples, run: pip install blackboxopt [ examples ] List of available examples Dask Distributed Multi Objective Optimization","title":"Overview"},{"location":"examples/overview/#examples-overview","text":"To get all dependencies for the examples, run: pip install blackboxopt [ examples ]","title":"Examples - Overview"},{"location":"examples/overview/#list-of-available-examples","text":"Dask Distributed Multi Objective Optimization","title":"List of available examples"},{"location":"reference/base/","text":"blackboxopt.base ConstraintsError ( ValueError ) Raised on incomplete or missing constraints. Source code in blackboxopt/base.py class ConstraintsError ( ValueError ): \"\"\"Raised on incomplete or missing constraints.\"\"\" ContextError ( ValueError ) Raised on incomplete or missing context information. Source code in blackboxopt/base.py class ContextError ( ValueError ): \"\"\"Raised on incomplete or missing context information.\"\"\" EvaluationsError ( ValueError ) Raised on invalid evaluations. The problematic evaluations and their respective exceptions are passed in the evaluations_with_errors attribute. Source code in blackboxopt/base.py class EvaluationsError ( ValueError ): \"\"\"Raised on invalid evaluations. The problematic evaluations and their respective exceptions are passed in the `evaluations_with_errors` attribute. \"\"\" def __init__ ( self , evaluations_with_errors : List [ Tuple [ Evaluation , Exception ]]): self . message = ( \"An error with one or more evaluations occurred. Check the \" \"'evaluations_with_errors' attribute of this exception for details.\" ) self . evaluations_with_errors = evaluations_with_errors MultiObjectiveOptimizer ( Optimizer ) Source code in blackboxopt/base.py class MultiObjectiveOptimizer ( Optimizer ): def __init__ ( self , search_space : ParameterSpace , objectives : List [ Objective ], seed : int = None , ) -> None : \"\"\"Initialize the optimizer with an optional seed for reproducibility. Args: search_space: The search space to optimize. objectives: The objectives of the optimization. seed: A seed for the optimizer, which is also used to re-seed the provided search space. \"\"\" super () . __init__ ( search_space = search_space , seed = seed ) _raise_on_duplicate_objective_names ( objectives ) self . objectives = objectives def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , ) generate_evaluation_specification ( self ) inherited Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" report ( self , evaluations ) Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Source code in blackboxopt/base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , ) Objective dataclass Objective(name: str, greater_is_better: bool) Source code in blackboxopt/base.py class Objective : name : str greater_is_better : bool ObjectivesError ( ValueError ) Raised on incomplete or missing objectives. Source code in blackboxopt/base.py class ObjectivesError ( ValueError ): \"\"\"Raised on incomplete or missing objectives.\"\"\" OptimizationComplete ( Exception ) Exception that is raised when the optimization run is finished, e.g. when the budget has been exhausted. Source code in blackboxopt/base.py class OptimizationComplete ( Exception ): \"\"\"Exception that is raised when the optimization run is finished, e.g. when the budget has been exhausted. \"\"\" Optimizer ( ABC ) Abstract base class for blackbox optimizer implementations. Source code in blackboxopt/base.py class Optimizer ( abc . ABC ): \"\"\"Abstract base class for blackbox optimizer implementations.\"\"\" def __init__ ( self , search_space : ParameterSpace , seed : int = None ) -> None : \"\"\"Initialize the optimizer with an optional seed for reproducibility. Args: search_space: The search space to optimize. seed: A seed for the optimizer, which is also used to re-seed the provided search space. \"\"\" super () . __init__ () self . search_space = search_space self . seed = seed if self . seed is not None : self . search_space . seed ( self . seed ) @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" @abc . abstractmethod def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. \"\"\" generate_evaluation_specification ( self ) Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" report ( self , evaluations ) Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Source code in blackboxopt/base.py @abc . abstractmethod def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. \"\"\" OptimizerNotReady ( Exception ) Exception that is raised when the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py class OptimizerNotReady ( Exception ): \"\"\"Exception that is raised when the optimizer is not ready to propose a new evaluation specification. \"\"\" SingleObjectiveOptimizer ( Optimizer ) Source code in blackboxopt/base.py class SingleObjectiveOptimizer ( Optimizer ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , seed : int = None , ) -> None : \"\"\"Initialize the optimizer with an optional seed for reproducibility. Args: search_space: The search space to optimize. objective: The objectives of the optimization. seed: A seed for the optimizer, which is also used to re-seed the provided search space. \"\"\" super () . __init__ ( search_space = search_space , seed = seed ) self . objective = objective def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an `EvaluationsError` is raised, which includes the problematic evaluations with their respective Exceptions in the `evaluations_with_errors` attribute. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. Raises: EvaluationsError: Raised when an evaluation could not be processed. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = [ self . objective ])], _evals , ) generate_evaluation_specification ( self ) inherited Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" report ( self , evaluations ) Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an `EvaluationsError` is raised, which includes the problematic evaluations with their respective Exceptions in the `evaluations_with_errors` attribute. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. Raises: EvaluationsError: Raised when an evaluation could not be processed. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = [ self . objective ])], _evals , ) call_functions_with_evaluations_and_collect_errors ( functions , evaluations ) The given evaluations are passed to all given functions in order and the first Exception that occurrs for an evaluation is recorded alongside the evaluation and raised together with all erroneous evaluations as part of an EvaluationsError . NOTE: Even if reporting some evaluations fails, all that can be are successfully reported. Also, if an evaluation passes through some of the functions before causing issues, the effect the evaluation had on the previous functions can't be reverted. Exceptions: Type Description EvaluationsError In case exceptions occurred when calling the functions with the evaluations. Source code in blackboxopt/base.py def call_functions_with_evaluations_and_collect_errors ( functions : Iterable [ Callable [[ Evaluation ], None ]], evaluations : Iterable [ Evaluation ], ) -> None : \"\"\"The given evaluations are passed to all given functions in order and the first Exception that occurrs for an evaluation is recorded alongside the evaluation and raised together with all erroneous evaluations as part of an `EvaluationsError`. NOTE: Even if reporting some evaluations fails, all that can be are successfully reported. Also, if an evaluation passes through some of the functions before causing issues, the effect the evaluation had on the previous functions can't be reverted. Raises: EvaluationsError: In case exceptions occurred when calling the functions with the evaluations. \"\"\" evaluations_with_errors = [] for evaluation in evaluations : for func in functions : try : func ( evaluation ) except EvaluationsError as e : evaluations_with_errors . extend ( e . evaluations_with_errors ) break except Exception as e : evaluations_with_errors . append (( evaluation , e )) break if evaluations_with_errors : raise EvaluationsError ( evaluations_with_errors ) raise_on_unknown_or_incomplete ( exception , known , reported ) Raise the given exception if not all known strings are contained in reported or the other way around. Source code in blackboxopt/base.py def raise_on_unknown_or_incomplete ( exception : Type [ ValueError ], known : Iterable [ str ], reported : Iterable [ str ] ) -> None : \"\"\"Raise the given exception if not all known strings are contained in reported or the other way around. \"\"\" known_set = set ( known ) reported_set = set ( reported ) unknown = reported_set - known_set if unknown : raise exception ( f \"Unknown reported: { list ( unknown ) } . Valid are only: { list ( known_set ) } \" ) missing = known_set - reported_set if missing : raise exception ( f \"Missing: { list ( missing ) } \" )","title":"Base"},{"location":"reference/base/#blackboxopt.base","text":"","title":"base"},{"location":"reference/base/#blackboxopt.base.ConstraintsError","text":"Raised on incomplete or missing constraints. Source code in blackboxopt/base.py class ConstraintsError ( ValueError ): \"\"\"Raised on incomplete or missing constraints.\"\"\"","title":"ConstraintsError"},{"location":"reference/base/#blackboxopt.base.ContextError","text":"Raised on incomplete or missing context information. Source code in blackboxopt/base.py class ContextError ( ValueError ): \"\"\"Raised on incomplete or missing context information.\"\"\"","title":"ContextError"},{"location":"reference/base/#blackboxopt.base.EvaluationsError","text":"Raised on invalid evaluations. The problematic evaluations and their respective exceptions are passed in the evaluations_with_errors attribute. Source code in blackboxopt/base.py class EvaluationsError ( ValueError ): \"\"\"Raised on invalid evaluations. The problematic evaluations and their respective exceptions are passed in the `evaluations_with_errors` attribute. \"\"\" def __init__ ( self , evaluations_with_errors : List [ Tuple [ Evaluation , Exception ]]): self . message = ( \"An error with one or more evaluations occurred. Check the \" \"'evaluations_with_errors' attribute of this exception for details.\" ) self . evaluations_with_errors = evaluations_with_errors","title":"EvaluationsError"},{"location":"reference/base/#blackboxopt.base.MultiObjectiveOptimizer","text":"Source code in blackboxopt/base.py class MultiObjectiveOptimizer ( Optimizer ): def __init__ ( self , search_space : ParameterSpace , objectives : List [ Objective ], seed : int = None , ) -> None : \"\"\"Initialize the optimizer with an optional seed for reproducibility. Args: search_space: The search space to optimize. objectives: The objectives of the optimization. seed: A seed for the optimizer, which is also used to re-seed the provided search space. \"\"\" super () . __init__ ( search_space = search_space , seed = seed ) _raise_on_duplicate_objective_names ( objectives ) self . objectives = objectives def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , )","title":"MultiObjectiveOptimizer"},{"location":"reference/base/#blackboxopt.base.MultiObjectiveOptimizer.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\"","title":"generate_evaluation_specification()"},{"location":"reference/base/#blackboxopt.base.MultiObjectiveOptimizer.report","text":"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Source code in blackboxopt/base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , )","title":"report()"},{"location":"reference/base/#blackboxopt.base.Objective","text":"Objective(name: str, greater_is_better: bool) Source code in blackboxopt/base.py class Objective : name : str greater_is_better : bool","title":"Objective"},{"location":"reference/base/#blackboxopt.base.ObjectivesError","text":"Raised on incomplete or missing objectives. Source code in blackboxopt/base.py class ObjectivesError ( ValueError ): \"\"\"Raised on incomplete or missing objectives.\"\"\"","title":"ObjectivesError"},{"location":"reference/base/#blackboxopt.base.OptimizationComplete","text":"Exception that is raised when the optimization run is finished, e.g. when the budget has been exhausted. Source code in blackboxopt/base.py class OptimizationComplete ( Exception ): \"\"\"Exception that is raised when the optimization run is finished, e.g. when the budget has been exhausted. \"\"\"","title":"OptimizationComplete"},{"location":"reference/base/#blackboxopt.base.Optimizer","text":"Abstract base class for blackbox optimizer implementations. Source code in blackboxopt/base.py class Optimizer ( abc . ABC ): \"\"\"Abstract base class for blackbox optimizer implementations.\"\"\" def __init__ ( self , search_space : ParameterSpace , seed : int = None ) -> None : \"\"\"Initialize the optimizer with an optional seed for reproducibility. Args: search_space: The search space to optimize. seed: A seed for the optimizer, which is also used to re-seed the provided search space. \"\"\" super () . __init__ () self . search_space = search_space self . seed = seed if self . seed is not None : self . search_space . seed ( self . seed ) @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" @abc . abstractmethod def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. \"\"\"","title":"Optimizer"},{"location":"reference/base/#blackboxopt.base.Optimizer.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\"","title":"generate_evaluation_specification()"},{"location":"reference/base/#blackboxopt.base.Optimizer.report","text":"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Source code in blackboxopt/base.py @abc . abstractmethod def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. \"\"\"","title":"report()"},{"location":"reference/base/#blackboxopt.base.OptimizerNotReady","text":"Exception that is raised when the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py class OptimizerNotReady ( Exception ): \"\"\"Exception that is raised when the optimizer is not ready to propose a new evaluation specification. \"\"\"","title":"OptimizerNotReady"},{"location":"reference/base/#blackboxopt.base.SingleObjectiveOptimizer","text":"Source code in blackboxopt/base.py class SingleObjectiveOptimizer ( Optimizer ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , seed : int = None , ) -> None : \"\"\"Initialize the optimizer with an optional seed for reproducibility. Args: search_space: The search space to optimize. objective: The objectives of the optimization. seed: A seed for the optimizer, which is also used to re-seed the provided search space. \"\"\" super () . __init__ ( search_space = search_space , seed = seed ) self . objective = objective def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an `EvaluationsError` is raised, which includes the problematic evaluations with their respective Exceptions in the `evaluations_with_errors` attribute. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. Raises: EvaluationsError: Raised when an evaluation could not be processed. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = [ self . objective ])], _evals , )","title":"SingleObjectiveOptimizer"},{"location":"reference/base/#blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/base.py @abc . abstractmethod def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\"","title":"generate_evaluation_specification()"},{"location":"reference/base/#blackboxopt.base.SingleObjectiveOptimizer.report","text":"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an `EvaluationsError` is raised, which includes the problematic evaluations with their respective Exceptions in the `evaluations_with_errors` attribute. Args: evaluations: A single evaluated evaluation specifications, or an iterable of many. Raises: EvaluationsError: Raised when an evaluation could not be processed. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = [ self . objective ])], _evals , )","title":"report()"},{"location":"reference/base/#blackboxopt.base.call_functions_with_evaluations_and_collect_errors","text":"The given evaluations are passed to all given functions in order and the first Exception that occurrs for an evaluation is recorded alongside the evaluation and raised together with all erroneous evaluations as part of an EvaluationsError . NOTE: Even if reporting some evaluations fails, all that can be are successfully reported. Also, if an evaluation passes through some of the functions before causing issues, the effect the evaluation had on the previous functions can't be reverted. Exceptions: Type Description EvaluationsError In case exceptions occurred when calling the functions with the evaluations. Source code in blackboxopt/base.py def call_functions_with_evaluations_and_collect_errors ( functions : Iterable [ Callable [[ Evaluation ], None ]], evaluations : Iterable [ Evaluation ], ) -> None : \"\"\"The given evaluations are passed to all given functions in order and the first Exception that occurrs for an evaluation is recorded alongside the evaluation and raised together with all erroneous evaluations as part of an `EvaluationsError`. NOTE: Even if reporting some evaluations fails, all that can be are successfully reported. Also, if an evaluation passes through some of the functions before causing issues, the effect the evaluation had on the previous functions can't be reverted. Raises: EvaluationsError: In case exceptions occurred when calling the functions with the evaluations. \"\"\" evaluations_with_errors = [] for evaluation in evaluations : for func in functions : try : func ( evaluation ) except EvaluationsError as e : evaluations_with_errors . extend ( e . evaluations_with_errors ) break except Exception as e : evaluations_with_errors . append (( evaluation , e )) break if evaluations_with_errors : raise EvaluationsError ( evaluations_with_errors )","title":"call_functions_with_evaluations_and_collect_errors()"},{"location":"reference/base/#blackboxopt.base.raise_on_unknown_or_incomplete","text":"Raise the given exception if not all known strings are contained in reported or the other way around. Source code in blackboxopt/base.py def raise_on_unknown_or_incomplete ( exception : Type [ ValueError ], known : Iterable [ str ], reported : Iterable [ str ] ) -> None : \"\"\"Raise the given exception if not all known strings are contained in reported or the other way around. \"\"\" known_set = set ( known ) reported_set = set ( reported ) unknown = reported_set - known_set if unknown : raise exception ( f \"Unknown reported: { list ( unknown ) } . Valid are only: { list ( known_set ) } \" ) missing = known_set - reported_set if missing : raise exception ( f \"Missing: { list ( missing ) } \" )","title":"raise_on_unknown_or_incomplete()"},{"location":"reference/evaluation/","text":"blackboxopt.evaluation Evaluation ( EvaluationSpecification , _EvaluationBase ) dataclass An evaluated specification with a timestamp indicating the time of the evaluation, and a result dictionary for all objective values. NOTE: NaN is not allowed as an objective value, use None instead. Source code in blackboxopt/evaluation.py class Evaluation ( EvaluationSpecification , _EvaluationBase ): \"\"\"An evaluated specification with a timestamp indicating the time of the evaluation, and a result dictionary for all objective values. NOTE: `NaN` is not allowed as an objective value, use `None` instead. \"\"\" constraints : Optional [ Dict [ str , Optional [ float ]]] = field ( default = None , metadata = { \"Description\" : \"For each constraint name the float value indicates \" + \"how much the constraint was satisfied, with negative values implying \" + \"a violated and positive values indicating a satisfied constraint.\" }, ) finished_unixtime : float = field ( default_factory = _datetime_now_timestamp , metadata = { \"Description\" : \"Timestamp at completion of this evaluation.\" }, ) stacktrace : Optional [ str ] = field ( default = None , metadata = { \"Description\" : \"The stacktrace in case an unhandled exception occurred \" + \"inside the evaluation function.\" }, ) user_info : Optional [ dict ] = field ( default = None , metadata = { \"Description\" : \"Miscellaneous information provided by the user.\" }, ) def get_specification ( self , reset_created_unixtime : bool = False ) -> EvaluationSpecification : \"\"\"Get the evaluation specifiation for which this result was evaluated.\"\"\" eval_spec_kwargs = deepcopy ( dict ( configuration = self . configuration , settings = self . settings , optimizer_info = self . optimizer_info , context = self . context , ) ) if reset_created_unixtime : return EvaluationSpecification ( created_unixtime = _datetime_now_timestamp (), ** eval_spec_kwargs ) return EvaluationSpecification ( created_unixtime = self . created_unixtime , ** eval_spec_kwargs ) @property def any_objective_none ( self ) -> bool : return any ([ v is None for v in self . objectives . values ()]) @property def all_objectives_none ( self ) -> bool : return all ([ v is None for v in self . objectives . values ()]) create_evaluation ( self , objectives , constraints = None , user_info = None , stacktrace = None , finished_unixtime = None ) inherited Create a blackboxopt.Evaluation based on this evaluation specification. Parameters: Name Type Description Default objectives Dict[str, Optional[float]] For each objective name the respective value. required constraints Optional[Dict[str, Union[float, NoneType]]] For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. None user_info Optional[dict] Miscellaneous information provided by the user. None stacktrace Optional[str] The stacktrace in case an unhandled exception occurred inside the evaluation function. None finished_unixtime Optional[float] Timestamp at completion of this evaluation. If none is provided, the current time is used. None Source code in blackboxopt/evaluation.py def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation get ( self , key , default = None ) inherited D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in blackboxopt/evaluation.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default get_specification ( self , reset_created_unixtime = False ) Get the evaluation specifiation for which this result was evaluated. Source code in blackboxopt/evaluation.py def get_specification ( self , reset_created_unixtime : bool = False ) -> EvaluationSpecification : \"\"\"Get the evaluation specifiation for which this result was evaluated.\"\"\" eval_spec_kwargs = deepcopy ( dict ( configuration = self . configuration , settings = self . settings , optimizer_info = self . optimizer_info , context = self . context , ) ) if reset_created_unixtime : return EvaluationSpecification ( created_unixtime = _datetime_now_timestamp (), ** eval_spec_kwargs ) return EvaluationSpecification ( created_unixtime = self . created_unixtime , ** eval_spec_kwargs ) items ( self ) inherited D.items() -> a set-like object providing a view on D's items Source code in blackboxopt/evaluation.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self ) keys ( self ) inherited D.keys() -> a set-like object providing a view on D's keys Source code in blackboxopt/evaluation.py def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member values ( self ) inherited D.values() -> an object providing a view on D's values Source code in blackboxopt/evaluation.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self ) EvaluationSpecification ( Mapping , Generic ) dataclass EvaluationSpecification( args, *kwds) Source code in blackboxopt/evaluation.py class EvaluationSpecification ( Mapping [ str , Any ]): configuration : dict = field ( metadata = { \"Description\" : \"The configuration to be evaluated next.\" } ) settings : dict = field ( default_factory = dict , metadata = { \"Description\" : \"Additional settings like the fidelity or target task.\" }, ) optimizer_info : dict = field ( default_factory = dict , metadata = { \"Description\" : \"Information about and for internal optimizer state.\" }, ) created_unixtime : float = field ( default_factory = _datetime_now_timestamp , metadata = { \"Description\" : \"Creation time of the evaluation specificiation.\" }, ) context : Optional [ Dict [ str , Any ]] = field ( default = None , metadata = { \"Description\" : \"Contextual information is what you can determine but not \" + \"influence, like the environmental temperature.\" }, ) def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation def __getitem__ ( self , key ): if key not in self . __dataclass_fields__ : # pylint: disable=no-member raise KeyError ( f \"Only dataclass fields are accessible via __getitem__, ' { key } ' is not.\" ) return deepcopy ( getattr ( self , key )) def __iter__ ( self ): return self . __dataclass_fields__ . __iter__ # pylint: disable=no-member def __len__ ( self ): return self . __dataclass_fields__ . __len__ # pylint: disable=no-member def to_json ( self , ** json_dump_kwargs ): return json . dumps ( asdict ( self ), ** json_dump_kwargs ) def to_dict ( self ): return self . __dict__ create_evaluation ( self , objectives , constraints = None , user_info = None , stacktrace = None , finished_unixtime = None ) Create a blackboxopt.Evaluation based on this evaluation specification. Parameters: Name Type Description Default objectives Dict[str, Optional[float]] For each objective name the respective value. required constraints Optional[Dict[str, Union[float, NoneType]]] For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. None user_info Optional[dict] Miscellaneous information provided by the user. None stacktrace Optional[str] The stacktrace in case an unhandled exception occurred inside the evaluation function. None finished_unixtime Optional[float] Timestamp at completion of this evaluation. If none is provided, the current time is used. None Source code in blackboxopt/evaluation.py def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation get ( self , key , default = None ) inherited D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in blackboxopt/evaluation.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default items ( self ) inherited D.items() -> a set-like object providing a view on D's items Source code in blackboxopt/evaluation.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self ) keys ( self ) D.keys() -> a set-like object providing a view on D's keys Source code in blackboxopt/evaluation.py def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member values ( self ) inherited D.values() -> an object providing a view on D's values Source code in blackboxopt/evaluation.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self )","title":"Evaluation"},{"location":"reference/evaluation/#blackboxopt.evaluation","text":"","title":"evaluation"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation","text":"An evaluated specification with a timestamp indicating the time of the evaluation, and a result dictionary for all objective values. NOTE: NaN is not allowed as an objective value, use None instead. Source code in blackboxopt/evaluation.py class Evaluation ( EvaluationSpecification , _EvaluationBase ): \"\"\"An evaluated specification with a timestamp indicating the time of the evaluation, and a result dictionary for all objective values. NOTE: `NaN` is not allowed as an objective value, use `None` instead. \"\"\" constraints : Optional [ Dict [ str , Optional [ float ]]] = field ( default = None , metadata = { \"Description\" : \"For each constraint name the float value indicates \" + \"how much the constraint was satisfied, with negative values implying \" + \"a violated and positive values indicating a satisfied constraint.\" }, ) finished_unixtime : float = field ( default_factory = _datetime_now_timestamp , metadata = { \"Description\" : \"Timestamp at completion of this evaluation.\" }, ) stacktrace : Optional [ str ] = field ( default = None , metadata = { \"Description\" : \"The stacktrace in case an unhandled exception occurred \" + \"inside the evaluation function.\" }, ) user_info : Optional [ dict ] = field ( default = None , metadata = { \"Description\" : \"Miscellaneous information provided by the user.\" }, ) def get_specification ( self , reset_created_unixtime : bool = False ) -> EvaluationSpecification : \"\"\"Get the evaluation specifiation for which this result was evaluated.\"\"\" eval_spec_kwargs = deepcopy ( dict ( configuration = self . configuration , settings = self . settings , optimizer_info = self . optimizer_info , context = self . context , ) ) if reset_created_unixtime : return EvaluationSpecification ( created_unixtime = _datetime_now_timestamp (), ** eval_spec_kwargs ) return EvaluationSpecification ( created_unixtime = self . created_unixtime , ** eval_spec_kwargs ) @property def any_objective_none ( self ) -> bool : return any ([ v is None for v in self . objectives . values ()]) @property def all_objectives_none ( self ) -> bool : return all ([ v is None for v in self . objectives . values ()])","title":"Evaluation"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.create_evaluation","text":"Create a blackboxopt.Evaluation based on this evaluation specification. Parameters: Name Type Description Default objectives Dict[str, Optional[float]] For each objective name the respective value. required constraints Optional[Dict[str, Union[float, NoneType]]] For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. None user_info Optional[dict] Miscellaneous information provided by the user. None stacktrace Optional[str] The stacktrace in case an unhandled exception occurred inside the evaluation function. None finished_unixtime Optional[float] Timestamp at completion of this evaluation. If none is provided, the current time is used. None Source code in blackboxopt/evaluation.py def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation","title":"create_evaluation()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.get","text":"D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in blackboxopt/evaluation.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default","title":"get()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.get_specification","text":"Get the evaluation specifiation for which this result was evaluated. Source code in blackboxopt/evaluation.py def get_specification ( self , reset_created_unixtime : bool = False ) -> EvaluationSpecification : \"\"\"Get the evaluation specifiation for which this result was evaluated.\"\"\" eval_spec_kwargs = deepcopy ( dict ( configuration = self . configuration , settings = self . settings , optimizer_info = self . optimizer_info , context = self . context , ) ) if reset_created_unixtime : return EvaluationSpecification ( created_unixtime = _datetime_now_timestamp (), ** eval_spec_kwargs ) return EvaluationSpecification ( created_unixtime = self . created_unixtime , ** eval_spec_kwargs )","title":"get_specification()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.items","text":"D.items() -> a set-like object providing a view on D's items Source code in blackboxopt/evaluation.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self )","title":"items()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.keys","text":"D.keys() -> a set-like object providing a view on D's keys Source code in blackboxopt/evaluation.py def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member","title":"keys()"},{"location":"reference/evaluation/#blackboxopt.evaluation.Evaluation.values","text":"D.values() -> an object providing a view on D's values Source code in blackboxopt/evaluation.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self )","title":"values()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification","text":"EvaluationSpecification( args, *kwds) Source code in blackboxopt/evaluation.py class EvaluationSpecification ( Mapping [ str , Any ]): configuration : dict = field ( metadata = { \"Description\" : \"The configuration to be evaluated next.\" } ) settings : dict = field ( default_factory = dict , metadata = { \"Description\" : \"Additional settings like the fidelity or target task.\" }, ) optimizer_info : dict = field ( default_factory = dict , metadata = { \"Description\" : \"Information about and for internal optimizer state.\" }, ) created_unixtime : float = field ( default_factory = _datetime_now_timestamp , metadata = { \"Description\" : \"Creation time of the evaluation specificiation.\" }, ) context : Optional [ Dict [ str , Any ]] = field ( default = None , metadata = { \"Description\" : \"Contextual information is what you can determine but not \" + \"influence, like the environmental temperature.\" }, ) def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation def __getitem__ ( self , key ): if key not in self . __dataclass_fields__ : # pylint: disable=no-member raise KeyError ( f \"Only dataclass fields are accessible via __getitem__, ' { key } ' is not.\" ) return deepcopy ( getattr ( self , key )) def __iter__ ( self ): return self . __dataclass_fields__ . __iter__ # pylint: disable=no-member def __len__ ( self ): return self . __dataclass_fields__ . __len__ # pylint: disable=no-member def to_json ( self , ** json_dump_kwargs ): return json . dumps ( asdict ( self ), ** json_dump_kwargs ) def to_dict ( self ): return self . __dict__","title":"EvaluationSpecification"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.create_evaluation","text":"Create a blackboxopt.Evaluation based on this evaluation specification. Parameters: Name Type Description Default objectives Dict[str, Optional[float]] For each objective name the respective value. required constraints Optional[Dict[str, Union[float, NoneType]]] For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. None user_info Optional[dict] Miscellaneous information provided by the user. None stacktrace Optional[str] The stacktrace in case an unhandled exception occurred inside the evaluation function. None finished_unixtime Optional[float] Timestamp at completion of this evaluation. If none is provided, the current time is used. None Source code in blackboxopt/evaluation.py def create_evaluation ( self , objectives : Dict [ str , Optional [ float ]], constraints : Optional [ Dict [ str , Optional [ float ]]] = None , user_info : Optional [ dict ] = None , stacktrace : Optional [ str ] = None , finished_unixtime : Optional [ float ] = None , ): \"\"\"Create a blackboxopt.Evaluation based on this evaluation specification. Args: objectives: For each objective name the respective value. constraints: For each constraint name the float value indicates how much the constraint was satisfied, with negative values implying a violated and positive values indicating a satisfied constraint. user_info: Miscellaneous information provided by the user. stacktrace: The stacktrace in case an unhandled exception occurred inside the evaluation function. finished_unixtime: Timestamp at completion of this evaluation. If none is provided, the current time is used. \"\"\" evaluation = Evaluation ( objectives = objectives , constraints = constraints , user_info = user_info , stacktrace = stacktrace , ** self , ) # Data class default factories like in this case time.time are only triggered # when the argument is not provided, so in case of it being None we can't just # pass the argument value in, because it would set it to None instead of # triggering the default factory for the current time. if finished_unixtime is not None : evaluation . finished_unixtime = finished_unixtime return evaluation","title":"create_evaluation()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.get","text":"D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in blackboxopt/evaluation.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default","title":"get()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.items","text":"D.items() -> a set-like object providing a view on D's items Source code in blackboxopt/evaluation.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self )","title":"items()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.keys","text":"D.keys() -> a set-like object providing a view on D's keys Source code in blackboxopt/evaluation.py def keys ( self ): return self . __dataclass_fields__ . keys () # pylint: disable=no-member","title":"keys()"},{"location":"reference/evaluation/#blackboxopt.evaluation.EvaluationSpecification.values","text":"D.values() -> an object providing a view on D's values Source code in blackboxopt/evaluation.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self )","title":"values()"},{"location":"reference/io/","text":"blackboxopt.io load_study_from_json ( json_file_path ) Load space, objectives and evaluations from a given json_file_path . Source code in blackboxopt/io.py def load_study_from_json ( json_file_path : os . PathLike , ) -> Tuple [ ps . ParameterSpace , List [ Objective ], List [ Evaluation ]]: \"\"\"Load space, objectives and evaluations from a given `json_file_path`.\"\"\" with open ( json_file_path , \"r\" , encoding = \"UTF-8\" ) as fh : study = json . load ( fh ) search_space = ps . ParameterSpace . from_dict ( study [ \"search_space\" ]) objectives = [ Objective ( ** o ) for o in study [ \"objectives\" ]] evaluations = [ Evaluation ( ** e ) for e in study [ \"evaluations\" ]] return search_space , objectives , evaluations load_study_from_pickle ( pickle_file_path ) Load space, objectives and evaluations from a given lzma compressed pickle. Source code in blackboxopt/io.py def load_study_from_pickle ( pickle_file_path : os . PathLike , ) -> Tuple [ ps . ParameterSpace , List [ Objective ], List [ Evaluation ]]: \"\"\"Load space, objectives and evaluations from a given lzma compressed pickle.\"\"\" with lzma . open ( pickle_file_path , \"rb\" ) as fh : study = pickle . load ( fh ) return study [ \"search_space\" ], study [ \"objectives\" ], study [ \"evaluations\" ] save_study_as_json ( search_space , objectives , evaluations , json_file_path , overwrite = False ) Save space, objectives and evaluations as json at json_file_path . Source code in blackboxopt/io.py def save_study_as_json ( search_space : ps . ParameterSpace , objectives : List [ Objective ], evaluations : List [ Evaluation ], json_file_path : os . PathLike , overwrite : bool = False , ): \"\"\"Save space, objectives and evaluations as json at `json_file_path`.\"\"\" _file_path = Path ( json_file_path ) if not _file_path . parent . exists (): raise IOError ( f \"The parent directory for { _file_path } does not exist, please create it.\" ) if _file_path . exists () and not overwrite : raise IOError ( f \" { _file_path } exists and overwrite is False\" ) with open ( _file_path , \"w\" , encoding = \"UTF-8\" ) as fh : json . dump ( { \"search_space\" : search_space . to_dict (), \"objectives\" : [ o . __dict__ for o in objectives ], \"evaluations\" : [ e . __dict__ for e in evaluations ], }, fh , ) save_study_as_pickle ( search_space , objectives , evaluations , pickle_file_path , overwrite = False ) Save space, objectives and evaluations as an lzma compressed pickle. Source code in blackboxopt/io.py def save_study_as_pickle ( search_space : ps . ParameterSpace , objectives : List [ Objective ], evaluations : List [ Evaluation ], pickle_file_path : os . PathLike , overwrite : bool = False , ): \"\"\"Save space, objectives and evaluations as an lzma compressed pickle.\"\"\" _file_path = Path ( pickle_file_path ) if not _file_path . parent . exists (): raise IOError ( f \"The parent directory for { _file_path } does not exist, please create it.\" ) if _file_path . exists () and not overwrite : raise IOError ( f \" { _file_path } exists and overwrite is False\" ) with lzma . open ( _file_path , \"wb\" ) as fh : pickle . dump ( { \"search_space\" : search_space , \"objectives\" : objectives , \"evaluations\" : evaluations , }, fh , )","title":"Io"},{"location":"reference/io/#blackboxopt.io","text":"","title":"io"},{"location":"reference/io/#blackboxopt.io.load_study_from_json","text":"Load space, objectives and evaluations from a given json_file_path . Source code in blackboxopt/io.py def load_study_from_json ( json_file_path : os . PathLike , ) -> Tuple [ ps . ParameterSpace , List [ Objective ], List [ Evaluation ]]: \"\"\"Load space, objectives and evaluations from a given `json_file_path`.\"\"\" with open ( json_file_path , \"r\" , encoding = \"UTF-8\" ) as fh : study = json . load ( fh ) search_space = ps . ParameterSpace . from_dict ( study [ \"search_space\" ]) objectives = [ Objective ( ** o ) for o in study [ \"objectives\" ]] evaluations = [ Evaluation ( ** e ) for e in study [ \"evaluations\" ]] return search_space , objectives , evaluations","title":"load_study_from_json()"},{"location":"reference/io/#blackboxopt.io.load_study_from_pickle","text":"Load space, objectives and evaluations from a given lzma compressed pickle. Source code in blackboxopt/io.py def load_study_from_pickle ( pickle_file_path : os . PathLike , ) -> Tuple [ ps . ParameterSpace , List [ Objective ], List [ Evaluation ]]: \"\"\"Load space, objectives and evaluations from a given lzma compressed pickle.\"\"\" with lzma . open ( pickle_file_path , \"rb\" ) as fh : study = pickle . load ( fh ) return study [ \"search_space\" ], study [ \"objectives\" ], study [ \"evaluations\" ]","title":"load_study_from_pickle()"},{"location":"reference/io/#blackboxopt.io.save_study_as_json","text":"Save space, objectives and evaluations as json at json_file_path . Source code in blackboxopt/io.py def save_study_as_json ( search_space : ps . ParameterSpace , objectives : List [ Objective ], evaluations : List [ Evaluation ], json_file_path : os . PathLike , overwrite : bool = False , ): \"\"\"Save space, objectives and evaluations as json at `json_file_path`.\"\"\" _file_path = Path ( json_file_path ) if not _file_path . parent . exists (): raise IOError ( f \"The parent directory for { _file_path } does not exist, please create it.\" ) if _file_path . exists () and not overwrite : raise IOError ( f \" { _file_path } exists and overwrite is False\" ) with open ( _file_path , \"w\" , encoding = \"UTF-8\" ) as fh : json . dump ( { \"search_space\" : search_space . to_dict (), \"objectives\" : [ o . __dict__ for o in objectives ], \"evaluations\" : [ e . __dict__ for e in evaluations ], }, fh , )","title":"save_study_as_json()"},{"location":"reference/io/#blackboxopt.io.save_study_as_pickle","text":"Save space, objectives and evaluations as an lzma compressed pickle. Source code in blackboxopt/io.py def save_study_as_pickle ( search_space : ps . ParameterSpace , objectives : List [ Objective ], evaluations : List [ Evaluation ], pickle_file_path : os . PathLike , overwrite : bool = False , ): \"\"\"Save space, objectives and evaluations as an lzma compressed pickle.\"\"\" _file_path = Path ( pickle_file_path ) if not _file_path . parent . exists (): raise IOError ( f \"The parent directory for { _file_path } does not exist, please create it.\" ) if _file_path . exists () and not overwrite : raise IOError ( f \" { _file_path } exists and overwrite is False\" ) with lzma . open ( _file_path , \"wb\" ) as fh : pickle . dump ( { \"search_space\" : search_space , \"objectives\" : objectives , \"evaluations\" : evaluations , }, fh , )","title":"save_study_as_pickle()"},{"location":"reference/logger/","text":"blackboxopt.logger","title":"Logger"},{"location":"reference/logger/#blackboxopt.logger","text":"","title":"logger"},{"location":"reference/utils/","text":"blackboxopt.utils filter_pareto_efficient ( evaluations , objectives ) Filter pareto efficient evaluations with respect to given objectives. Source code in blackboxopt/utils.py def filter_pareto_efficient ( evaluations : List [ Evaluation ], objectives : List [ Objective ] ) -> List [ Evaluation ]: \"\"\"Filter pareto efficient evaluations with respect to given objectives.\"\"\" losses = np . array ( [ get_loss_vector ( known_objectives = objectives , reported_objectives = e . objectives ) for e in evaluations ] ) pareto_efficient_mask = mask_pareto_efficient ( losses ) return list ( compress ( evaluations , pareto_efficient_mask )) get_loss_vector ( known_objectives , reported_objectives , none_replacement = nan ) Convert reported objectives into a vector of known objectives. Parameters: Name Type Description Default known_objectives Sequence[blackboxopt.base.Objective] A sequence of objectives with names and directions (whether greate is better). The order of the objectives dictates the order of the returned loss values. required reported_objectives Dict[str, Optional[float]] A dictionary with the objective value for each of the known objectives' names. required none_replacement float The value to use for missing objective values that are None nan Returns: Type Description List[float] A list of loss values. Source code in blackboxopt/utils.py def get_loss_vector ( known_objectives : Sequence [ Objective ], reported_objectives : Dict [ str , Optional [ float ]], none_replacement : float = float ( \"NaN\" ), ) -> List [ float ]: \"\"\"Convert reported objectives into a vector of known objectives. Args: known_objectives: A sequence of objectives with names and directions (whether greate is better). The order of the objectives dictates the order of the returned loss values. reported_objectives: A dictionary with the objective value for each of the known objectives' names. none_replacement: The value to use for missing objective values that are `None` Returns: A list of loss values. \"\"\" losses = [] for objective in known_objectives : objective_value = reported_objectives [ objective . name ] if objective_value is None : losses . append ( none_replacement ) elif objective . greater_is_better : losses . append ( - 1.0 * objective_value ) else : losses . append ( objective_value ) return losses init_logger ( level = None ) Initialize the default blackboxopt.logger as a nicely formatted stdout logger. Should no log level be given, the environment variable BBO_LOG_LEVEL is used and if that is not present, the default is logging.DEBUG . Parameters: Name Type Description Default level int the log level to set None Returns: Type Description Logger The logger instance (equivalent to blackboxopt.logger ) Source code in blackboxopt/utils.py def init_logger ( level : int = None ) -> logging . Logger : # pragma: no cover \"\"\"Initialize the default `blackboxopt.logger` as a nicely formatted stdout logger. Should no log level be given, the environment variable `BBO_LOG_LEVEL` is used and if that is not present, the default is `logging.DEBUG`. Args: level: the log level to set Returns: The logger instance (equivalent to `blackboxopt.logger`) \"\"\" if level is None : level_name = os . environ . get ( \"BBO_LOG_LEVEL\" , \"DEBUG\" ) level = getattr ( logging , level_name ) logger . setLevel ( level ) handler = logging . StreamHandler ( sys . stdout ) handler . setLevel ( level ) formatter = logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger mask_pareto_efficient ( losses ) For a given array of objective values where lower values are considered better and the dimensions are samples x objectives, return a mask that is True for all pareto efficient values. NOTE: The result marks multiple occurrences of the same point all as pareto efficient. Source code in blackboxopt/utils.py def mask_pareto_efficient ( losses : np . ndarray ): \"\"\"For a given array of objective values where lower values are considered better and the dimensions are samples x objectives, return a mask that is `True` for all pareto efficient values. NOTE: The result marks multiple occurrences of the same point all as pareto efficient. \"\"\" is_efficient = np . ones ( losses . shape [ 0 ], dtype = bool ) for i , c in enumerate ( losses ): if not is_efficient [ i ]: continue # Keep any point with a lower cost or when they are the same efficient = np . any ( losses [ is_efficient ] < c , axis = 1 ) duplicates = np . all ( losses [ is_efficient ] == c , axis = 1 ) is_efficient [ is_efficient ] = np . logical_or ( efficient , duplicates ) return is_efficient","title":"Utils"},{"location":"reference/utils/#blackboxopt.utils","text":"","title":"utils"},{"location":"reference/utils/#blackboxopt.utils.filter_pareto_efficient","text":"Filter pareto efficient evaluations with respect to given objectives. Source code in blackboxopt/utils.py def filter_pareto_efficient ( evaluations : List [ Evaluation ], objectives : List [ Objective ] ) -> List [ Evaluation ]: \"\"\"Filter pareto efficient evaluations with respect to given objectives.\"\"\" losses = np . array ( [ get_loss_vector ( known_objectives = objectives , reported_objectives = e . objectives ) for e in evaluations ] ) pareto_efficient_mask = mask_pareto_efficient ( losses ) return list ( compress ( evaluations , pareto_efficient_mask ))","title":"filter_pareto_efficient()"},{"location":"reference/utils/#blackboxopt.utils.get_loss_vector","text":"Convert reported objectives into a vector of known objectives. Parameters: Name Type Description Default known_objectives Sequence[blackboxopt.base.Objective] A sequence of objectives with names and directions (whether greate is better). The order of the objectives dictates the order of the returned loss values. required reported_objectives Dict[str, Optional[float]] A dictionary with the objective value for each of the known objectives' names. required none_replacement float The value to use for missing objective values that are None nan Returns: Type Description List[float] A list of loss values. Source code in blackboxopt/utils.py def get_loss_vector ( known_objectives : Sequence [ Objective ], reported_objectives : Dict [ str , Optional [ float ]], none_replacement : float = float ( \"NaN\" ), ) -> List [ float ]: \"\"\"Convert reported objectives into a vector of known objectives. Args: known_objectives: A sequence of objectives with names and directions (whether greate is better). The order of the objectives dictates the order of the returned loss values. reported_objectives: A dictionary with the objective value for each of the known objectives' names. none_replacement: The value to use for missing objective values that are `None` Returns: A list of loss values. \"\"\" losses = [] for objective in known_objectives : objective_value = reported_objectives [ objective . name ] if objective_value is None : losses . append ( none_replacement ) elif objective . greater_is_better : losses . append ( - 1.0 * objective_value ) else : losses . append ( objective_value ) return losses","title":"get_loss_vector()"},{"location":"reference/utils/#blackboxopt.utils.init_logger","text":"Initialize the default blackboxopt.logger as a nicely formatted stdout logger. Should no log level be given, the environment variable BBO_LOG_LEVEL is used and if that is not present, the default is logging.DEBUG . Parameters: Name Type Description Default level int the log level to set None Returns: Type Description Logger The logger instance (equivalent to blackboxopt.logger ) Source code in blackboxopt/utils.py def init_logger ( level : int = None ) -> logging . Logger : # pragma: no cover \"\"\"Initialize the default `blackboxopt.logger` as a nicely formatted stdout logger. Should no log level be given, the environment variable `BBO_LOG_LEVEL` is used and if that is not present, the default is `logging.DEBUG`. Args: level: the log level to set Returns: The logger instance (equivalent to `blackboxopt.logger`) \"\"\" if level is None : level_name = os . environ . get ( \"BBO_LOG_LEVEL\" , \"DEBUG\" ) level = getattr ( logging , level_name ) logger . setLevel ( level ) handler = logging . StreamHandler ( sys . stdout ) handler . setLevel ( level ) formatter = logging . Formatter ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger","title":"init_logger()"},{"location":"reference/utils/#blackboxopt.utils.mask_pareto_efficient","text":"For a given array of objective values where lower values are considered better and the dimensions are samples x objectives, return a mask that is True for all pareto efficient values. NOTE: The result marks multiple occurrences of the same point all as pareto efficient. Source code in blackboxopt/utils.py def mask_pareto_efficient ( losses : np . ndarray ): \"\"\"For a given array of objective values where lower values are considered better and the dimensions are samples x objectives, return a mask that is `True` for all pareto efficient values. NOTE: The result marks multiple occurrences of the same point all as pareto efficient. \"\"\" is_efficient = np . ones ( losses . shape [ 0 ], dtype = bool ) for i , c in enumerate ( losses ): if not is_efficient [ i ]: continue # Keep any point with a lower cost or when they are the same efficient = np . any ( losses [ is_efficient ] < c , axis = 1 ) duplicates = np . all ( losses [ is_efficient ] == c , axis = 1 ) is_efficient [ is_efficient ] = np . logical_or ( efficient , duplicates ) return is_efficient","title":"mask_pareto_efficient()"},{"location":"reference/optimization_loops/dask_distributed/","text":"blackboxopt.optimization_loops.dask_distributed run_optimization_loop ( optimizer , evaluation_function , dask_client , timeout_s = inf , max_evaluations = None , logger = None ) Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Parameters: Name Type Description Default optimizer Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer] The blackboxopt optimizer to run. required dask_client Client A Dask Distributed client that is configured with workers. required evaluation_function Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation] The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a blackboxopt.Evaluation as a result. required timeout_s float If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. inf max_evaluations int If given, the optimization loop will terminate after the given number of steps. Defaults to None. None logger Logger The logger to use for logging progress. Defaults to None. None Returns: Type Description List[blackboxopt.evaluation.Evaluation] List of evluation specification and result for all evaluations. Source code in blackboxopt/optimization_loops/dask_distributed.py def run_optimization_loop ( optimizer : Union [ SingleObjectiveOptimizer , MultiObjectiveOptimizer ], evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], dask_client : dd . Client , timeout_s : float = float ( \"inf\" ), max_evaluations : int = None , logger : logging . Logger = None , ) -> List [ Evaluation ]: \"\"\"Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Args: optimizer: The blackboxopt optimizer to run. dask_client: A Dask Distributed client that is configured with workers. evaluation_function: The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a `blackboxopt.Evaluation` as a result. timeout_s: If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. max_evaluations: If given, the optimization loop will terminate after the given number of steps. Defaults to None. logger: The logger to use for logging progress. Defaults to None. Returns: List of evluation specification and result for all evaluations. \"\"\" logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger objectives = ( optimizer . objectives if isinstance ( optimizer , MultiObjectiveOptimizer ) else [ optimizer . objective ] ) evaluations : List [ Evaluation ] = [] dask_scheduler = MinimalDaskScheduler ( dask_client = dask_client , objectives = objectives , logger = logger ) _max_evaluations = init_max_evaluations_with_limit_logging ( max_evaluations = max_evaluations , timeout_s = timeout_s , logger = logger ) n_eval_specs = 0 start = time . time () while time . time () - start < timeout_s and n_eval_specs < _max_evaluations : if dask_scheduler . has_capacity (): try : eval_spec = optimizer . generate_evaluation_specification () dask_scheduler . submit ( evaluation_function , eval_spec ) n_eval_specs += 1 continue except OptimizerNotReady : logger . info ( \"Optimizer is not ready yet; will retry after short pause.\" ) except OptimizationComplete : logger . info ( \"Optimization is complete\" ) break new_evaluations = dask_scheduler . check_for_results ( timeout_s = 20 ) optimizer . report ( new_evaluations ) evaluations . extend ( new_evaluations ) while dask_scheduler . has_running_jobs (): new_evaluations = dask_scheduler . check_for_results ( timeout_s = 20 ) optimizer . report ( new_evaluations ) evaluations . extend ( new_evaluations ) return evaluations","title":"Dask distributed"},{"location":"reference/optimization_loops/dask_distributed/#blackboxopt.optimization_loops.dask_distributed","text":"","title":"dask_distributed"},{"location":"reference/optimization_loops/dask_distributed/#blackboxopt.optimization_loops.dask_distributed.run_optimization_loop","text":"Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Parameters: Name Type Description Default optimizer Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer] The blackboxopt optimizer to run. required dask_client Client A Dask Distributed client that is configured with workers. required evaluation_function Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation] The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a blackboxopt.Evaluation as a result. required timeout_s float If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. inf max_evaluations int If given, the optimization loop will terminate after the given number of steps. Defaults to None. None logger Logger The logger to use for logging progress. Defaults to None. None Returns: Type Description List[blackboxopt.evaluation.Evaluation] List of evluation specification and result for all evaluations. Source code in blackboxopt/optimization_loops/dask_distributed.py def run_optimization_loop ( optimizer : Union [ SingleObjectiveOptimizer , MultiObjectiveOptimizer ], evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], dask_client : dd . Client , timeout_s : float = float ( \"inf\" ), max_evaluations : int = None , logger : logging . Logger = None , ) -> List [ Evaluation ]: \"\"\"Convenience wrapper for an optimization loop that uses Dask to parallelize optimization until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Args: optimizer: The blackboxopt optimizer to run. dask_client: A Dask Distributed client that is configured with workers. evaluation_function: The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a `blackboxopt.Evaluation` as a result. timeout_s: If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. max_evaluations: If given, the optimization loop will terminate after the given number of steps. Defaults to None. logger: The logger to use for logging progress. Defaults to None. Returns: List of evluation specification and result for all evaluations. \"\"\" logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger objectives = ( optimizer . objectives if isinstance ( optimizer , MultiObjectiveOptimizer ) else [ optimizer . objective ] ) evaluations : List [ Evaluation ] = [] dask_scheduler = MinimalDaskScheduler ( dask_client = dask_client , objectives = objectives , logger = logger ) _max_evaluations = init_max_evaluations_with_limit_logging ( max_evaluations = max_evaluations , timeout_s = timeout_s , logger = logger ) n_eval_specs = 0 start = time . time () while time . time () - start < timeout_s and n_eval_specs < _max_evaluations : if dask_scheduler . has_capacity (): try : eval_spec = optimizer . generate_evaluation_specification () dask_scheduler . submit ( evaluation_function , eval_spec ) n_eval_specs += 1 continue except OptimizerNotReady : logger . info ( \"Optimizer is not ready yet; will retry after short pause.\" ) except OptimizationComplete : logger . info ( \"Optimization is complete\" ) break new_evaluations = dask_scheduler . check_for_results ( timeout_s = 20 ) optimizer . report ( new_evaluations ) evaluations . extend ( new_evaluations ) while dask_scheduler . has_running_jobs (): new_evaluations = dask_scheduler . check_for_results ( timeout_s = 20 ) optimizer . report ( new_evaluations ) evaluations . extend ( new_evaluations ) return evaluations","title":"run_optimization_loop()"},{"location":"reference/optimization_loops/sequential/","text":"blackboxopt.optimization_loops.sequential run_optimization_loop ( optimizer , evaluation_function , timeout_s = inf , max_evaluations = None , catch_exceptions_from_evaluation_function = False , pre_evaluation_callback = None , post_evaluation_callback = None , logger = None ) Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Parameters: Name Type Description Default optimizer Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer] The blackboxopt optimizer to run. required evaluation_function Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation] The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a blackboxopt.Evaluation as a result. required timeout_s float If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. inf max_evaluations Optional[int] If given, the optimization loop will terminate after the given number of steps. None catch_exceptions_from_evaluation_function bool Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's stacktrace attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. For more details, see the wrapper that is used internally blackboxopt.optimization_loops.utils.evaluation_function_wrapper False pre_evaluation_callback Optional[Callable[[blackboxopt.evaluation.EvaluationSpecification], Any]] Reference to a callable that is invoked before each evaluation and takes a blackboxopt.EvaluationSpecification as an argument. None post_evaluation_callback Optional[Callable[[blackboxopt.evaluation.Evaluation], Any]] Reference to a callable that is invoked after each evaluation and takes a blackboxopt.Evaluation as an argument. None logger Optional[logging.Logger] The logger to use for logging progress. Default: blackboxopt.logger None Returns: Type Description List[blackboxopt.evaluation.Evaluation] List of evaluation specification and result for all evaluations. Source code in blackboxopt/optimization_loops/sequential.py def run_optimization_loop ( optimizer : Union [ SingleObjectiveOptimizer , MultiObjectiveOptimizer ], evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], timeout_s : float = float ( \"inf\" ), max_evaluations : Optional [ int ] = None , catch_exceptions_from_evaluation_function : bool = False , pre_evaluation_callback : Optional [ Callable [[ EvaluationSpecification ], Any ]] = None , post_evaluation_callback : Optional [ Callable [[ Evaluation ], Any ]] = None , logger : Optional [ logging . Logger ] = None , ) -> List [ Evaluation ]: \"\"\"Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Args: optimizer: The blackboxopt optimizer to run. evaluation_function: The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a `blackboxopt.Evaluation` as a result. timeout_s: If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. max_evaluations: If given, the optimization loop will terminate after the given number of steps. catch_exceptions_from_evaluation_function: Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's `stacktrace` attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. For more details, see the wrapper that is used internally `blackboxopt.optimization_loops.utils.evaluation_function_wrapper` pre_evaluation_callback: Reference to a callable that is invoked before each evaluation and takes a `blackboxopt.EvaluationSpecification` as an argument. post_evaluation_callback: Reference to a callable that is invoked after each evaluation and takes a `blackboxopt.Evaluation` as an argument. logger: The logger to use for logging progress. Default: `blackboxopt.logger` Returns: List of evaluation specification and result for all evaluations. \"\"\" if logger is None : logger = default_logger objectives = ( optimizer . objectives if isinstance ( optimizer , MultiObjectiveOptimizer ) else [ optimizer . objective ] ) evaluations : List [ Evaluation ] = [] _max_evaluations = init_max_evaluations_with_limit_logging ( max_evaluations = max_evaluations , timeout_s = timeout_s , logger = logger ) start = time . time () num_evaluations = 0 while time . time () - start < timeout_s and num_evaluations < _max_evaluations : num_evaluations += 1 try : evaluation_specification = optimizer . generate_evaluation_specification () logger . info ( \"The optimizer proposed the following evaluation specification: \\n %s \" , pprint . pformat ( evaluation_specification . to_dict (), compact = True ), ) if pre_evaluation_callback : pre_evaluation_callback ( evaluation_specification ) evaluation = evaluation_function_wrapper ( evaluation_function = evaluation_function , evaluation_specification = evaluation_specification , logger = logger , objectives = objectives , catch_exceptions_from_evaluation_function = catch_exceptions_from_evaluation_function , ) logger . info ( \"Reporting the following evaluation result to the optimizer: \\n %s \" , pprint . pformat ( evaluation . to_dict (), compact = True ), ) if post_evaluation_callback : post_evaluation_callback ( evaluation ) optimizer . report ( evaluation ) evaluations . append ( evaluation ) except OptimizerNotReady : logger . info ( \"Optimizer is not ready yet, retrying in two seconds\" ) time . sleep ( 2 ) continue except OptimizationComplete : logger . info ( \"Optimization is complete\" ) return evaluations logger . info ( \"Aborting optimization due to specified maximum evaluations or timeout\" ) return evaluations","title":"Sequential"},{"location":"reference/optimization_loops/sequential/#blackboxopt.optimization_loops.sequential","text":"","title":"sequential"},{"location":"reference/optimization_loops/sequential/#blackboxopt.optimization_loops.sequential.run_optimization_loop","text":"Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Parameters: Name Type Description Default optimizer Union[blackboxopt.base.SingleObjectiveOptimizer, blackboxopt.base.MultiObjectiveOptimizer] The blackboxopt optimizer to run. required evaluation_function Callable[[blackboxopt.evaluation.EvaluationSpecification], blackboxopt.evaluation.Evaluation] The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a blackboxopt.Evaluation as a result. required timeout_s float If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. inf max_evaluations Optional[int] If given, the optimization loop will terminate after the given number of steps. None catch_exceptions_from_evaluation_function bool Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's stacktrace attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. For more details, see the wrapper that is used internally blackboxopt.optimization_loops.utils.evaluation_function_wrapper False pre_evaluation_callback Optional[Callable[[blackboxopt.evaluation.EvaluationSpecification], Any]] Reference to a callable that is invoked before each evaluation and takes a blackboxopt.EvaluationSpecification as an argument. None post_evaluation_callback Optional[Callable[[blackboxopt.evaluation.Evaluation], Any]] Reference to a callable that is invoked after each evaluation and takes a blackboxopt.Evaluation as an argument. None logger Optional[logging.Logger] The logger to use for logging progress. Default: blackboxopt.logger None Returns: Type Description List[blackboxopt.evaluation.Evaluation] List of evaluation specification and result for all evaluations. Source code in blackboxopt/optimization_loops/sequential.py def run_optimization_loop ( optimizer : Union [ SingleObjectiveOptimizer , MultiObjectiveOptimizer ], evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], timeout_s : float = float ( \"inf\" ), max_evaluations : Optional [ int ] = None , catch_exceptions_from_evaluation_function : bool = False , pre_evaluation_callback : Optional [ Callable [[ EvaluationSpecification ], Any ]] = None , post_evaluation_callback : Optional [ Callable [[ Evaluation ], Any ]] = None , logger : Optional [ logging . Logger ] = None , ) -> List [ Evaluation ]: \"\"\"Convenience wrapper for an optimization loop that sequentially fetches evaluation specifications until a given timeout or maximum number of evaluations is reached. This already handles signals from the optimizer in case there is no evaluation specification available yet. Args: optimizer: The blackboxopt optimizer to run. evaluation_function: The function that is called with configuration, settings and optimizer info dictionaries as arguments like provided by an evaluation specification. This is the function that encapsulates the actual execution of a parametrized experiment (e.g. ML model training) and should return a `blackboxopt.Evaluation` as a result. timeout_s: If given, the optimization loop will terminate after the first optimization step that exceeded the timeout (in seconds). Defaults to inf. max_evaluations: If given, the optimization loop will terminate after the given number of steps. catch_exceptions_from_evaluation_function: Whether to exit on an unhandled exception raised by the evaluation function or instead store their stack trace in the evaluation's `stacktrace` attribute. Set to True if there are spurious errors due to e.g. numerical instability that should not halt the optimization loop. For more details, see the wrapper that is used internally `blackboxopt.optimization_loops.utils.evaluation_function_wrapper` pre_evaluation_callback: Reference to a callable that is invoked before each evaluation and takes a `blackboxopt.EvaluationSpecification` as an argument. post_evaluation_callback: Reference to a callable that is invoked after each evaluation and takes a `blackboxopt.Evaluation` as an argument. logger: The logger to use for logging progress. Default: `blackboxopt.logger` Returns: List of evaluation specification and result for all evaluations. \"\"\" if logger is None : logger = default_logger objectives = ( optimizer . objectives if isinstance ( optimizer , MultiObjectiveOptimizer ) else [ optimizer . objective ] ) evaluations : List [ Evaluation ] = [] _max_evaluations = init_max_evaluations_with_limit_logging ( max_evaluations = max_evaluations , timeout_s = timeout_s , logger = logger ) start = time . time () num_evaluations = 0 while time . time () - start < timeout_s and num_evaluations < _max_evaluations : num_evaluations += 1 try : evaluation_specification = optimizer . generate_evaluation_specification () logger . info ( \"The optimizer proposed the following evaluation specification: \\n %s \" , pprint . pformat ( evaluation_specification . to_dict (), compact = True ), ) if pre_evaluation_callback : pre_evaluation_callback ( evaluation_specification ) evaluation = evaluation_function_wrapper ( evaluation_function = evaluation_function , evaluation_specification = evaluation_specification , logger = logger , objectives = objectives , catch_exceptions_from_evaluation_function = catch_exceptions_from_evaluation_function , ) logger . info ( \"Reporting the following evaluation result to the optimizer: \\n %s \" , pprint . pformat ( evaluation . to_dict (), compact = True ), ) if post_evaluation_callback : post_evaluation_callback ( evaluation ) optimizer . report ( evaluation ) evaluations . append ( evaluation ) except OptimizerNotReady : logger . info ( \"Optimizer is not ready yet, retrying in two seconds\" ) time . sleep ( 2 ) continue except OptimizationComplete : logger . info ( \"Optimization is complete\" ) return evaluations logger . info ( \"Aborting optimization due to specified maximum evaluations or timeout\" ) return evaluations","title":"run_optimization_loop()"},{"location":"reference/optimization_loops/testing/","text":"blackboxopt.optimization_loops.testing","title":"Testing"},{"location":"reference/optimization_loops/testing/#blackboxopt.optimization_loops.testing","text":"","title":"testing"},{"location":"reference/optimization_loops/utils/","text":"blackboxopt.optimization_loops.utils EvaluationFunctionError ( ValueError ) Raised on errors originating from the user defined evaluation function. Source code in blackboxopt/optimization_loops/utils.py class EvaluationFunctionError ( ValueError ): \"\"\"Raised on errors originating from the user defined evaluation function.\"\"\" def __init__ ( self , evaluation_specification : EvaluationSpecification ): self . message = ( \"An error occurred when attempting to call the user specified evaluation \" \"function with the specification below. Please check the cause of this \" \"exception in the output further up for the original stacktrace. \\n \" f \" { evaluation_specification } \" ) self . evaluation_specification = evaluation_specification evaluation_function_wrapper ( evaluation_function , evaluation_specification , objectives , catch_exceptions_from_evaluation_function , logger ) Wrapper for evaluation functions. The evaluation result returned by the evaluation function is checked to contain all relevant objectives. An empty evaluation with a stacktrace is reported to the optimizer in case an unhandled Exception occurrs during the evaluation function call when catch_exceptions_from_evaluation_function is set to True , otherwise an EvaluationFunctionError is raised based on the original exception. Source code in blackboxopt/optimization_loops/utils.py def evaluation_function_wrapper ( evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], evaluation_specification : EvaluationSpecification , objectives : List [ Objective ], catch_exceptions_from_evaluation_function : bool , logger : logging . Logger , ) -> Evaluation : \"\"\"Wrapper for evaluation functions. The evaluation result returned by the evaluation function is checked to contain all relevant objectives. An empty evaluation with a stacktrace is reported to the optimizer in case an unhandled Exception occurrs during the evaluation function call when `catch_exceptions_from_evaluation_function` is set to `True`, otherwise an `EvaluationFunctionError` is raised based on the original exception. \"\"\" try : evaluation = evaluation_function ( evaluation_specification ) except Exception as e : if not catch_exceptions_from_evaluation_function : raise EvaluationFunctionError ( evaluation_specification ) from e stacktrace = traceback . format_exc () logger . warning ( \"Reporting FAILURE due to unhandled error in evaluation function. See \" + \"DEBUG log level output or evaluation.stacktrace for details. \" + \"Alternatively, disable automated exception handling by setting \" + \"catch_exceptions_from_evaluation_function=False to exit on errors.\" ) logger . debug ( stacktrace ) evaluation = evaluation_specification . create_evaluation ( stacktrace = stacktrace , objectives = { o . name : None for o in objectives } ) raise_on_unknown_or_incomplete ( exception = ObjectivesError , known = [ o . name for o in objectives ], reported = evaluation . objectives . keys (), ) return evaluation init_max_evaluations_with_limit_logging ( timeout_s , logger , max_evaluations = None ) [summary] Parameters: Name Type Description Default timeout_s float [description] required logger Logger [description] required max_evaluations int [description] None Returns: Type Description float [description] Source code in blackboxopt/optimization_loops/utils.py def init_max_evaluations_with_limit_logging ( timeout_s : float , logger : logging . Logger , max_evaluations : int = None ) -> float : \"\"\"[summary] Args: timeout_s: [description] logger: [description] max_evaluations: [description] Returns: [description] \"\"\" if max_evaluations : logger . info ( \"Starting optimization run. Stops when complete or \" + f \" { max_evaluations } evaluations reached.\" ) return float ( max_evaluations ) if timeout_s == float ( \"inf\" ): logger . info ( \"Starting optimization run. Stops when complete.\" ) else : timeout_pretty = datetime . timedelta ( seconds = timeout_s ) logger . info ( \"Starting optimization run. Stops when complete or \" + f \" { timeout_pretty } passed.\" ) return float ( \"inf\" )","title":"Utils"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils","text":"","title":"utils"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils.EvaluationFunctionError","text":"Raised on errors originating from the user defined evaluation function. Source code in blackboxopt/optimization_loops/utils.py class EvaluationFunctionError ( ValueError ): \"\"\"Raised on errors originating from the user defined evaluation function.\"\"\" def __init__ ( self , evaluation_specification : EvaluationSpecification ): self . message = ( \"An error occurred when attempting to call the user specified evaluation \" \"function with the specification below. Please check the cause of this \" \"exception in the output further up for the original stacktrace. \\n \" f \" { evaluation_specification } \" ) self . evaluation_specification = evaluation_specification","title":"EvaluationFunctionError"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils.evaluation_function_wrapper","text":"Wrapper for evaluation functions. The evaluation result returned by the evaluation function is checked to contain all relevant objectives. An empty evaluation with a stacktrace is reported to the optimizer in case an unhandled Exception occurrs during the evaluation function call when catch_exceptions_from_evaluation_function is set to True , otherwise an EvaluationFunctionError is raised based on the original exception. Source code in blackboxopt/optimization_loops/utils.py def evaluation_function_wrapper ( evaluation_function : Callable [[ EvaluationSpecification ], Evaluation ], evaluation_specification : EvaluationSpecification , objectives : List [ Objective ], catch_exceptions_from_evaluation_function : bool , logger : logging . Logger , ) -> Evaluation : \"\"\"Wrapper for evaluation functions. The evaluation result returned by the evaluation function is checked to contain all relevant objectives. An empty evaluation with a stacktrace is reported to the optimizer in case an unhandled Exception occurrs during the evaluation function call when `catch_exceptions_from_evaluation_function` is set to `True`, otherwise an `EvaluationFunctionError` is raised based on the original exception. \"\"\" try : evaluation = evaluation_function ( evaluation_specification ) except Exception as e : if not catch_exceptions_from_evaluation_function : raise EvaluationFunctionError ( evaluation_specification ) from e stacktrace = traceback . format_exc () logger . warning ( \"Reporting FAILURE due to unhandled error in evaluation function. See \" + \"DEBUG log level output or evaluation.stacktrace for details. \" + \"Alternatively, disable automated exception handling by setting \" + \"catch_exceptions_from_evaluation_function=False to exit on errors.\" ) logger . debug ( stacktrace ) evaluation = evaluation_specification . create_evaluation ( stacktrace = stacktrace , objectives = { o . name : None for o in objectives } ) raise_on_unknown_or_incomplete ( exception = ObjectivesError , known = [ o . name for o in objectives ], reported = evaluation . objectives . keys (), ) return evaluation","title":"evaluation_function_wrapper()"},{"location":"reference/optimization_loops/utils/#blackboxopt.optimization_loops.utils.init_max_evaluations_with_limit_logging","text":"[summary] Parameters: Name Type Description Default timeout_s float [description] required logger Logger [description] required max_evaluations int [description] None Returns: Type Description float [description] Source code in blackboxopt/optimization_loops/utils.py def init_max_evaluations_with_limit_logging ( timeout_s : float , logger : logging . Logger , max_evaluations : int = None ) -> float : \"\"\"[summary] Args: timeout_s: [description] logger: [description] max_evaluations: [description] Returns: [description] \"\"\" if max_evaluations : logger . info ( \"Starting optimization run. Stops when complete or \" + f \" { max_evaluations } evaluations reached.\" ) return float ( max_evaluations ) if timeout_s == float ( \"inf\" ): logger . info ( \"Starting optimization run. Stops when complete.\" ) else : timeout_pretty = datetime . timedelta ( seconds = timeout_s ) logger . info ( \"Starting optimization run. Stops when complete or \" + f \" { timeout_pretty } passed.\" ) return float ( \"inf\" )","title":"init_max_evaluations_with_limit_logging()"},{"location":"reference/optimizers/bohb/","text":"blackboxopt.optimizers.bohb BOHB ( StagedIterationOptimizer ) Source code in blackboxopt/optimizers/bohb.py class BOHB ( StagedIterationOptimizer ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , min_fidelity : float , max_fidelity : float , num_iterations : int , eta : float = 3.0 , top_n_percent : int = 15 , min_samples_in_model : int = None , num_samples : int = 64 , random_fraction : float = 1 / 3 , bandwidth_factor : float = 3.0 , min_bandwidth : float = 1e-3 , seed : int = None , logger : logging . Logger = None , ): \"\"\"BOHB Optimizer. BOHB performs robust and efficient hyperparameter optimization at scale by combining the speed of Hyperband searches with the guidance and guarantees of convergence of Bayesian Optimization. Instead of sampling new configurations at random, BOHB uses kernel density estimators to select promising candidates. For reference: ``` @InProceedings{falkner-icml-18, title = {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale}, author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {1436--1445}, year = {2018}, } ``` Args: search_space: [description] objective: [description] min_fidelity: The smallest fidelity value that is still meaningful. Must be strictly greater than zero! max_fidelity: The largest fidelity value used during the optimization. Must not be smaller than `min_fidelity`. num_iterations: The number of iterations that the optimizer will run. eta: Scaling parameter to control the aggressiveness of Hyperband's racing. top_n_percent: Determines the percentile of configurations that will be used as training data for the kernel density estimator of the good configuration, e.g if set to 10 the best 10% configurations will be considered for training. min_samples_in_model: Minimum number of datapoints needed to fit a model. num_samples: Number of samples drawn to optimize EI via sampling. random_fraction: Fraction of random configurations returned. bandwidth_factor: Widens the bandwidth for contiuous parameters for proposed points to optimize EI min_bandwidth: to keep diversity, even when all (good) samples have the same value for one of the parameters, a minimum bandwidth (reasonable default: 1e-3) is used instead of zero. seed: [description] logger: [description] \"\"\" if min_samples_in_model is None : min_samples_in_model = 3 * len ( search_space ) self . min_fidelity = min_fidelity self . max_fidelity = max_fidelity self . eta = eta self . config_sampler = BOHBSampler ( search_space = search_space , objective = objective , min_samples_in_model = min_samples_in_model , top_n_percent = top_n_percent , num_samples = num_samples , random_fraction = random_fraction , bandwidth_factor = bandwidth_factor , min_bandwidth = min_bandwidth , seed = seed , ) super () . __init__ ( search_space = search_space , objective = objective , num_iterations = num_iterations , seed = seed , logger = logger , ) def _create_new_iteration ( self , iteration_index ): \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.utils.staged_iteration.StagedIteration` object \"\"\" return create_hyperband_iteration ( iteration_index , self . min_fidelity , self . max_fidelity , self . eta , self . config_sampler , self . objective , self . logger , ) generate_evaluation_specification ( self ) inherited Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/bohb.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady report ( self , evaluations ) inherited Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/bohb.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"Bohb"},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb","text":"","title":"bohb"},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB","text":"Source code in blackboxopt/optimizers/bohb.py class BOHB ( StagedIterationOptimizer ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , min_fidelity : float , max_fidelity : float , num_iterations : int , eta : float = 3.0 , top_n_percent : int = 15 , min_samples_in_model : int = None , num_samples : int = 64 , random_fraction : float = 1 / 3 , bandwidth_factor : float = 3.0 , min_bandwidth : float = 1e-3 , seed : int = None , logger : logging . Logger = None , ): \"\"\"BOHB Optimizer. BOHB performs robust and efficient hyperparameter optimization at scale by combining the speed of Hyperband searches with the guidance and guarantees of convergence of Bayesian Optimization. Instead of sampling new configurations at random, BOHB uses kernel density estimators to select promising candidates. For reference: ``` @InProceedings{falkner-icml-18, title = {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale}, author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {1436--1445}, year = {2018}, } ``` Args: search_space: [description] objective: [description] min_fidelity: The smallest fidelity value that is still meaningful. Must be strictly greater than zero! max_fidelity: The largest fidelity value used during the optimization. Must not be smaller than `min_fidelity`. num_iterations: The number of iterations that the optimizer will run. eta: Scaling parameter to control the aggressiveness of Hyperband's racing. top_n_percent: Determines the percentile of configurations that will be used as training data for the kernel density estimator of the good configuration, e.g if set to 10 the best 10% configurations will be considered for training. min_samples_in_model: Minimum number of datapoints needed to fit a model. num_samples: Number of samples drawn to optimize EI via sampling. random_fraction: Fraction of random configurations returned. bandwidth_factor: Widens the bandwidth for contiuous parameters for proposed points to optimize EI min_bandwidth: to keep diversity, even when all (good) samples have the same value for one of the parameters, a minimum bandwidth (reasonable default: 1e-3) is used instead of zero. seed: [description] logger: [description] \"\"\" if min_samples_in_model is None : min_samples_in_model = 3 * len ( search_space ) self . min_fidelity = min_fidelity self . max_fidelity = max_fidelity self . eta = eta self . config_sampler = BOHBSampler ( search_space = search_space , objective = objective , min_samples_in_model = min_samples_in_model , top_n_percent = top_n_percent , num_samples = num_samples , random_fraction = random_fraction , bandwidth_factor = bandwidth_factor , min_bandwidth = min_bandwidth , seed = seed , ) super () . __init__ ( search_space = search_space , objective = objective , num_iterations = num_iterations , seed = seed , logger = logger , ) def _create_new_iteration ( self , iteration_index ): \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.utils.staged_iteration.StagedIteration` object \"\"\" return create_hyperband_iteration ( iteration_index , self . min_fidelity , self . max_fidelity , self . eta , self . config_sampler , self . objective , self . logger , )","title":"BOHB"},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/bohb.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/bohb/#blackboxopt.optimizers.bohb.BOHB.report","text":"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/bohb.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"report()"},{"location":"reference/optimizers/botorch_base/","text":"blackboxopt.optimizers.botorch_base SingleObjectiveBOTorchOptimizer ( SingleObjectiveOptimizer ) Source code in blackboxopt/optimizers/botorch_base.py class SingleObjectiveBOTorchOptimizer ( SingleObjectiveOptimizer ): def __init__ ( self , search_space : ps . ParameterSpace , objective : Objective , model : Model , acquisition_function_factory : Callable [[ Model ], AcquisitionFunction ], af_optimizer_kwargs = None , num_initial_random_samples : int = 1 , max_pending_evaluations : Optional [ int ] = 1 , batch_shape : torch . Size = torch . Size (), logger : Optional [ logging . Logger ] = None , seed : Optional [ int ] = None , torch_dtype : torch . dtype = torch . float64 , ): \"\"\"Single objective BO optimizer that uses as a surrogate model the `model` object provided by user. The `model` is expected to be extended from BoTorch base model `Model` class, and does not require to be a GP model. Args: search_space: The space in which to optimize. objective: The objective to optimize. model: Surrogate model of `Model` type. acquisition_function_factory: Callable that produces an acquisition function instance, could also be a compatible acquisition function class. Only acquisition functions to be minimized are supported. Providing a partially initialized class is possible with, e.g. `functools.partial(UpperConfidenceBound, beta=6.0, maximize=False)`. af_optimizer_kwargs: Settings for acquisition function optimizer, see `botorch.optim.optimize_acqf` and in case the whole search space is discrete: `botorch.optim.optimize_acqf_discrete`. The former can be enforced by providing `raw_samples` or `num_restarts`, the latter by providing `num_random_choices`. num_initial_random_samples: Size of the initial space-filling design that is used before starting BO. The points are sampled randomly in the search space. If no random sampling is required, set it to 0. max_pending_evaluations: Maximum number of parallel evaluations. For sequential BO use the default value of 1. If no limit is required, set it to None. batch_shape: Batch dimension(s) used for batched models. logger: Custom logger. seed: A seed to make the optimization reproducible. torch_dtype: Torch data type used for storing the data. This needs to match the dtype of the model used \"\"\" super () . __init__ ( search_space = search_space , objective = objective , seed = seed ) self . num_initial_random = num_initial_random_samples self . max_pending_evaluations = max_pending_evaluations self . batch_shape = batch_shape self . logger = logger or logging . getLogger ( \"blackboxopt\" ) self . torch_dtype = torch_dtype self . X = torch . empty ( ( * self . batch_shape , 0 , len ( search_space )), dtype = torch_dtype ) self . losses = torch . empty (( * self . batch_shape , 0 , 1 ), dtype = torch_dtype ) self . pending_specifications : Dict [ int , EvaluationSpecification ] = {} if seed is not None : torch . manual_seed ( seed = seed ) self . model = model self . acquisition_function_factory = acquisition_function_factory self . af_optimizer_kwargs = af_optimizer_kwargs def _create_fantasy_model ( self , model : Model ) -> Model : \"\"\"Create model with the pending specifications and model based outcomes added to the training data.\"\"\" if not self . pending_specifications : # nothing to do when there are no pending specs return model pending_X = torch . tensor ( np . array ( [ self . search_space . to_numerical ( e . configuration ) for e in self . pending_specifications . values () ] ), dtype = self . torch_dtype , ) model = model . fantasize ( pending_X , IIDNormalSampler ( 1 ), observation_noise = False ) if isinstance ( model , ExactGP ): # ExactGP.fantasize extends model's X and Y with batch_size, even if # originally not given -> need to reshape these to their original # representation n_samples = model . train_targets . size ( - 1 ) n_features = len ( self . search_space ) model . train_inputs [ 0 ] = model . train_inputs [ 0 ] . reshape ( torch . Size (( * self . batch_shape , n_samples , n_features )) ) model . train_targets = model . train_targets . reshape ( torch . Size (( * self . batch_shape , n_samples , 1 )) ) return model def _generate_evaluation_specification ( self ): \"\"\"Optimize acquisition on fantasy model to pick next point.\"\"\" fantasy_model = self . _create_fantasy_model ( self . model ) fantasy_model . eval () af = self . acquisition_function_factory ( fantasy_model ) if getattr ( af , \"maximize\" , False ): raise ValueError ( \"Only acquisition functions that need to be minimized are supported. \" f \"The given { af . __class__ . __name__ } has maximize=True. \" \"One potential fix is using functools.partial(\" f \" { af . __class__ . __name__ } , maximize=False) as the \" \"acquisition_function_factory init argument.\" ) acquisition_function_optimizer = _acquisition_function_optimizer_factory ( search_space = self . search_space , af_opt_kwargs = self . af_optimizer_kwargs , torch_dtype = self . torch_dtype , ) configuration , _ = acquisition_function_optimizer ( af ) return EvaluationSpecification ( configuration = self . search_space . from_numerical ( configuration [ 0 ]), ) def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Call the optimizer specific function and append a unique integer id to the specification. Please refer to the docstring of `blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification` for a description of the method. \"\"\" if ( self . max_pending_evaluations and len ( self . pending_specifications ) == self . max_pending_evaluations ): raise OptimizerNotReady if self . num_initial_random > 0 and ( self . X . size ( - 2 ) < self . num_initial_random or torch . nonzero ( ~ torch . any ( self . losses . isnan (), dim = 1 )) . numel () == 0 ): # We keep generating random samples until there are enough samples, and # at least one of them has a valid objective eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), ) else : eval_spec = self . _generate_evaluation_specification () eval_id = self . X . size ( - 2 ) + len ( self . pending_specifications ) eval_spec . optimizer_info [ \"evaluation_id\" ] = eval_id self . pending_specifications [ eval_id ] = eval_spec return eval_spec def _remove_pending_specifications ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]] ): \"\"\"Find and remove the corresponding entries in `self.pending_specifications`. Args: evaluations: List of completed evaluations. Raises: ValueError: If an evaluation is reported with an ID that was not issued by the optimizer, the method will fail. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations for e in _evals : if \"evaluation_id\" not in e . optimizer_info : self . logger . debug ( \"User provided EvaluationSpecification received.\" ) continue if e . optimizer_info [ \"evaluation_id\" ] not in self . pending_specifications : msg = ( \"Unknown evaluation_id reported. This could indicate that the \" \"evaluation has been reported before!\" ) self . logger . error ( msg ) raise ValueError ( msg ) del self . pending_specifications [ e . optimizer_info [ \"evaluation_id\" ]] def _append_evaluations_to_data ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]] ): \"\"\"Convert the reported evaluation into its numerical representation and append it to the training data. Args: evaluations: List of completed evaluations. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations X , Y = to_numerical ( _evals , self . search_space , objectives = [ self . objective ], batch_shape = self . batch_shape , torch_dtype = self . torch_dtype , ) # fill in NaNs originating from inactive parameters (conditional spaces support) # botorch expect numerical representation of inputs to be within the unit # hypercube, thus we can't use the default c=-1.0 X = impute_nans_with_constant ( X , c = 0.0 ) self . logger . debug ( f \"Next training configuration(s): { X } , { Y } \" ) self . X = torch . cat ([ self . X , X ], dim =- 2 ) self . losses = torch . cat ([ self . losses , Y ], dim =- 2 ) def _update_internal_evaluation_data ( self , evaluations : Iterable [ Evaluation ] ) -> None : \"\"\"Check validity of the evaluations and do optimizer agnostic bookkeeping.\"\"\" call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = [ self . objective ]), self . _remove_pending_specifications , self . _append_evaluations_to_data , ], sort_evaluations ( evaluations ), ) def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"A simple report method that conditions the model on data. This likely needs to be overridden for more specific BO implementations. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations self . _update_internal_evaluation_data ( _evals ) # Just for populating all relevant caches self . model . posterior ( self . X ) x_filtered , y_filtered = filter_y_nans ( self . X , self . losses ) # The actual model update # Ignore BotorchTensorDimensionWarning which is always reported to make the user # aware that they are reponsible for the right input Tensors dimensionality. with warnings . catch_warnings (): warnings . simplefilter ( action = \"ignore\" , category = BotorchTensorDimensionWarning ) self . model = self . model . condition_on_observations ( x_filtered , y_filtered ) def predict_model_based_best ( self ) -> Optional [ Evaluation ]: \"\"\"Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return `None` in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Returns: blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or `None` in case no evaluations have been reported yet. \"\"\" return predict_model_based_best ( model = self . model , objective = self . objective , search_space = self . search_space , torch_dtype = self . torch_dtype , ) generate_evaluation_specification ( self ) Call the optimizer specific function and append a unique integer id to the specification. Please refer to the docstring of blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification for a description of the method. Source code in blackboxopt/optimizers/botorch_base.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Call the optimizer specific function and append a unique integer id to the specification. Please refer to the docstring of `blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification` for a description of the method. \"\"\" if ( self . max_pending_evaluations and len ( self . pending_specifications ) == self . max_pending_evaluations ): raise OptimizerNotReady if self . num_initial_random > 0 and ( self . X . size ( - 2 ) < self . num_initial_random or torch . nonzero ( ~ torch . any ( self . losses . isnan (), dim = 1 )) . numel () == 0 ): # We keep generating random samples until there are enough samples, and # at least one of them has a valid objective eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), ) else : eval_spec = self . _generate_evaluation_specification () eval_id = self . X . size ( - 2 ) + len ( self . pending_specifications ) eval_spec . optimizer_info [ \"evaluation_id\" ] = eval_id self . pending_specifications [ eval_id ] = eval_spec return eval_spec predict_model_based_best ( self ) Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return None in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Returns: Type Description Optional[blackboxopt.evaluation.Evaluation] blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or None in case no evaluations have been reported yet. Source code in blackboxopt/optimizers/botorch_base.py def predict_model_based_best ( self ) -> Optional [ Evaluation ]: \"\"\"Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return `None` in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Returns: blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or `None` in case no evaluations have been reported yet. \"\"\" return predict_model_based_best ( model = self . model , objective = self . objective , search_space = self . search_space , torch_dtype = self . torch_dtype , ) report ( self , evaluations ) A simple report method that conditions the model on data. This likely needs to be overridden for more specific BO implementations. Source code in blackboxopt/optimizers/botorch_base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"A simple report method that conditions the model on data. This likely needs to be overridden for more specific BO implementations. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations self . _update_internal_evaluation_data ( _evals ) # Just for populating all relevant caches self . model . posterior ( self . X ) x_filtered , y_filtered = filter_y_nans ( self . X , self . losses ) # The actual model update # Ignore BotorchTensorDimensionWarning which is always reported to make the user # aware that they are reponsible for the right input Tensors dimensionality. with warnings . catch_warnings (): warnings . simplefilter ( action = \"ignore\" , category = BotorchTensorDimensionWarning ) self . model = self . model . condition_on_observations ( x_filtered , y_filtered )","title":"Botorch base"},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base","text":"","title":"botorch_base"},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer","text":"Source code in blackboxopt/optimizers/botorch_base.py class SingleObjectiveBOTorchOptimizer ( SingleObjectiveOptimizer ): def __init__ ( self , search_space : ps . ParameterSpace , objective : Objective , model : Model , acquisition_function_factory : Callable [[ Model ], AcquisitionFunction ], af_optimizer_kwargs = None , num_initial_random_samples : int = 1 , max_pending_evaluations : Optional [ int ] = 1 , batch_shape : torch . Size = torch . Size (), logger : Optional [ logging . Logger ] = None , seed : Optional [ int ] = None , torch_dtype : torch . dtype = torch . float64 , ): \"\"\"Single objective BO optimizer that uses as a surrogate model the `model` object provided by user. The `model` is expected to be extended from BoTorch base model `Model` class, and does not require to be a GP model. Args: search_space: The space in which to optimize. objective: The objective to optimize. model: Surrogate model of `Model` type. acquisition_function_factory: Callable that produces an acquisition function instance, could also be a compatible acquisition function class. Only acquisition functions to be minimized are supported. Providing a partially initialized class is possible with, e.g. `functools.partial(UpperConfidenceBound, beta=6.0, maximize=False)`. af_optimizer_kwargs: Settings for acquisition function optimizer, see `botorch.optim.optimize_acqf` and in case the whole search space is discrete: `botorch.optim.optimize_acqf_discrete`. The former can be enforced by providing `raw_samples` or `num_restarts`, the latter by providing `num_random_choices`. num_initial_random_samples: Size of the initial space-filling design that is used before starting BO. The points are sampled randomly in the search space. If no random sampling is required, set it to 0. max_pending_evaluations: Maximum number of parallel evaluations. For sequential BO use the default value of 1. If no limit is required, set it to None. batch_shape: Batch dimension(s) used for batched models. logger: Custom logger. seed: A seed to make the optimization reproducible. torch_dtype: Torch data type used for storing the data. This needs to match the dtype of the model used \"\"\" super () . __init__ ( search_space = search_space , objective = objective , seed = seed ) self . num_initial_random = num_initial_random_samples self . max_pending_evaluations = max_pending_evaluations self . batch_shape = batch_shape self . logger = logger or logging . getLogger ( \"blackboxopt\" ) self . torch_dtype = torch_dtype self . X = torch . empty ( ( * self . batch_shape , 0 , len ( search_space )), dtype = torch_dtype ) self . losses = torch . empty (( * self . batch_shape , 0 , 1 ), dtype = torch_dtype ) self . pending_specifications : Dict [ int , EvaluationSpecification ] = {} if seed is not None : torch . manual_seed ( seed = seed ) self . model = model self . acquisition_function_factory = acquisition_function_factory self . af_optimizer_kwargs = af_optimizer_kwargs def _create_fantasy_model ( self , model : Model ) -> Model : \"\"\"Create model with the pending specifications and model based outcomes added to the training data.\"\"\" if not self . pending_specifications : # nothing to do when there are no pending specs return model pending_X = torch . tensor ( np . array ( [ self . search_space . to_numerical ( e . configuration ) for e in self . pending_specifications . values () ] ), dtype = self . torch_dtype , ) model = model . fantasize ( pending_X , IIDNormalSampler ( 1 ), observation_noise = False ) if isinstance ( model , ExactGP ): # ExactGP.fantasize extends model's X and Y with batch_size, even if # originally not given -> need to reshape these to their original # representation n_samples = model . train_targets . size ( - 1 ) n_features = len ( self . search_space ) model . train_inputs [ 0 ] = model . train_inputs [ 0 ] . reshape ( torch . Size (( * self . batch_shape , n_samples , n_features )) ) model . train_targets = model . train_targets . reshape ( torch . Size (( * self . batch_shape , n_samples , 1 )) ) return model def _generate_evaluation_specification ( self ): \"\"\"Optimize acquisition on fantasy model to pick next point.\"\"\" fantasy_model = self . _create_fantasy_model ( self . model ) fantasy_model . eval () af = self . acquisition_function_factory ( fantasy_model ) if getattr ( af , \"maximize\" , False ): raise ValueError ( \"Only acquisition functions that need to be minimized are supported. \" f \"The given { af . __class__ . __name__ } has maximize=True. \" \"One potential fix is using functools.partial(\" f \" { af . __class__ . __name__ } , maximize=False) as the \" \"acquisition_function_factory init argument.\" ) acquisition_function_optimizer = _acquisition_function_optimizer_factory ( search_space = self . search_space , af_opt_kwargs = self . af_optimizer_kwargs , torch_dtype = self . torch_dtype , ) configuration , _ = acquisition_function_optimizer ( af ) return EvaluationSpecification ( configuration = self . search_space . from_numerical ( configuration [ 0 ]), ) def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Call the optimizer specific function and append a unique integer id to the specification. Please refer to the docstring of `blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification` for a description of the method. \"\"\" if ( self . max_pending_evaluations and len ( self . pending_specifications ) == self . max_pending_evaluations ): raise OptimizerNotReady if self . num_initial_random > 0 and ( self . X . size ( - 2 ) < self . num_initial_random or torch . nonzero ( ~ torch . any ( self . losses . isnan (), dim = 1 )) . numel () == 0 ): # We keep generating random samples until there are enough samples, and # at least one of them has a valid objective eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), ) else : eval_spec = self . _generate_evaluation_specification () eval_id = self . X . size ( - 2 ) + len ( self . pending_specifications ) eval_spec . optimizer_info [ \"evaluation_id\" ] = eval_id self . pending_specifications [ eval_id ] = eval_spec return eval_spec def _remove_pending_specifications ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]] ): \"\"\"Find and remove the corresponding entries in `self.pending_specifications`. Args: evaluations: List of completed evaluations. Raises: ValueError: If an evaluation is reported with an ID that was not issued by the optimizer, the method will fail. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations for e in _evals : if \"evaluation_id\" not in e . optimizer_info : self . logger . debug ( \"User provided EvaluationSpecification received.\" ) continue if e . optimizer_info [ \"evaluation_id\" ] not in self . pending_specifications : msg = ( \"Unknown evaluation_id reported. This could indicate that the \" \"evaluation has been reported before!\" ) self . logger . error ( msg ) raise ValueError ( msg ) del self . pending_specifications [ e . optimizer_info [ \"evaluation_id\" ]] def _append_evaluations_to_data ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]] ): \"\"\"Convert the reported evaluation into its numerical representation and append it to the training data. Args: evaluations: List of completed evaluations. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations X , Y = to_numerical ( _evals , self . search_space , objectives = [ self . objective ], batch_shape = self . batch_shape , torch_dtype = self . torch_dtype , ) # fill in NaNs originating from inactive parameters (conditional spaces support) # botorch expect numerical representation of inputs to be within the unit # hypercube, thus we can't use the default c=-1.0 X = impute_nans_with_constant ( X , c = 0.0 ) self . logger . debug ( f \"Next training configuration(s): { X } , { Y } \" ) self . X = torch . cat ([ self . X , X ], dim =- 2 ) self . losses = torch . cat ([ self . losses , Y ], dim =- 2 ) def _update_internal_evaluation_data ( self , evaluations : Iterable [ Evaluation ] ) -> None : \"\"\"Check validity of the evaluations and do optimizer agnostic bookkeeping.\"\"\" call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = [ self . objective ]), self . _remove_pending_specifications , self . _append_evaluations_to_data , ], sort_evaluations ( evaluations ), ) def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"A simple report method that conditions the model on data. This likely needs to be overridden for more specific BO implementations. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations self . _update_internal_evaluation_data ( _evals ) # Just for populating all relevant caches self . model . posterior ( self . X ) x_filtered , y_filtered = filter_y_nans ( self . X , self . losses ) # The actual model update # Ignore BotorchTensorDimensionWarning which is always reported to make the user # aware that they are reponsible for the right input Tensors dimensionality. with warnings . catch_warnings (): warnings . simplefilter ( action = \"ignore\" , category = BotorchTensorDimensionWarning ) self . model = self . model . condition_on_observations ( x_filtered , y_filtered ) def predict_model_based_best ( self ) -> Optional [ Evaluation ]: \"\"\"Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return `None` in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Returns: blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or `None` in case no evaluations have been reported yet. \"\"\" return predict_model_based_best ( model = self . model , objective = self . objective , search_space = self . search_space , torch_dtype = self . torch_dtype , )","title":"SingleObjectiveBOTorchOptimizer"},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.generate_evaluation_specification","text":"Call the optimizer specific function and append a unique integer id to the specification. Please refer to the docstring of blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification for a description of the method. Source code in blackboxopt/optimizers/botorch_base.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Call the optimizer specific function and append a unique integer id to the specification. Please refer to the docstring of `blackboxopt.base.SingleObjectiveOptimizer.generate_evaluation_specification` for a description of the method. \"\"\" if ( self . max_pending_evaluations and len ( self . pending_specifications ) == self . max_pending_evaluations ): raise OptimizerNotReady if self . num_initial_random > 0 and ( self . X . size ( - 2 ) < self . num_initial_random or torch . nonzero ( ~ torch . any ( self . losses . isnan (), dim = 1 )) . numel () == 0 ): # We keep generating random samples until there are enough samples, and # at least one of them has a valid objective eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), ) else : eval_spec = self . _generate_evaluation_specification () eval_id = self . X . size ( - 2 ) + len ( self . pending_specifications ) eval_spec . optimizer_info [ \"evaluation_id\" ] = eval_id self . pending_specifications [ eval_id ] = eval_spec return eval_spec","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.predict_model_based_best","text":"Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return None in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Returns: Type Description Optional[blackboxopt.evaluation.Evaluation] blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or None in case no evaluations have been reported yet. Source code in blackboxopt/optimizers/botorch_base.py def predict_model_based_best ( self ) -> Optional [ Evaluation ]: \"\"\"Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return `None` in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Returns: blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or `None` in case no evaluations have been reported yet. \"\"\" return predict_model_based_best ( model = self . model , objective = self . objective , search_space = self . search_space , torch_dtype = self . torch_dtype , )","title":"predict_model_based_best()"},{"location":"reference/optimizers/botorch_base/#blackboxopt.optimizers.botorch_base.SingleObjectiveBOTorchOptimizer.report","text":"A simple report method that conditions the model on data. This likely needs to be overridden for more specific BO implementations. Source code in blackboxopt/optimizers/botorch_base.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : \"\"\"A simple report method that conditions the model on data. This likely needs to be overridden for more specific BO implementations. \"\"\" _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations self . _update_internal_evaluation_data ( _evals ) # Just for populating all relevant caches self . model . posterior ( self . X ) x_filtered , y_filtered = filter_y_nans ( self . X , self . losses ) # The actual model update # Ignore BotorchTensorDimensionWarning which is always reported to make the user # aware that they are reponsible for the right input Tensors dimensionality. with warnings . catch_warnings (): warnings . simplefilter ( action = \"ignore\" , category = BotorchTensorDimensionWarning ) self . model = self . model . condition_on_observations ( x_filtered , y_filtered )","title":"report()"},{"location":"reference/optimizers/botorch_utils/","text":"blackboxopt.optimizers.botorch_utils filter_y_nans ( x , y ) Filter rows jointly for x and y , where y is NaN . Parameters: Name Type Description Default x Tensor Input tensor of shape n x d or 1 x n x d . required y Tensor Input tensor of shape n x m or 1 x n x m . required Returns: Type Description - x_f Filtered x . - y_f: Filtered y . Exceptions: Type Description ValueError If input is 3D (batched representation) with first dimension not 1 (multiple batches). Source code in blackboxopt/optimizers/botorch_utils.py def filter_y_nans ( x : torch . Tensor , y : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Filter rows jointly for `x` and `y`, where `y` is `NaN`. Args: x: Input tensor of shape `n x d` or `1 x n x d`. y: Input tensor of shape `n x m` or `1 x n x m`. Returns: - x_f: Filtered `x`. - y_f: Filtered `y`. Raises: ValueError: If input is 3D (batched representation) with first dimension not `1` (multiple batches). \"\"\" if ( len ( x . shape ) == 3 and x . shape [ 0 ] > 1 ) or ( len ( y . shape ) == 3 and y . shape [ 0 ] > 1 ): raise ValueError ( \"Multiple batches are not supported for now.\" ) x_f = x . clone () y_f = y . clone () # filter rows jointly where y is NaN x_f = x_f [ ~ torch . any ( y_f . isnan (), dim =- 1 )] y_f = y_f [ ~ torch . any ( y_f . isnan (), dim =- 1 )] # cast n x d back to 1 x n x d if originally batch case if len ( x . shape ) == 3 : x_f = x_f . reshape ( torch . Size (( 1 ,)) + x_f . shape ) if len ( y . shape ) == 3 : y_f = y_f . reshape ( torch . Size (( 1 ,)) + y_f . shape ) return x_f , y_f impute_nans_with_constant ( x , c =- 1.0 ) Impute NaN values with given constant value. Parameters: Name Type Description Default x Tensor Input tensor of shape n x d or b x n x d . required c float Constant used as fill value to replace NaNs . -1.0 Returns: Type Description Tensor x_i - x where all NaN s are replaced with given constant. Source code in blackboxopt/optimizers/botorch_utils.py def impute_nans_with_constant ( x : torch . Tensor , c : float = - 1.0 ) -> torch . Tensor : \"\"\"Impute `NaN` values with given constant value. Args: x: Input tensor of shape `n x d` or `b x n x d`. c: Constant used as fill value to replace `NaNs`. Returns: - x_i - `x` where all `NaN`s are replaced with given constant. \"\"\" if x . numel () == 0 : # empty tensor, nothing to impute return x x_i = x . clone () # cast n x d to 1 x n x d (cover non-batch case) if len ( x . shape ) == 2 : x_i = x_i . reshape ( torch . Size (( 1 ,)) + x_i . shape ) for b in range ( x_i . shape [ 0 ]): x_1 = x_i [ b , :, :] x_1 = torch . tensor ( SimpleImputer ( missing_values = np . nan , strategy = \"constant\" , fill_value = c ) . fit_transform ( x_1 ), dtype = x . dtype , ) x_i [ b , :, :] = x_1 # cast 1 x n x d back to n x d if originally non-batch if len ( x . shape ) == 2 : x_i = x_i . reshape ( x . shape ) return x_i predict_model_based_best ( model , search_space , objective , torch_dtype ) Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return None in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Parameters: Name Type Description Default model Model The model to use for predicting the best. required search_space ParameterSpace Space to convert between numerical and original configurations. required objective Objective Objective to convert the model based loss prediction to the target. required Returns: Type Description Optional[blackboxopt.evaluation.Evaluation] blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or None in case no evaluations have been reported yet. Source code in blackboxopt/optimizers/botorch_utils.py def predict_model_based_best ( model : botorch . models . model . Model , search_space : ps . ParameterSpace , objective : Objective , torch_dtype : torch . dtype , ) -> Optional [ Evaluation ]: \"\"\"Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return `None` in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Args: model: The model to use for predicting the best. search_space: Space to convert between numerical and original configurations. objective: Objective to convert the model based loss prediction to the target. Returns: blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or `None` in case no evaluations have been reported yet. \"\"\" if model . train_inputs [ 0 ] . numel () == 0 : return None def posterior_mean ( x ): # function to be optimized: posterior mean # scipy's minimize expects the following interface: # - input: 1-D array with shape (n,) # - output: float mean = model . posterior ( torch . from_numpy ( np . atleast_2d ( x ))) . mean return mean . item () # prepare initial random samples and bounds for scipy's minimize n_init_samples = 10 init_points = np . asarray ( [ search_space . to_numerical ( search_space . sample ()) for _ in range ( n_init_samples ) ] ) # use scipy's minimize to find optimum of the posterior mean optimized_points = [ sci_opt . minimize ( fun = posterior_mean , constraints = None , jac = False , x0 = x , args = (), # The numerical representation always lives on the unit hypercube bounds = torch . Tensor ([[ 0 , 1 ]] * len ( search_space )) . to ( dtype = torch_dtype ), method = \"L-BFGS-B\" , options = None , ) for x in init_points ] f_optimized = np . array ([ np . atleast_1d ( p . fun ) for p in optimized_points ]) . flatten () # get indexes of optimum value (with a tolerance) inds = np . argwhere ( np . isclose ( f_optimized , np . min ( f_optimized ))) # randomly select one index if there are multiple ind = np . random . choice ( inds . flatten ()) # create Evaluation from the best estimated configuration best_x = optimized_points [ ind ] . x best_y = posterior_mean ( best_x ) return Evaluation ( configuration = search_space . from_numerical ( best_x ), objectives = { objective . name : - 1 * best_y if objective . greater_is_better else best_y }, ) to_numerical ( evaluations , search_space , objectives , constraint_names = None , batch_shape = torch . Size ([]), torch_dtype = torch . float32 ) Convert evaluations to one (#batch, #evaluations, #parameters) tensor containing the numerical representations of the configurations and one (#batch, #evaluations, 1) tensor containing the loss representation of the evaluations' objective value (flips the sign for objective value if objective.greater_is_better=True ) and optionally constraints value. Parameters: Name Type Description Default evaluations Iterable[blackboxopt.evaluation.Evaluation] List of evaluations that were collected during optimization. required search_space ParameterSpace Search space used during optimization. required objectives Sequence[blackboxopt.base.Objective] Objectives that were used for optimization. required constraint_names Optional[List[str]] Name of constraints that are used for optimization. None batch_shape Size Batch dimension(s) used for batched models. torch.Size([]) torch_dtype dtype Type of returned tensors. torch.float32 Returns: Type Description - X Numerical representation of the configurations - Y: Numerical representation of the objective values and optionally constraints Exceptions: Type Description ValueError If one of configurations is not valid w.r.t. search space. ValueError If one of configurations includes parameters that are not part of the search space. ConstraintError If one of the constraint names is not defined in evaluations. Source code in blackboxopt/optimizers/botorch_utils.py def to_numerical ( evaluations : Iterable [ Evaluation ], search_space : ps . ParameterSpace , objectives : Sequence [ Objective ], constraint_names : Optional [ List [ str ]] = None , batch_shape : torch . Size = torch . Size (), torch_dtype : torch . dtype = torch . float32 , ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Convert evaluations to one `(#batch, #evaluations, #parameters)` tensor containing the numerical representations of the configurations and one `(#batch, #evaluations, 1)` tensor containing the loss representation of the evaluations' objective value (flips the sign for objective value if `objective.greater_is_better=True`) and optionally constraints value. Args: evaluations: List of evaluations that were collected during optimization. search_space: Search space used during optimization. objectives: Objectives that were used for optimization. constraint_names: Name of constraints that are used for optimization. batch_shape: Batch dimension(s) used for batched models. torch_dtype: Type of returned tensors. Returns: - X: Numerical representation of the configurations - Y: Numerical representation of the objective values and optionally constraints Raises: ValueError: If one of configurations is not valid w.r.t. search space. ValueError: If one of configurations includes parameters that are not part of the search space. ConstraintError: If one of the constraint names is not defined in evaluations. \"\"\" # validate configuration values and dimensions parameter_names = search_space . get_parameter_names () + list ( search_space . get_constant_names () ) for e in evaluations : with warnings . catch_warnings (): # we already raise error if search space not valid, thus can ignore warnings warnings . filterwarnings ( \"ignore\" , category = RuntimeWarning , message = \"Parameter\" ) if not search_space . check_validity ( e . configuration ): raise ValueError ( f \"The provided configuration { e . configuration } is not valid.\" ) if not set ( parameter_names ) >= set ( e . configuration . keys ()): raise ValueError ( f \"Mismatch in parameter names from search space { parameter_names } and \" + f \"configuration { e . configuration } \" ) X = torch . tensor ( np . array ([ search_space . to_numerical ( e . configuration ) for e in evaluations ]), dtype = torch_dtype , ) X = X . reshape ( * batch_shape + X . shape ) Y = torch . Tensor ( [ get_loss_vector ( known_objectives = objectives , reported_objectives = e . objectives ) for e in evaluations ] ) . to ( dtype = torch_dtype ) if constraint_names is not None : try : Y_constraints = torch . tensor ( np . array ( [[ e . constraints [ c ] for c in constraint_names ] for e in evaluations ], dtype = float , ), dtype = torch_dtype , ) Y = torch . cat (( Y , Y_constraints ), dim = 1 ) except KeyError as e : raise ConstraintsError ( f \"Constraint name { e } is not defined in input evaluations.\" ) from e except TypeError as e : raise ConstraintsError ( f \"Constraint name(s) { constraint_names } are not defined in input \" + \"evaluations.\" ) from e Y = Y . reshape ( * batch_shape + Y . shape ) return X , Y","title":"Botorch utils"},{"location":"reference/optimizers/botorch_utils/#blackboxopt.optimizers.botorch_utils","text":"","title":"botorch_utils"},{"location":"reference/optimizers/botorch_utils/#blackboxopt.optimizers.botorch_utils.filter_y_nans","text":"Filter rows jointly for x and y , where y is NaN . Parameters: Name Type Description Default x Tensor Input tensor of shape n x d or 1 x n x d . required y Tensor Input tensor of shape n x m or 1 x n x m . required Returns: Type Description - x_f Filtered x . - y_f: Filtered y . Exceptions: Type Description ValueError If input is 3D (batched representation) with first dimension not 1 (multiple batches). Source code in blackboxopt/optimizers/botorch_utils.py def filter_y_nans ( x : torch . Tensor , y : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Filter rows jointly for `x` and `y`, where `y` is `NaN`. Args: x: Input tensor of shape `n x d` or `1 x n x d`. y: Input tensor of shape `n x m` or `1 x n x m`. Returns: - x_f: Filtered `x`. - y_f: Filtered `y`. Raises: ValueError: If input is 3D (batched representation) with first dimension not `1` (multiple batches). \"\"\" if ( len ( x . shape ) == 3 and x . shape [ 0 ] > 1 ) or ( len ( y . shape ) == 3 and y . shape [ 0 ] > 1 ): raise ValueError ( \"Multiple batches are not supported for now.\" ) x_f = x . clone () y_f = y . clone () # filter rows jointly where y is NaN x_f = x_f [ ~ torch . any ( y_f . isnan (), dim =- 1 )] y_f = y_f [ ~ torch . any ( y_f . isnan (), dim =- 1 )] # cast n x d back to 1 x n x d if originally batch case if len ( x . shape ) == 3 : x_f = x_f . reshape ( torch . Size (( 1 ,)) + x_f . shape ) if len ( y . shape ) == 3 : y_f = y_f . reshape ( torch . Size (( 1 ,)) + y_f . shape ) return x_f , y_f","title":"filter_y_nans()"},{"location":"reference/optimizers/botorch_utils/#blackboxopt.optimizers.botorch_utils.impute_nans_with_constant","text":"Impute NaN values with given constant value. Parameters: Name Type Description Default x Tensor Input tensor of shape n x d or b x n x d . required c float Constant used as fill value to replace NaNs . -1.0 Returns: Type Description Tensor x_i - x where all NaN s are replaced with given constant. Source code in blackboxopt/optimizers/botorch_utils.py def impute_nans_with_constant ( x : torch . Tensor , c : float = - 1.0 ) -> torch . Tensor : \"\"\"Impute `NaN` values with given constant value. Args: x: Input tensor of shape `n x d` or `b x n x d`. c: Constant used as fill value to replace `NaNs`. Returns: - x_i - `x` where all `NaN`s are replaced with given constant. \"\"\" if x . numel () == 0 : # empty tensor, nothing to impute return x x_i = x . clone () # cast n x d to 1 x n x d (cover non-batch case) if len ( x . shape ) == 2 : x_i = x_i . reshape ( torch . Size (( 1 ,)) + x_i . shape ) for b in range ( x_i . shape [ 0 ]): x_1 = x_i [ b , :, :] x_1 = torch . tensor ( SimpleImputer ( missing_values = np . nan , strategy = \"constant\" , fill_value = c ) . fit_transform ( x_1 ), dtype = x . dtype , ) x_i [ b , :, :] = x_1 # cast 1 x n x d back to n x d if originally non-batch if len ( x . shape ) == 2 : x_i = x_i . reshape ( x . shape ) return x_i","title":"impute_nans_with_constant()"},{"location":"reference/optimizers/botorch_utils/#blackboxopt.optimizers.botorch_utils.predict_model_based_best","text":"Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return None in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Parameters: Name Type Description Default model Model The model to use for predicting the best. required search_space ParameterSpace Space to convert between numerical and original configurations. required objective Objective Objective to convert the model based loss prediction to the target. required Returns: Type Description Optional[blackboxopt.evaluation.Evaluation] blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or None in case no evaluations have been reported yet. Source code in blackboxopt/optimizers/botorch_utils.py def predict_model_based_best ( model : botorch . models . model . Model , search_space : ps . ParameterSpace , objective : Objective , torch_dtype : torch . dtype , ) -> Optional [ Evaluation ]: \"\"\"Get the current configuration that is estimated to be the best (in terms of optimal objective value) without waiting for a reported evaluation of that configuration. Instead, the objective value estimation relies on BO's underlying model. This might return `None` in case there is no successfully evaluated configuration yet (thus, the optimizer has not been given training data yet). Args: model: The model to use for predicting the best. search_space: Space to convert between numerical and original configurations. objective: Objective to convert the model based loss prediction to the target. Returns: blackboxopt.evaluation.Evaluation The evaluated specification containing the estimated best configuration or `None` in case no evaluations have been reported yet. \"\"\" if model . train_inputs [ 0 ] . numel () == 0 : return None def posterior_mean ( x ): # function to be optimized: posterior mean # scipy's minimize expects the following interface: # - input: 1-D array with shape (n,) # - output: float mean = model . posterior ( torch . from_numpy ( np . atleast_2d ( x ))) . mean return mean . item () # prepare initial random samples and bounds for scipy's minimize n_init_samples = 10 init_points = np . asarray ( [ search_space . to_numerical ( search_space . sample ()) for _ in range ( n_init_samples ) ] ) # use scipy's minimize to find optimum of the posterior mean optimized_points = [ sci_opt . minimize ( fun = posterior_mean , constraints = None , jac = False , x0 = x , args = (), # The numerical representation always lives on the unit hypercube bounds = torch . Tensor ([[ 0 , 1 ]] * len ( search_space )) . to ( dtype = torch_dtype ), method = \"L-BFGS-B\" , options = None , ) for x in init_points ] f_optimized = np . array ([ np . atleast_1d ( p . fun ) for p in optimized_points ]) . flatten () # get indexes of optimum value (with a tolerance) inds = np . argwhere ( np . isclose ( f_optimized , np . min ( f_optimized ))) # randomly select one index if there are multiple ind = np . random . choice ( inds . flatten ()) # create Evaluation from the best estimated configuration best_x = optimized_points [ ind ] . x best_y = posterior_mean ( best_x ) return Evaluation ( configuration = search_space . from_numerical ( best_x ), objectives = { objective . name : - 1 * best_y if objective . greater_is_better else best_y }, )","title":"predict_model_based_best()"},{"location":"reference/optimizers/botorch_utils/#blackboxopt.optimizers.botorch_utils.to_numerical","text":"Convert evaluations to one (#batch, #evaluations, #parameters) tensor containing the numerical representations of the configurations and one (#batch, #evaluations, 1) tensor containing the loss representation of the evaluations' objective value (flips the sign for objective value if objective.greater_is_better=True ) and optionally constraints value. Parameters: Name Type Description Default evaluations Iterable[blackboxopt.evaluation.Evaluation] List of evaluations that were collected during optimization. required search_space ParameterSpace Search space used during optimization. required objectives Sequence[blackboxopt.base.Objective] Objectives that were used for optimization. required constraint_names Optional[List[str]] Name of constraints that are used for optimization. None batch_shape Size Batch dimension(s) used for batched models. torch.Size([]) torch_dtype dtype Type of returned tensors. torch.float32 Returns: Type Description - X Numerical representation of the configurations - Y: Numerical representation of the objective values and optionally constraints Exceptions: Type Description ValueError If one of configurations is not valid w.r.t. search space. ValueError If one of configurations includes parameters that are not part of the search space. ConstraintError If one of the constraint names is not defined in evaluations. Source code in blackboxopt/optimizers/botorch_utils.py def to_numerical ( evaluations : Iterable [ Evaluation ], search_space : ps . ParameterSpace , objectives : Sequence [ Objective ], constraint_names : Optional [ List [ str ]] = None , batch_shape : torch . Size = torch . Size (), torch_dtype : torch . dtype = torch . float32 , ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Convert evaluations to one `(#batch, #evaluations, #parameters)` tensor containing the numerical representations of the configurations and one `(#batch, #evaluations, 1)` tensor containing the loss representation of the evaluations' objective value (flips the sign for objective value if `objective.greater_is_better=True`) and optionally constraints value. Args: evaluations: List of evaluations that were collected during optimization. search_space: Search space used during optimization. objectives: Objectives that were used for optimization. constraint_names: Name of constraints that are used for optimization. batch_shape: Batch dimension(s) used for batched models. torch_dtype: Type of returned tensors. Returns: - X: Numerical representation of the configurations - Y: Numerical representation of the objective values and optionally constraints Raises: ValueError: If one of configurations is not valid w.r.t. search space. ValueError: If one of configurations includes parameters that are not part of the search space. ConstraintError: If one of the constraint names is not defined in evaluations. \"\"\" # validate configuration values and dimensions parameter_names = search_space . get_parameter_names () + list ( search_space . get_constant_names () ) for e in evaluations : with warnings . catch_warnings (): # we already raise error if search space not valid, thus can ignore warnings warnings . filterwarnings ( \"ignore\" , category = RuntimeWarning , message = \"Parameter\" ) if not search_space . check_validity ( e . configuration ): raise ValueError ( f \"The provided configuration { e . configuration } is not valid.\" ) if not set ( parameter_names ) >= set ( e . configuration . keys ()): raise ValueError ( f \"Mismatch in parameter names from search space { parameter_names } and \" + f \"configuration { e . configuration } \" ) X = torch . tensor ( np . array ([ search_space . to_numerical ( e . configuration ) for e in evaluations ]), dtype = torch_dtype , ) X = X . reshape ( * batch_shape + X . shape ) Y = torch . Tensor ( [ get_loss_vector ( known_objectives = objectives , reported_objectives = e . objectives ) for e in evaluations ] ) . to ( dtype = torch_dtype ) if constraint_names is not None : try : Y_constraints = torch . tensor ( np . array ( [[ e . constraints [ c ] for c in constraint_names ] for e in evaluations ], dtype = float , ), dtype = torch_dtype , ) Y = torch . cat (( Y , Y_constraints ), dim = 1 ) except KeyError as e : raise ConstraintsError ( f \"Constraint name { e } is not defined in input evaluations.\" ) from e except TypeError as e : raise ConstraintsError ( f \"Constraint name(s) { constraint_names } are not defined in input \" + \"evaluations.\" ) from e Y = Y . reshape ( * batch_shape + Y . shape ) return X , Y","title":"to_numerical()"},{"location":"reference/optimizers/hyperband/","text":"blackboxopt.optimizers.hyperband Hyperband ( StagedIterationOptimizer ) Source code in blackboxopt/optimizers/hyperband.py class Hyperband ( StagedIterationOptimizer ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , min_fidelity : float , max_fidelity : float , num_iterations : int , eta : float = 3.0 , seed : int = None , logger : logging . Logger = None , ): \"\"\"Implementation of Hyperband as proposed in Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., & Talwalkar, A. (2016). Hyperband: A novel bandit-based approach to hyperparameter optimization. arXiv preprint arXiv:1603.06560. Args: search_space: [description] objective: [description] min_fidelity: The smallest fidelity value that is still meaningful. Must be strictly greater than zero! max_fidelity: The largest fidelity value used during the optimization. Must not be smaller than `min_fidelity` num_iterations: [description] eta: Scaling parameter to control the aggressiveness of Hyperband's racing. seed: [description] logger: [description] \"\"\" self . config_sampler = RandomSearchSampler ( search_space ) self . min_fidelity = min_fidelity self . max_fidelity = max_fidelity self . eta = eta super () . __init__ ( search_space = search_space , objective = objective , num_iterations = num_iterations , seed = seed , logger = logger , ) def _create_new_iteration ( self , iteration_index : int ) -> StagedIteration : \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.staged.iteration.StagedIteration` object \"\"\" return create_hyperband_iteration ( iteration_index , self . min_fidelity , self . max_fidelity , self . eta , self . config_sampler , self . objective , self . logger , ) generate_evaluation_specification ( self ) inherited Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/hyperband.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady report ( self , evaluations ) inherited Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/hyperband.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"Hyperband"},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband","text":"","title":"hyperband"},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband.Hyperband","text":"Source code in blackboxopt/optimizers/hyperband.py class Hyperband ( StagedIterationOptimizer ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , min_fidelity : float , max_fidelity : float , num_iterations : int , eta : float = 3.0 , seed : int = None , logger : logging . Logger = None , ): \"\"\"Implementation of Hyperband as proposed in Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., & Talwalkar, A. (2016). Hyperband: A novel bandit-based approach to hyperparameter optimization. arXiv preprint arXiv:1603.06560. Args: search_space: [description] objective: [description] min_fidelity: The smallest fidelity value that is still meaningful. Must be strictly greater than zero! max_fidelity: The largest fidelity value used during the optimization. Must not be smaller than `min_fidelity` num_iterations: [description] eta: Scaling parameter to control the aggressiveness of Hyperband's racing. seed: [description] logger: [description] \"\"\" self . config_sampler = RandomSearchSampler ( search_space ) self . min_fidelity = min_fidelity self . max_fidelity = max_fidelity self . eta = eta super () . __init__ ( search_space = search_space , objective = objective , num_iterations = num_iterations , seed = seed , logger = logger , ) def _create_new_iteration ( self , iteration_index : int ) -> StagedIteration : \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.staged.iteration.StagedIteration` object \"\"\" return create_hyperband_iteration ( iteration_index , self . min_fidelity , self . max_fidelity , self . eta , self . config_sampler , self . objective , self . logger , )","title":"Hyperband"},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband.Hyperband.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/hyperband.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/hyperband/#blackboxopt.optimizers.hyperband.Hyperband.report","text":"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/hyperband.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"report()"},{"location":"reference/optimizers/random_search/","text":"blackboxopt.optimizers.random_search RandomSearch ( MultiObjectiveOptimizer ) Source code in blackboxopt/optimizers/random_search.py class RandomSearch ( MultiObjectiveOptimizer ): def __init__ ( self , search_space : ParameterSpace , objectives : List [ Objective ], max_steps : int , seed : int = None , ) -> None : \"\"\"Randomly sample up to `max_steps` configurations from the given search space. Args: search_space: Space to search objectives: The objectives of the optimization. max_steps: Max number of evaluation specifications the optimizer generates before raising `OptimizationComplete` seed: Optional number to seed the random number generator with. Defaults to None. \"\"\" super () . __init__ ( search_space = search_space , objectives = objectives , seed = seed ) self . max_steps : int = max_steps self . n_steps : int = 0 def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"[summary] Raises: OptimizationComplete: Raised if the optimizer's `max_steps` are reached. Returns: [description] \"\"\" if self . n_steps >= self . max_steps : raise OptimizationComplete () eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), settings = {}, optimizer_info = { \"step\" : self . n_steps }, ) self . n_steps += 1 return eval_spec generate_evaluation_specification ( self ) [summary] Exceptions: Type Description OptimizationComplete Raised if the optimizer's max_steps are reached. Returns: Type Description EvaluationSpecification [description] Source code in blackboxopt/optimizers/random_search.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"[summary] Raises: OptimizationComplete: Raised if the optimizer's `max_steps` are reached. Returns: [description] \"\"\" if self . n_steps >= self . max_steps : raise OptimizationComplete () eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), settings = {}, optimizer_info = { \"step\" : self . n_steps }, ) self . n_steps += 1 return eval_spec report ( self , evaluations ) inherited Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Source code in blackboxopt/optimizers/random_search.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , )","title":"Random search"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search","text":"","title":"random_search"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search.RandomSearch","text":"Source code in blackboxopt/optimizers/random_search.py class RandomSearch ( MultiObjectiveOptimizer ): def __init__ ( self , search_space : ParameterSpace , objectives : List [ Objective ], max_steps : int , seed : int = None , ) -> None : \"\"\"Randomly sample up to `max_steps` configurations from the given search space. Args: search_space: Space to search objectives: The objectives of the optimization. max_steps: Max number of evaluation specifications the optimizer generates before raising `OptimizationComplete` seed: Optional number to seed the random number generator with. Defaults to None. \"\"\" super () . __init__ ( search_space = search_space , objectives = objectives , seed = seed ) self . max_steps : int = max_steps self . n_steps : int = 0 def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"[summary] Raises: OptimizationComplete: Raised if the optimizer's `max_steps` are reached. Returns: [description] \"\"\" if self . n_steps >= self . max_steps : raise OptimizationComplete () eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), settings = {}, optimizer_info = { \"step\" : self . n_steps }, ) self . n_steps += 1 return eval_spec","title":"RandomSearch"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search.RandomSearch.generate_evaluation_specification","text":"[summary] Exceptions: Type Description OptimizationComplete Raised if the optimizer's max_steps are reached. Returns: Type Description EvaluationSpecification [description] Source code in blackboxopt/optimizers/random_search.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"[summary] Raises: OptimizationComplete: Raised if the optimizer's `max_steps` are reached. Returns: [description] \"\"\" if self . n_steps >= self . max_steps : raise OptimizationComplete () eval_spec = EvaluationSpecification ( configuration = self . search_space . sample (), settings = {}, optimizer_info = { \"step\" : self . n_steps }, ) self . n_steps += 1 return eval_spec","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/random_search/#blackboxopt.optimizers.random_search.RandomSearch.report","text":"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Source code in blackboxopt/optimizers/random_search.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , )","title":"report()"},{"location":"reference/optimizers/space_filling/","text":"blackboxopt.optimizers.space_filling SpaceFilling ( MultiObjectiveOptimizer ) Sobol sequence based, space filling optimizer. Parameters: Name Type Description Default search_space ParameterSpace The search space to optimize required objectives List[blackboxopt.base.Objective] The objectives of the optimization required seed int The sobol sequence is Owen scrambled and can be seeded for reproducibility None Source code in blackboxopt/optimizers/space_filling.py class SpaceFilling ( MultiObjectiveOptimizer ): \"\"\"Sobol sequence based, space filling optimizer. Args: search_space: The search space to optimize objectives: The objectives of the optimization seed: The sobol sequence is Owen scrambled and can be seeded for reproducibility \"\"\" def __init__ ( self , search_space : ParameterSpace , objectives : List [ Objective ], seed : int = None , ) -> None : super () . __init__ ( search_space = search_space , objectives = objectives , seed = seed ) self . sobol = Sobol ( d = len ( self . search_space ), scramble = True , seed = seed ) def generate_evaluation_specification ( self ) -> EvaluationSpecification : vector = self . sobol . random () . flatten () configuration = self . search_space . from_numerical ( vector ) return EvaluationSpecification ( configuration = configuration ) generate_evaluation_specification ( self ) Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/space_filling.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : vector = self . sobol . random () . flatten () configuration = self . search_space . from_numerical ( vector ) return EvaluationSpecification ( configuration = configuration ) report ( self , evaluations ) inherited Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Source code in blackboxopt/optimizers/space_filling.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , )","title":"Space filling"},{"location":"reference/optimizers/space_filling/#blackboxopt.optimizers.space_filling","text":"","title":"space_filling"},{"location":"reference/optimizers/space_filling/#blackboxopt.optimizers.space_filling.SpaceFilling","text":"Sobol sequence based, space filling optimizer. Parameters: Name Type Description Default search_space ParameterSpace The search space to optimize required objectives List[blackboxopt.base.Objective] The objectives of the optimization required seed int The sobol sequence is Owen scrambled and can be seeded for reproducibility None Source code in blackboxopt/optimizers/space_filling.py class SpaceFilling ( MultiObjectiveOptimizer ): \"\"\"Sobol sequence based, space filling optimizer. Args: search_space: The search space to optimize objectives: The objectives of the optimization seed: The sobol sequence is Owen scrambled and can be seeded for reproducibility \"\"\" def __init__ ( self , search_space : ParameterSpace , objectives : List [ Objective ], seed : int = None , ) -> None : super () . __init__ ( search_space = search_space , objectives = objectives , seed = seed ) self . sobol = Sobol ( d = len ( self . search_space ), scramble = True , seed = seed ) def generate_evaluation_specification ( self ) -> EvaluationSpecification : vector = self . sobol . random () . flatten () configuration = self . search_space . from_numerical ( vector ) return EvaluationSpecification ( configuration = configuration )","title":"SpaceFilling"},{"location":"reference/optimizers/space_filling/#blackboxopt.optimizers.space_filling.SpaceFilling.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/space_filling.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : vector = self . sobol . random () . flatten () configuration = self . search_space . from_numerical ( vector ) return EvaluationSpecification ( configuration = configuration )","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/space_filling/#blackboxopt.optimizers.space_filling.SpaceFilling.report","text":"Report one or more evaluated evaluation specifications. NOTE: Not all optimizers support reporting results for evaluation specifications that were not proposed by the optimizer. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Source code in blackboxopt/optimizers/space_filling.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ functools . partial ( validate_objectives , objectives = self . objectives )], _evals , )","title":"report()"},{"location":"reference/optimizers/testing/","text":"blackboxopt.optimizers.testing Tests that can be imported and used to test optimizer implementations against this packages blackbox optimizer interface. handles_conditional_space ( optimizer_class , optimizer_kwargs , seed = None , n_max_evaluations = 10 ) Check if optimizer handles conditional i.e. hierarchical search spaces. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None n_max_evaluations int Maximum number of evaluation to try 10 Source code in blackboxopt/optimizers/testing.py def handles_conditional_space ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , n_max_evaluations : int = 10 , ): \"\"\"Check if optimizer handles conditional i.e. hierarchical search spaces. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed n_max_evaluations: Maximum number of evaluation to try \"\"\" space = ps . ParameterSpace () space . add ( ps . CategoricalParameter ( \"optimizer\" , ( \"adam\" , \"sgd\" ))) space . add ( ps . ContinuousParameter ( \"lr\" , ( 0.0001 , 0.1 ), transformation = \"log\" )) space . add ( ps . ContinuousParameter ( \"momentum\" , ( 0.0 , 1.0 )), lambda optimizer : optimizer == \"sgd\" , ) space . seed ( seed ) opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , space = space , seed = seed ) for _ in range ( n_max_evaluations ): es = opt . generate_evaluation_specification () objectives = { \"loss\" : es . configuration . get ( \"momentum\" , 1.0 ) * es . configuration [ \"lr\" ] ** 2 } if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = - 1.0 * es . configuration [ \"lr\" ] ** 2 opt . report ( es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 } ) ) handles_reporting_evaluations_list ( optimizer_class , optimizer_kwargs , seed = None ) Check if optimizer's report method can process an iterable of evaluations. All optimizers should be able to allow reporting batches of evaluations. It's up to the optimizer's implementation, if evaluations in a batch are processed one by one like if they were reported individually, or if a batch is handled differently. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def handles_reporting_evaluations_list ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if optimizer's report method can process an iterable of evaluations. All optimizers should be able to allow reporting batches of evaluations. It's up to the optimizer's implementation, if evaluations in a batch are processed one by one like if they were reported individually, or if a batch is handled differently. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , seed = seed ) evaluations = [] for i in range ( 3 ): es = opt . generate_evaluation_specification () objectives = { \"loss\" : 0.42 * i } if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = float ( i ) evaluation = es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 * i } ) evaluations . append ( evaluation ) opt . report ( evaluations ) is_deterministic_when_reporting_shuffled_evaluations ( optimizer_class , optimizer_kwargs , seed = None ) Check if determinism isn't affected by the order of initially reported data. Repeatedly initialize the optimizer with the same parameter space and a fixed seed. Report a set of initial evaluations in randomized order as initial data. Start optimizing and check if the generated configurations for all optimizers are equal. By doing multiple evaluations, this tests covers effects that become visible after a while, e.g. only after stages got completed in staged iteration samplers. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def is_deterministic_when_reporting_shuffled_evaluations ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if determinism isn't affected by the order of initially reported data. Repeatedly initialize the optimizer with the same parameter space and a fixed seed. Report a set of initial evaluations in randomized order as initial data. Start optimizing and check if the generated configurations for all optimizers are equal. By doing multiple evaluations, this tests covers effects that become visible after a while, e.g. only after stages got completed in staged iteration samplers. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" if seed is None : seed = 0 space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( 0 , 1 ))) space . seed ( seed ) def _run_experiment_1d ( es ): x = es . configuration [ \"p1\" ] _x = np . copy ( np . atleast_2d ( x )) params = np . array ([ 0.75 , 0.0 , - 10.0 , 0.0 , 0.0 ]) y = np . polyval ( params , _x ) return float ( np . squeeze ( y )) runs : Dict [ int , Dict ] = { 0 : {}, 1 : {}} for run_idx , run in runs . items (): run [ \"evaluations\" ] = [] opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , space = space , seed = seed ) # Report initial data in different order eval_specs = [ opt . generate_evaluation_specification () for _ in range ( 5 )] run [ \"initial_evaluations\" ] = [ es . create_evaluation ( objectives = { \"loss\" : _run_experiment_1d ( es )}, constraints = { \"constraint\" : 10.0 }, ) for es in eval_specs ] if isinstance ( opt , MultiObjectiveOptimizer ): for e in run [ \"initial_evaluations\" ]: e . objectives [ \"score\" ] = - 1.0 * e . objectives [ \"loss\" ] ** 2 shuffle_rng = random . Random ( run_idx ) shuffle_rng . shuffle ( run [ \"initial_evaluations\" ]) opt . report ( run [ \"initial_evaluations\" ]) # Start optimizing for _ in range ( 5 ): es = opt . generate_evaluation_specification () objectives = { \"loss\" : _run_experiment_1d ( es )} if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = - 1.0 * objectives [ \"loss\" ] ** 2 evaluation = es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 } ) opt . report ( evaluation ) run [ \"evaluations\" ] . append ( evaluation ) initial_configs_run_0 = [ e . configuration for e in runs [ 0 ][ \"initial_evaluations\" ]] initial_configs_run_1 = [ e . configuration for e in runs [ 1 ][ \"initial_evaluations\" ]] configs_run_0_as_floats = [ e . configuration [ \"p1\" ] for e in runs [ 0 ][ \"evaluations\" ]] configs_run_1_as_floats = [ e . configuration [ \"p1\" ] for e in runs [ 1 ][ \"evaluations\" ]] assert initial_configs_run_0 != initial_configs_run_1 np . testing . assert_almost_equal ( configs_run_0_as_floats , configs_run_1_as_floats , decimal = 3 ) is_deterministic_with_fixed_seed_and_larger_space ( optimizer_class , optimizer_kwargs , seed = None ) Check if optimizer is deterministic. Initialize the optimizer twice with the same parameter space and a fixed seed. For each optimizer run optimization loop n_evaluations times, namely, get an evaluation specification and report a placeholder result back. The list of configurations should be equal for both optimizers. This tests covers multiple parameter types by using a mixed search space. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def is_deterministic_with_fixed_seed_and_larger_space ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if optimizer is deterministic. Initialize the optimizer twice with the same parameter space and a fixed seed. For each optimizer run optimization loop `n_evaluations` times, namely, get an evaluation specification and report a placeholder result back. The list of configurations should be equal for both optimizers. This tests covers multiple parameter types by using a mixed search space. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" if seed is None : seed = 42 n_evaluations = 5 losses = [ 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] run_0_configs : List [ Evaluation ] = [] run_1_configs : List [ Evaluation ] = [] for run_configs in [ run_0_configs , run_1_configs ]: opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , seed = seed ) for i in range ( n_evaluations ): es = opt . generate_evaluation_specification () objectives = { \"loss\" : losses [ i ]} if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = - 1.0 * losses [ i ] ** 2 evaluation = es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 } ) opt . report ( evaluation ) run_configs . append ( evaluation . configuration ) assert len ( run_0_configs ) == n_evaluations assert run_0_configs == run_1_configs optimize_single_parameter_sequentially_for_n_max_evaluations ( optimizer_class , optimizer_kwargs , seed = None , n_max_evaluations = 20 ) [summary] Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] [description] required optimizer_kwargs dict [description] required n_max_evaluations int [description] 20 seed Optional[int] (optional) custom seed None Returns: Type Description [description] Source code in blackboxopt/optimizers/testing.py def optimize_single_parameter_sequentially_for_n_max_evaluations ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , n_max_evaluations : int = 20 , ): \"\"\"[summary] Args: optimizer_class: [description] optimizer_kwargs: [description] n_max_evaluations: [description] seed: (optional) custom seed Returns: [description] \"\"\" def quadratic_function ( p1 ): return p1 ** 2 assert issubclass ( optimizer_class , Optimizer ), ( \"The default test suite is only applicable for implementations of \" \"blackboxopt.base.Optimizer\" ) optimizer = _initialize_optimizer ( optimizer_class , optimizer_kwargs , seed = seed ) eval_spec = optimizer . generate_evaluation_specification () if issubclass ( optimizer_class , MultiObjectiveOptimizer ): evaluation = eval_spec . create_evaluation ( objectives = { \"loss\" : None , \"score\" : None }, constraints = { \"constraint\" : 10.0 }, ) else : evaluation = eval_spec . create_evaluation ( objectives = { \"loss\" : None }, constraints = { \"constraint\" : 10.0 } ) optimizer . report ( evaluation ) for _ in range ( n_max_evaluations ): try : eval_spec = optimizer . generate_evaluation_specification () except OptimizationComplete : break loss = quadratic_function ( p1 = eval_spec . configuration [ \"p1\" ]) if issubclass ( optimizer_class , MultiObjectiveOptimizer ): evaluation_result = { \"loss\" : loss , \"score\" : - loss } else : evaluation_result = { \"loss\" : loss } evaluation = eval_spec . create_evaluation ( objectives = evaluation_result , constraints = { \"constraint\" : 10.0 } ) optimizer . report ( evaluation ) raises_evaluation_error_when_reporting_unknown_objective ( optimizer_class , optimizer_kwargs , seed = None ) Check if optimizer's report method raises exception in case objective is unknown. Also make sure that the faulty evaluations (and only those) are included in the exception. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def raises_evaluation_error_when_reporting_unknown_objective ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if optimizer's report method raises exception in case objective is unknown. Also make sure that the faulty evaluations (and only those) are included in the exception. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , seed = seed ) es_1 = opt . generate_evaluation_specification () es_2 = opt . generate_evaluation_specification () es_3 = opt . generate_evaluation_specification () # NOTE: The following is not using pytest.raises because this would add pytest as # a regular dependency to blackboxopt. try : evaluation_1 = es_1 . create_evaluation ( objectives = { \"loss\" : 1 }, constraints = { \"constraint\" : 10.0 } ) evaluation_2 = es_2 . create_evaluation ( objectives = { \"unknown_objective\" : 2 }, constraints = { \"constraint\" : 10.0 } ) evaluation_3 = es_3 . create_evaluation ( objectives = { \"loss\" : 4 }, constraints = { \"constraint\" : 10.0 } ) evaluations = [ evaluation_1 , evaluation_2 , evaluation_3 ] if isinstance ( opt , MultiObjectiveOptimizer ): for e in evaluations : e . objectives [ \"score\" ] = 0.0 opt . report ( evaluations ) raise AssertionError ( f \"Optimizer { optimizer_class } did not raise an ObjectivesError when a \" + \"result including an unknown objective name was reported.\" ) except EvaluationsError as exception : invalid_evaluations = [ e for e , _ in exception . evaluations_with_errors ] assert len ( invalid_evaluations ) == 1 assert evaluation_2 in invalid_evaluations respects_fixed_parameter ( optimizer_class , optimizer_kwargs , seed = None ) Check if optimizer's generated evaluation specifications contain the values a parameter in the search space was fixed to. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def respects_fixed_parameter ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if optimizer's generated evaluation specifications contain the values a parameter in the search space was fixed to. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"my_fixed_param\" , ( - 10.0 , 200.0 ))) space . add ( ps . ContinuousParameter ( \"x\" , ( - 2.0 , 2.0 ))) space . seed ( seed ) fixed_value = 1.0 space . fix ( my_fixed_param = fixed_value ) opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , space = space , seed = seed ) for _ in range ( 5 ): es = opt . generate_evaluation_specification () assert es . configuration [ \"my_fixed_param\" ] == fixed_value objectives = { \"loss\" : es . configuration [ \"x\" ] ** 2 } if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = - objectives [ \"loss\" ] opt . report ( es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 }, ) )","title":"Testing"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing","text":"Tests that can be imported and used to test optimizer implementations against this packages blackbox optimizer interface.","title":"testing"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.handles_conditional_space","text":"Check if optimizer handles conditional i.e. hierarchical search spaces. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None n_max_evaluations int Maximum number of evaluation to try 10 Source code in blackboxopt/optimizers/testing.py def handles_conditional_space ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , n_max_evaluations : int = 10 , ): \"\"\"Check if optimizer handles conditional i.e. hierarchical search spaces. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed n_max_evaluations: Maximum number of evaluation to try \"\"\" space = ps . ParameterSpace () space . add ( ps . CategoricalParameter ( \"optimizer\" , ( \"adam\" , \"sgd\" ))) space . add ( ps . ContinuousParameter ( \"lr\" , ( 0.0001 , 0.1 ), transformation = \"log\" )) space . add ( ps . ContinuousParameter ( \"momentum\" , ( 0.0 , 1.0 )), lambda optimizer : optimizer == \"sgd\" , ) space . seed ( seed ) opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , space = space , seed = seed ) for _ in range ( n_max_evaluations ): es = opt . generate_evaluation_specification () objectives = { \"loss\" : es . configuration . get ( \"momentum\" , 1.0 ) * es . configuration [ \"lr\" ] ** 2 } if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = - 1.0 * es . configuration [ \"lr\" ] ** 2 opt . report ( es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 } ) )","title":"handles_conditional_space()"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.handles_reporting_evaluations_list","text":"Check if optimizer's report method can process an iterable of evaluations. All optimizers should be able to allow reporting batches of evaluations. It's up to the optimizer's implementation, if evaluations in a batch are processed one by one like if they were reported individually, or if a batch is handled differently. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def handles_reporting_evaluations_list ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if optimizer's report method can process an iterable of evaluations. All optimizers should be able to allow reporting batches of evaluations. It's up to the optimizer's implementation, if evaluations in a batch are processed one by one like if they were reported individually, or if a batch is handled differently. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , seed = seed ) evaluations = [] for i in range ( 3 ): es = opt . generate_evaluation_specification () objectives = { \"loss\" : 0.42 * i } if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = float ( i ) evaluation = es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 * i } ) evaluations . append ( evaluation ) opt . report ( evaluations )","title":"handles_reporting_evaluations_list()"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.is_deterministic_when_reporting_shuffled_evaluations","text":"Check if determinism isn't affected by the order of initially reported data. Repeatedly initialize the optimizer with the same parameter space and a fixed seed. Report a set of initial evaluations in randomized order as initial data. Start optimizing and check if the generated configurations for all optimizers are equal. By doing multiple evaluations, this tests covers effects that become visible after a while, e.g. only after stages got completed in staged iteration samplers. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def is_deterministic_when_reporting_shuffled_evaluations ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if determinism isn't affected by the order of initially reported data. Repeatedly initialize the optimizer with the same parameter space and a fixed seed. Report a set of initial evaluations in randomized order as initial data. Start optimizing and check if the generated configurations for all optimizers are equal. By doing multiple evaluations, this tests covers effects that become visible after a while, e.g. only after stages got completed in staged iteration samplers. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" if seed is None : seed = 0 space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"p1\" , ( 0 , 1 ))) space . seed ( seed ) def _run_experiment_1d ( es ): x = es . configuration [ \"p1\" ] _x = np . copy ( np . atleast_2d ( x )) params = np . array ([ 0.75 , 0.0 , - 10.0 , 0.0 , 0.0 ]) y = np . polyval ( params , _x ) return float ( np . squeeze ( y )) runs : Dict [ int , Dict ] = { 0 : {}, 1 : {}} for run_idx , run in runs . items (): run [ \"evaluations\" ] = [] opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , space = space , seed = seed ) # Report initial data in different order eval_specs = [ opt . generate_evaluation_specification () for _ in range ( 5 )] run [ \"initial_evaluations\" ] = [ es . create_evaluation ( objectives = { \"loss\" : _run_experiment_1d ( es )}, constraints = { \"constraint\" : 10.0 }, ) for es in eval_specs ] if isinstance ( opt , MultiObjectiveOptimizer ): for e in run [ \"initial_evaluations\" ]: e . objectives [ \"score\" ] = - 1.0 * e . objectives [ \"loss\" ] ** 2 shuffle_rng = random . Random ( run_idx ) shuffle_rng . shuffle ( run [ \"initial_evaluations\" ]) opt . report ( run [ \"initial_evaluations\" ]) # Start optimizing for _ in range ( 5 ): es = opt . generate_evaluation_specification () objectives = { \"loss\" : _run_experiment_1d ( es )} if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = - 1.0 * objectives [ \"loss\" ] ** 2 evaluation = es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 } ) opt . report ( evaluation ) run [ \"evaluations\" ] . append ( evaluation ) initial_configs_run_0 = [ e . configuration for e in runs [ 0 ][ \"initial_evaluations\" ]] initial_configs_run_1 = [ e . configuration for e in runs [ 1 ][ \"initial_evaluations\" ]] configs_run_0_as_floats = [ e . configuration [ \"p1\" ] for e in runs [ 0 ][ \"evaluations\" ]] configs_run_1_as_floats = [ e . configuration [ \"p1\" ] for e in runs [ 1 ][ \"evaluations\" ]] assert initial_configs_run_0 != initial_configs_run_1 np . testing . assert_almost_equal ( configs_run_0_as_floats , configs_run_1_as_floats , decimal = 3 )","title":"is_deterministic_when_reporting_shuffled_evaluations()"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.is_deterministic_with_fixed_seed_and_larger_space","text":"Check if optimizer is deterministic. Initialize the optimizer twice with the same parameter space and a fixed seed. For each optimizer run optimization loop n_evaluations times, namely, get an evaluation specification and report a placeholder result back. The list of configurations should be equal for both optimizers. This tests covers multiple parameter types by using a mixed search space. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def is_deterministic_with_fixed_seed_and_larger_space ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if optimizer is deterministic. Initialize the optimizer twice with the same parameter space and a fixed seed. For each optimizer run optimization loop `n_evaluations` times, namely, get an evaluation specification and report a placeholder result back. The list of configurations should be equal for both optimizers. This tests covers multiple parameter types by using a mixed search space. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" if seed is None : seed = 42 n_evaluations = 5 losses = [ 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] run_0_configs : List [ Evaluation ] = [] run_1_configs : List [ Evaluation ] = [] for run_configs in [ run_0_configs , run_1_configs ]: opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , seed = seed ) for i in range ( n_evaluations ): es = opt . generate_evaluation_specification () objectives = { \"loss\" : losses [ i ]} if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = - 1.0 * losses [ i ] ** 2 evaluation = es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 } ) opt . report ( evaluation ) run_configs . append ( evaluation . configuration ) assert len ( run_0_configs ) == n_evaluations assert run_0_configs == run_1_configs","title":"is_deterministic_with_fixed_seed_and_larger_space()"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.optimize_single_parameter_sequentially_for_n_max_evaluations","text":"[summary] Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] [description] required optimizer_kwargs dict [description] required n_max_evaluations int [description] 20 seed Optional[int] (optional) custom seed None Returns: Type Description [description] Source code in blackboxopt/optimizers/testing.py def optimize_single_parameter_sequentially_for_n_max_evaluations ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , n_max_evaluations : int = 20 , ): \"\"\"[summary] Args: optimizer_class: [description] optimizer_kwargs: [description] n_max_evaluations: [description] seed: (optional) custom seed Returns: [description] \"\"\" def quadratic_function ( p1 ): return p1 ** 2 assert issubclass ( optimizer_class , Optimizer ), ( \"The default test suite is only applicable for implementations of \" \"blackboxopt.base.Optimizer\" ) optimizer = _initialize_optimizer ( optimizer_class , optimizer_kwargs , seed = seed ) eval_spec = optimizer . generate_evaluation_specification () if issubclass ( optimizer_class , MultiObjectiveOptimizer ): evaluation = eval_spec . create_evaluation ( objectives = { \"loss\" : None , \"score\" : None }, constraints = { \"constraint\" : 10.0 }, ) else : evaluation = eval_spec . create_evaluation ( objectives = { \"loss\" : None }, constraints = { \"constraint\" : 10.0 } ) optimizer . report ( evaluation ) for _ in range ( n_max_evaluations ): try : eval_spec = optimizer . generate_evaluation_specification () except OptimizationComplete : break loss = quadratic_function ( p1 = eval_spec . configuration [ \"p1\" ]) if issubclass ( optimizer_class , MultiObjectiveOptimizer ): evaluation_result = { \"loss\" : loss , \"score\" : - loss } else : evaluation_result = { \"loss\" : loss } evaluation = eval_spec . create_evaluation ( objectives = evaluation_result , constraints = { \"constraint\" : 10.0 } ) optimizer . report ( evaluation )","title":"optimize_single_parameter_sequentially_for_n_max_evaluations()"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.raises_evaluation_error_when_reporting_unknown_objective","text":"Check if optimizer's report method raises exception in case objective is unknown. Also make sure that the faulty evaluations (and only those) are included in the exception. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def raises_evaluation_error_when_reporting_unknown_objective ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if optimizer's report method raises exception in case objective is unknown. Also make sure that the faulty evaluations (and only those) are included in the exception. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , seed = seed ) es_1 = opt . generate_evaluation_specification () es_2 = opt . generate_evaluation_specification () es_3 = opt . generate_evaluation_specification () # NOTE: The following is not using pytest.raises because this would add pytest as # a regular dependency to blackboxopt. try : evaluation_1 = es_1 . create_evaluation ( objectives = { \"loss\" : 1 }, constraints = { \"constraint\" : 10.0 } ) evaluation_2 = es_2 . create_evaluation ( objectives = { \"unknown_objective\" : 2 }, constraints = { \"constraint\" : 10.0 } ) evaluation_3 = es_3 . create_evaluation ( objectives = { \"loss\" : 4 }, constraints = { \"constraint\" : 10.0 } ) evaluations = [ evaluation_1 , evaluation_2 , evaluation_3 ] if isinstance ( opt , MultiObjectiveOptimizer ): for e in evaluations : e . objectives [ \"score\" ] = 0.0 opt . report ( evaluations ) raise AssertionError ( f \"Optimizer { optimizer_class } did not raise an ObjectivesError when a \" + \"result including an unknown objective name was reported.\" ) except EvaluationsError as exception : invalid_evaluations = [ e for e , _ in exception . evaluations_with_errors ] assert len ( invalid_evaluations ) == 1 assert evaluation_2 in invalid_evaluations","title":"raises_evaluation_error_when_reporting_unknown_objective()"},{"location":"reference/optimizers/testing/#blackboxopt.optimizers.testing.respects_fixed_parameter","text":"Check if optimizer's generated evaluation specifications contain the values a parameter in the search space was fixed to. Parameters: Name Type Description Default optimizer_class Union[Type[blackboxopt.base.SingleObjectiveOptimizer], Type[blackboxopt.base.MultiObjectiveOptimizer]] Optimizer to test. required optimizer_kwargs dict Expected to contain additional arguments for initializing the optimizer. ( search_space and objective(s) are set automatically by the test.) required seed Optional[int] (optional) custom seed None Source code in blackboxopt/optimizers/testing.py def respects_fixed_parameter ( optimizer_class : Union [ Type [ SingleObjectiveOptimizer ], Type [ MultiObjectiveOptimizer ] ], optimizer_kwargs : dict , seed : Optional [ int ] = None , ): \"\"\"Check if optimizer's generated evaluation specifications contain the values a parameter in the search space was fixed to. Args: optimizer_class: Optimizer to test. optimizer_kwargs: Expected to contain additional arguments for initializing the optimizer. (`search_space` and `objective(s)` are set automatically by the test.) seed: (optional) custom seed \"\"\" space = ps . ParameterSpace () space . add ( ps . ContinuousParameter ( \"my_fixed_param\" , ( - 10.0 , 200.0 ))) space . add ( ps . ContinuousParameter ( \"x\" , ( - 2.0 , 2.0 ))) space . seed ( seed ) fixed_value = 1.0 space . fix ( my_fixed_param = fixed_value ) opt = _initialize_optimizer ( optimizer_class , optimizer_kwargs , space = space , seed = seed ) for _ in range ( 5 ): es = opt . generate_evaluation_specification () assert es . configuration [ \"my_fixed_param\" ] == fixed_value objectives = { \"loss\" : es . configuration [ \"x\" ] ** 2 } if isinstance ( opt , MultiObjectiveOptimizer ): objectives [ \"score\" ] = - objectives [ \"loss\" ] opt . report ( es . create_evaluation ( objectives = objectives , constraints = { \"constraint\" : 10.0 }, ) )","title":"respects_fixed_parameter()"},{"location":"reference/optimizers/staged/bohb/","text":"blackboxopt.optimizers.staged.bohb Sampler ( StagedIterationConfigurationSampler ) Source code in blackboxopt/optimizers/staged/bohb.py class Sampler ( StagedIterationConfigurationSampler ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , min_samples_in_model : int , top_n_percent : int , num_samples : int , random_fraction : float , bandwidth_factor : float , min_bandwidth : float , seed : int = None , logger = None , ): \"\"\"Fits for each given fidelity a kernel density estimator on the best N percent of the evaluated configurations on this fidelity. Args: search_space: ConfigurationSpace/ ParameterSpace object. objective: The objective of the optimization. min_samples_in_model: Minimum number of datapoints needed to fit a model. top_n_percent: Determines the percentile of configurations that will be used as training data for the kernel density estimator of the good configuration, e.g if set to 10 the best 10% configurations will be considered for training. num_samples: Number of samples drawn to optimize EI via sampling. random_fraction: Fraction of random configurations returned bandwidth_factor: Widens the bandwidth for contiuous parameters for proposed points to optimize EI min_bandwidth: To keep diversity, even when all (good) samples have the same value for one of the parameters, a minimum bandwidth (reasonable default: 1e-3) is used instead of zero. seed: A seed to make the sampler reproducible. logger: [description] Raises: RuntimeError: [description] \"\"\" self . logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger self . objective = objective self . min_samples_in_model = min_samples_in_model self . top_n_percent = top_n_percent self . search_space = search_space self . bw_factor = bandwidth_factor self . min_bandwidth = min_bandwidth self . seed = seed self . _rng = np . random . default_rng ( self . seed ) if self . min_samples_in_model < len ( search_space ) + 1 : self . min_samples_in_model = len ( search_space ) + 1 self . logger . warning ( \"Invalid min_samples_in_model value. \" + f \"Setting it to { self . min_samples_in_model } \" ) self . num_samples = num_samples self . random_fraction = random_fraction self . kde_vartypes = \"\" vartypes : List [ Union [ float , int ]] = [] for hp in search_space : # type: ignore hp = hp [ \"parameter\" ] if isinstance ( hp , ( ps . ContinuousParameter , ps . IntegerParameter )): self . kde_vartypes += \"c\" vartypes . append ( 0 ) elif isinstance ( hp , ps . CategoricalParameter ): self . kde_vartypes += \"u\" vartypes . append ( hp . num_values ) elif isinstance ( hp , ps . OrdinalParameter ): self . kde_vartypes += \"o\" vartypes . append ( - hp . num_values ) else : raise RuntimeError ( f \"This version on BOHB does not support { type ( hp ) } !\" ) self . vartypes = np . array ( vartypes , dtype = int ) self . configs : Dict [ float , List [ np . ndarray ]] = dict () self . losses : Dict [ float , List [ float ]] = dict () self . kde_models : Dict [ float , dict ] = dict () def sample_configuration ( self ) -> Tuple [ dict , dict ]: \"\"\"[summary] Returns: [description] \"\"\" self . logger . debug ( \"start sampling a new configuration.\" ) # Sample from prior, if no model is available or with given probability if len ( self . kde_models ) == 0 or self . _rng . random () < self . random_fraction : return self . search_space . sample (), { \"model_based_pick\" : False } best = np . inf best_vector = None try : # sample from largest fidelity fidelity = max ( self . kde_models . keys ()) good = self . kde_models [ fidelity ][ \"good\" ] . pdf bad = self . kde_models [ fidelity ][ \"bad\" ] . pdf def minimize_me ( x ): return max ( 1e-32 , bad ( x )) / max ( good ( x ), 1e-32 ) kde_good = self . kde_models [ fidelity ][ \"good\" ] kde_bad = self . kde_models [ fidelity ][ \"bad\" ] for i in range ( self . num_samples ): idx = self . _rng . integers ( 0 , len ( kde_good . data )) datum = kde_good . data [ idx ] vector = sample_around_values ( datum , kde_good . bw , self . vartypes , self . min_bandwidth , self . bw_factor , rng = self . _rng , ) if vector is None : continue # Statsmodels KDE estimators relies on seeding through numpy's global # state. We do this close to the evaluation of the PDF (`good`, `bad`) # to increase robustness for multi threading. # As we seed in a loop, we need to change it each iteration to not get # the same random numbers each time. # We also reset the np.random's global state, in case the user relies # on it in other parts of the code and to not hide other determinism # issues. # TODO: Check github issue if there was progress and the seeding can be # removed: https://github.com/statsmodels/statsmodels/issues/306 cached_rng_state = None if self . seed : cached_rng_state = np . random . get_state () np . random . seed ( self . seed + i ) val = minimize_me ( vector ) if cached_rng_state : np . random . set_state ( cached_rng_state ) if not np . isfinite ( val ): self . logger . warning ( \"sampled vector: %s has EI value %s \" % ( vector , val ) ) self . logger . warning ( \"data in the KDEs: \\n %s \\n %s \" % ( kde_good . data , kde_bad . data ) ) self . logger . warning ( \"bandwidth of the KDEs: \\n %s \\n %s \" % ( kde_good . bw , kde_bad . bw ) ) # right now, this happens because a KDE does not contain all values # for a categorical parameter this cannot be fixed with the # statsmodels KDE, so for now, we are just going to evaluate this # one if the good_kde has a finite value, i.e. there is no config # with that value in the bad kde, so it shouldn't be terrible. if np . isfinite ( good ( vector )) and best_vector is not None : best_vector = vector continue if val < best : best = val best_vector = convert_from_statsmodels_kde_representation ( vector , self . vartypes ) if best_vector is None : self . logger . debug ( f \"Sampling based optimization with { self . num_samples } samples did \" + \"not find any finite/numerical acquisition function value \" + \"-> using random configuration\" ) return self . search_space . sample (), { \"model_based_pick\" : False } else : self . logger . debug ( \"best_vector: {} , {} , {} , {} \" . format ( best_vector , best , good ( best_vector ), bad ( best_vector ) ) ) return ( self . search_space . from_numerical ( best_vector ), { \"model_based_pick\" : True }, ) except Exception : self . logger . debug ( \"Sample base optimization failed. Falling back to a random sample.\" ) return self . search_space . sample (), { \"model_based_pick\" : False } def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"[summary] Args: evaluation: [description] \"\"\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is None : loss = np . inf else : loss = ( - objective_value if self . objective . greater_is_better else objective_value ) config_vector = self . search_space . to_numerical ( evaluation . configuration ) config_vector = convert_to_statsmodels_kde_representation ( config_vector , self . vartypes ) fidelity = evaluation . settings [ \"fidelity\" ] if fidelity not in self . configs . keys (): self . configs [ fidelity ] = [] self . losses [ fidelity ] = [] self . configs [ fidelity ] . append ( config_vector ) self . losses [ fidelity ] . append ( loss ) if bool ( self . kde_models . keys ()) and max ( self . kde_models . keys ()) > fidelity : return if np . isfinite ( self . losses [ fidelity ]) . sum () <= self . min_samples_in_model - 1 : n_runs_finite_loss = np . isfinite ( self . losses [ fidelity ]) . sum () self . logger . debug ( f \"Only { n_runs_finite_loss } run(s) with a finite loss for fidelity \" + f \" { fidelity } available, need more than { self . min_samples_in_model + 1 } \" + \"-> can't build model!\" ) return train_configs = np . array ( self . configs [ fidelity ]) train_losses = np . array ( self . losses [ fidelity ]) n_good = max ( self . min_samples_in_model , ( self . top_n_percent * train_configs . shape [ 0 ]) // 100 , ) n_bad = max ( self . min_samples_in_model , (( 100 - self . top_n_percent ) * train_configs . shape [ 0 ]) // 100 , ) # Refit KDE for the current fidelity idx = np . argsort ( train_losses ) train_data_good = impute_conditional_data ( train_configs [ idx [: n_good ]], self . vartypes , rng = self . _rng ) train_data_bad = impute_conditional_data ( train_configs [ idx [ n_good : n_good + n_bad ]], self . vartypes , rng = self . _rng ) if train_data_good . shape [ 0 ] <= train_data_good . shape [ 1 ]: return if train_data_bad . shape [ 0 ] <= train_data_bad . shape [ 1 ]: return # more expensive crossvalidation method # bw_estimation = 'cv_ls' # quick rule of thumb bw_estimation = \"normal_reference\" bad_kde = sm . nonparametric . KDEMultivariate ( data = train_data_bad , var_type = self . kde_vartypes , bw = bw_estimation , ) good_kde = sm . nonparametric . KDEMultivariate ( data = train_data_good , var_type = self . kde_vartypes , bw = bw_estimation , ) bad_kde . bw = np . clip ( bad_kde . bw , self . min_bandwidth , None ) good_kde . bw = np . clip ( good_kde . bw , self . min_bandwidth , None ) self . kde_models [ fidelity ] = { \"good\" : good_kde , \"bad\" : bad_kde } # update probs for the categorical parameters for later sampling self . logger . debug ( f \"done building a new model for fidelity { fidelity } based on \" + f \" { n_good } / { n_bad } split \\n Best loss for this fidelity: \" + f \" { np . min ( train_losses ) } \\n \" + ( \"=\" * 40 ) ) digest_evaluation ( self , evaluation ) [summary] Parameters: Name Type Description Default evaluation Evaluation [description] required Source code in blackboxopt/optimizers/staged/bohb.py def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"[summary] Args: evaluation: [description] \"\"\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is None : loss = np . inf else : loss = ( - objective_value if self . objective . greater_is_better else objective_value ) config_vector = self . search_space . to_numerical ( evaluation . configuration ) config_vector = convert_to_statsmodels_kde_representation ( config_vector , self . vartypes ) fidelity = evaluation . settings [ \"fidelity\" ] if fidelity not in self . configs . keys (): self . configs [ fidelity ] = [] self . losses [ fidelity ] = [] self . configs [ fidelity ] . append ( config_vector ) self . losses [ fidelity ] . append ( loss ) if bool ( self . kde_models . keys ()) and max ( self . kde_models . keys ()) > fidelity : return if np . isfinite ( self . losses [ fidelity ]) . sum () <= self . min_samples_in_model - 1 : n_runs_finite_loss = np . isfinite ( self . losses [ fidelity ]) . sum () self . logger . debug ( f \"Only { n_runs_finite_loss } run(s) with a finite loss for fidelity \" + f \" { fidelity } available, need more than { self . min_samples_in_model + 1 } \" + \"-> can't build model!\" ) return train_configs = np . array ( self . configs [ fidelity ]) train_losses = np . array ( self . losses [ fidelity ]) n_good = max ( self . min_samples_in_model , ( self . top_n_percent * train_configs . shape [ 0 ]) // 100 , ) n_bad = max ( self . min_samples_in_model , (( 100 - self . top_n_percent ) * train_configs . shape [ 0 ]) // 100 , ) # Refit KDE for the current fidelity idx = np . argsort ( train_losses ) train_data_good = impute_conditional_data ( train_configs [ idx [: n_good ]], self . vartypes , rng = self . _rng ) train_data_bad = impute_conditional_data ( train_configs [ idx [ n_good : n_good + n_bad ]], self . vartypes , rng = self . _rng ) if train_data_good . shape [ 0 ] <= train_data_good . shape [ 1 ]: return if train_data_bad . shape [ 0 ] <= train_data_bad . shape [ 1 ]: return # more expensive crossvalidation method # bw_estimation = 'cv_ls' # quick rule of thumb bw_estimation = \"normal_reference\" bad_kde = sm . nonparametric . KDEMultivariate ( data = train_data_bad , var_type = self . kde_vartypes , bw = bw_estimation , ) good_kde = sm . nonparametric . KDEMultivariate ( data = train_data_good , var_type = self . kde_vartypes , bw = bw_estimation , ) bad_kde . bw = np . clip ( bad_kde . bw , self . min_bandwidth , None ) good_kde . bw = np . clip ( good_kde . bw , self . min_bandwidth , None ) self . kde_models [ fidelity ] = { \"good\" : good_kde , \"bad\" : bad_kde } # update probs for the categorical parameters for later sampling self . logger . debug ( f \"done building a new model for fidelity { fidelity } based on \" + f \" { n_good } / { n_bad } split \\n Best loss for this fidelity: \" + f \" { np . min ( train_losses ) } \\n \" + ( \"=\" * 40 ) ) sample_configuration ( self ) [summary] Returns: Type Description Tuple[dict, dict] [description] Source code in blackboxopt/optimizers/staged/bohb.py def sample_configuration ( self ) -> Tuple [ dict , dict ]: \"\"\"[summary] Returns: [description] \"\"\" self . logger . debug ( \"start sampling a new configuration.\" ) # Sample from prior, if no model is available or with given probability if len ( self . kde_models ) == 0 or self . _rng . random () < self . random_fraction : return self . search_space . sample (), { \"model_based_pick\" : False } best = np . inf best_vector = None try : # sample from largest fidelity fidelity = max ( self . kde_models . keys ()) good = self . kde_models [ fidelity ][ \"good\" ] . pdf bad = self . kde_models [ fidelity ][ \"bad\" ] . pdf def minimize_me ( x ): return max ( 1e-32 , bad ( x )) / max ( good ( x ), 1e-32 ) kde_good = self . kde_models [ fidelity ][ \"good\" ] kde_bad = self . kde_models [ fidelity ][ \"bad\" ] for i in range ( self . num_samples ): idx = self . _rng . integers ( 0 , len ( kde_good . data )) datum = kde_good . data [ idx ] vector = sample_around_values ( datum , kde_good . bw , self . vartypes , self . min_bandwidth , self . bw_factor , rng = self . _rng , ) if vector is None : continue # Statsmodels KDE estimators relies on seeding through numpy's global # state. We do this close to the evaluation of the PDF (`good`, `bad`) # to increase robustness for multi threading. # As we seed in a loop, we need to change it each iteration to not get # the same random numbers each time. # We also reset the np.random's global state, in case the user relies # on it in other parts of the code and to not hide other determinism # issues. # TODO: Check github issue if there was progress and the seeding can be # removed: https://github.com/statsmodels/statsmodels/issues/306 cached_rng_state = None if self . seed : cached_rng_state = np . random . get_state () np . random . seed ( self . seed + i ) val = minimize_me ( vector ) if cached_rng_state : np . random . set_state ( cached_rng_state ) if not np . isfinite ( val ): self . logger . warning ( \"sampled vector: %s has EI value %s \" % ( vector , val ) ) self . logger . warning ( \"data in the KDEs: \\n %s \\n %s \" % ( kde_good . data , kde_bad . data ) ) self . logger . warning ( \"bandwidth of the KDEs: \\n %s \\n %s \" % ( kde_good . bw , kde_bad . bw ) ) # right now, this happens because a KDE does not contain all values # for a categorical parameter this cannot be fixed with the # statsmodels KDE, so for now, we are just going to evaluate this # one if the good_kde has a finite value, i.e. there is no config # with that value in the bad kde, so it shouldn't be terrible. if np . isfinite ( good ( vector )) and best_vector is not None : best_vector = vector continue if val < best : best = val best_vector = convert_from_statsmodels_kde_representation ( vector , self . vartypes ) if best_vector is None : self . logger . debug ( f \"Sampling based optimization with { self . num_samples } samples did \" + \"not find any finite/numerical acquisition function value \" + \"-> using random configuration\" ) return self . search_space . sample (), { \"model_based_pick\" : False } else : self . logger . debug ( \"best_vector: {} , {} , {} , {} \" . format ( best_vector , best , good ( best_vector ), bad ( best_vector ) ) ) return ( self . search_space . from_numerical ( best_vector ), { \"model_based_pick\" : True }, ) except Exception : self . logger . debug ( \"Sample base optimization failed. Falling back to a random sample.\" ) return self . search_space . sample (), { \"model_based_pick\" : False } convert_from_statsmodels_kde_representation ( array , vartypes ) Convert numerical representation for categoricals and ordinals back into the unit hypercube. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations following the statsmodels convention for categorical and ordinal values being integers. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation consistent with a numerical representation in the hypercube. Source code in blackboxopt/optimizers/staged/bohb.py def convert_from_statsmodels_kde_representation ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Convert numerical representation for categoricals and ordinals back into the unit hypercube. Args: array: Numerical representation of the configurations following the statsmodels convention for categorical and ordinal values being integers. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation consistent with a numerical representation in the hypercube. \"\"\" processed_vector = np . copy ( array ) for i in range ( len ( processed_vector )): if vartypes [ i ] != 0 : num_values = abs ( vartypes [ i ]) processed_vector [ i ] = ( processed_vector [ i ] + 0.5 ) / num_values return processed_vector convert_to_statsmodels_kde_representation ( array , vartypes ) Convert numerical representation for categoricals and ordinals to integers. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations with categorical and ordinal values mapped into the unit hypercube. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation consistent with the statsmodels package. Source code in blackboxopt/optimizers/staged/bohb.py def convert_to_statsmodels_kde_representation ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Convert numerical representation for categoricals and ordinals to integers. Args: array: Numerical representation of the configurations with categorical and ordinal values mapped into the unit hypercube. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation consistent with the statsmodels package. \"\"\" processed_vector = np . copy ( array ) for i in range ( len ( processed_vector )): if vartypes [ i ] == 0 : continue num_values = abs ( vartypes [ i ]) processed_vector [ i ] = np . around (( processed_vector [ i ] * num_values ) - 0.5 ) return processed_vector impute_conditional_data ( array , vartypes , rng = None ) Impute NaNs in numerical representation with observed values or prior samples. This method is needed to use the statsmodels KDE, which doesn't handle missing values out of the box. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations which can include NaN values for inactive variables. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required rng Optional[numpy.random._generator.Generator] A random number generator to make the imputation reproducible. None Returns: Type Description ndarray Numerical representation where all NaNs have been replaced with observed values or prior samples. Source code in blackboxopt/optimizers/staged/bohb.py def impute_conditional_data ( array : np . ndarray , vartypes : Union [ list , np . ndarray ], rng : Optional [ np . random . Generator ] = None , ) -> np . ndarray : \"\"\"Impute NaNs in numerical representation with observed values or prior samples. This method is needed to use the `statsmodels` KDE, which doesn't handle missing values out of the box. Args: array: Numerical representation of the configurations which can include NaN values for inactive variables. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. rng: A random number generator to make the imputation reproducible. Returns: Numerical representation where all NaNs have been replaced with observed values or prior samples. \"\"\" rng = np . random . default_rng ( rng ) return_array = np . empty_like ( array ) for i in range ( array . shape [ 0 ]): datum = np . copy ( array [ i ]) nan_indices = np . argwhere ( np . isnan ( datum )) . flatten () while np . any ( nan_indices ): nan_idx = nan_indices [ 0 ] valid_indices = np . argwhere ( np . isfinite ( array [:, nan_idx ])) . flatten () if len ( valid_indices ) > 0 : # pick one of them at random and overwrite all NaN values row_idx = rng . choice ( valid_indices ) datum [ nan_indices ] = array [ row_idx , nan_indices ] else : # no point in the data has this value activated, so fill it with a valid # but random value t = vartypes [ nan_idx ] if t == 0 : datum [ nan_idx ] = rng . random () elif t > 0 : datum [ nan_idx ] = rng . integers ( t ) elif t < 0 : datum [ nan_idx ] = rng . integers ( - t ) nan_indices = np . argwhere ( np . isnan ( datum )) . flatten () return_array [ i , :] = datum return return_array sample_around_values ( datum , bandwidths , vartypes , min_bandwidth , bw_factor , rng = None ) Sample numerical representation close to a given datum. This is specific to the KDE in statsmodels and their kernel for the different variable types. Parameters: Name Type Description Default datum ndarray Numerical representation of a configuration that is used as the 'center' for sampling. required bandwidths ndarray Bandwidth of the corresponding kernels in each dimension. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values. required min_bandwidth float Smallest allowed bandwidth. Ensures diversity even if all samples agree on a value in a dimension. required bw_factor float To increase diversity, the bandwidth is actually multiplied by this factor before sampling. required rng Optional[numpy.random._generator.Generator] A random number generator to make the sampling reproducible. None Returns: Type Description Optional[numpy.ndarray] Numerical representation of a configuration close to the provided datum. Source code in blackboxopt/optimizers/staged/bohb.py def sample_around_values ( datum : np . ndarray , bandwidths : np . ndarray , vartypes : Union [ list , np . ndarray ], min_bandwidth : float , bw_factor : float , rng : Optional [ np . random . Generator ] = None , ) -> Optional [ np . ndarray ]: \"\"\"Sample numerical representation close to a given datum. This is specific to the KDE in statsmodels and their kernel for the different variable types. Args: datum: Numerical representation of a configuration that is used as the 'center' for sampling. bandwidths: Bandwidth of the corresponding kernels in each dimension. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values. min_bandwidth: Smallest allowed bandwidth. Ensures diversity even if all samples agree on a value in a dimension. bw_factor: To increase diversity, the bandwidth is actually multiplied by this factor before sampling. rng: A random number generator to make the sampling reproducible. Returns: Numerical representation of a configuration close to the provided datum. \"\"\" rng = np . random . default_rng ( rng ) vector = [] for m , bw , t in zip ( datum , bandwidths , vartypes ): bw = max ( bw , min_bandwidth ) if t == 0 : bw = bw_factor * bw try : v = sps . truncnorm . rvs ( - m / bw , ( 1 - m ) / bw , loc = m , scale = bw , random_state = rng ) except Exception : return None elif t > 0 : v = m if rng . random () < ( 1 - bw ) else rng . integers ( t ) else : bw = min ( 0.9999 , bw ) # bandwidth has to be less the one for this kernel! diffs = np . abs ( np . arange ( - t ) - m ) probs = 0.5 * ( 1 - bw ) * ( bw ** diffs ) idx = diffs == 0 probs [ idx ] = ( idx * ( 1 - bw ))[ idx ] probs /= probs . sum () v = rng . choice ( - t , p = probs ) vector . append ( v ) return np . array ( vector )","title":"Bohb"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb","text":"","title":"bohb"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler","text":"Source code in blackboxopt/optimizers/staged/bohb.py class Sampler ( StagedIterationConfigurationSampler ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , min_samples_in_model : int , top_n_percent : int , num_samples : int , random_fraction : float , bandwidth_factor : float , min_bandwidth : float , seed : int = None , logger = None , ): \"\"\"Fits for each given fidelity a kernel density estimator on the best N percent of the evaluated configurations on this fidelity. Args: search_space: ConfigurationSpace/ ParameterSpace object. objective: The objective of the optimization. min_samples_in_model: Minimum number of datapoints needed to fit a model. top_n_percent: Determines the percentile of configurations that will be used as training data for the kernel density estimator of the good configuration, e.g if set to 10 the best 10% configurations will be considered for training. num_samples: Number of samples drawn to optimize EI via sampling. random_fraction: Fraction of random configurations returned bandwidth_factor: Widens the bandwidth for contiuous parameters for proposed points to optimize EI min_bandwidth: To keep diversity, even when all (good) samples have the same value for one of the parameters, a minimum bandwidth (reasonable default: 1e-3) is used instead of zero. seed: A seed to make the sampler reproducible. logger: [description] Raises: RuntimeError: [description] \"\"\" self . logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger self . objective = objective self . min_samples_in_model = min_samples_in_model self . top_n_percent = top_n_percent self . search_space = search_space self . bw_factor = bandwidth_factor self . min_bandwidth = min_bandwidth self . seed = seed self . _rng = np . random . default_rng ( self . seed ) if self . min_samples_in_model < len ( search_space ) + 1 : self . min_samples_in_model = len ( search_space ) + 1 self . logger . warning ( \"Invalid min_samples_in_model value. \" + f \"Setting it to { self . min_samples_in_model } \" ) self . num_samples = num_samples self . random_fraction = random_fraction self . kde_vartypes = \"\" vartypes : List [ Union [ float , int ]] = [] for hp in search_space : # type: ignore hp = hp [ \"parameter\" ] if isinstance ( hp , ( ps . ContinuousParameter , ps . IntegerParameter )): self . kde_vartypes += \"c\" vartypes . append ( 0 ) elif isinstance ( hp , ps . CategoricalParameter ): self . kde_vartypes += \"u\" vartypes . append ( hp . num_values ) elif isinstance ( hp , ps . OrdinalParameter ): self . kde_vartypes += \"o\" vartypes . append ( - hp . num_values ) else : raise RuntimeError ( f \"This version on BOHB does not support { type ( hp ) } !\" ) self . vartypes = np . array ( vartypes , dtype = int ) self . configs : Dict [ float , List [ np . ndarray ]] = dict () self . losses : Dict [ float , List [ float ]] = dict () self . kde_models : Dict [ float , dict ] = dict () def sample_configuration ( self ) -> Tuple [ dict , dict ]: \"\"\"[summary] Returns: [description] \"\"\" self . logger . debug ( \"start sampling a new configuration.\" ) # Sample from prior, if no model is available or with given probability if len ( self . kde_models ) == 0 or self . _rng . random () < self . random_fraction : return self . search_space . sample (), { \"model_based_pick\" : False } best = np . inf best_vector = None try : # sample from largest fidelity fidelity = max ( self . kde_models . keys ()) good = self . kde_models [ fidelity ][ \"good\" ] . pdf bad = self . kde_models [ fidelity ][ \"bad\" ] . pdf def minimize_me ( x ): return max ( 1e-32 , bad ( x )) / max ( good ( x ), 1e-32 ) kde_good = self . kde_models [ fidelity ][ \"good\" ] kde_bad = self . kde_models [ fidelity ][ \"bad\" ] for i in range ( self . num_samples ): idx = self . _rng . integers ( 0 , len ( kde_good . data )) datum = kde_good . data [ idx ] vector = sample_around_values ( datum , kde_good . bw , self . vartypes , self . min_bandwidth , self . bw_factor , rng = self . _rng , ) if vector is None : continue # Statsmodels KDE estimators relies on seeding through numpy's global # state. We do this close to the evaluation of the PDF (`good`, `bad`) # to increase robustness for multi threading. # As we seed in a loop, we need to change it each iteration to not get # the same random numbers each time. # We also reset the np.random's global state, in case the user relies # on it in other parts of the code and to not hide other determinism # issues. # TODO: Check github issue if there was progress and the seeding can be # removed: https://github.com/statsmodels/statsmodels/issues/306 cached_rng_state = None if self . seed : cached_rng_state = np . random . get_state () np . random . seed ( self . seed + i ) val = minimize_me ( vector ) if cached_rng_state : np . random . set_state ( cached_rng_state ) if not np . isfinite ( val ): self . logger . warning ( \"sampled vector: %s has EI value %s \" % ( vector , val ) ) self . logger . warning ( \"data in the KDEs: \\n %s \\n %s \" % ( kde_good . data , kde_bad . data ) ) self . logger . warning ( \"bandwidth of the KDEs: \\n %s \\n %s \" % ( kde_good . bw , kde_bad . bw ) ) # right now, this happens because a KDE does not contain all values # for a categorical parameter this cannot be fixed with the # statsmodels KDE, so for now, we are just going to evaluate this # one if the good_kde has a finite value, i.e. there is no config # with that value in the bad kde, so it shouldn't be terrible. if np . isfinite ( good ( vector )) and best_vector is not None : best_vector = vector continue if val < best : best = val best_vector = convert_from_statsmodels_kde_representation ( vector , self . vartypes ) if best_vector is None : self . logger . debug ( f \"Sampling based optimization with { self . num_samples } samples did \" + \"not find any finite/numerical acquisition function value \" + \"-> using random configuration\" ) return self . search_space . sample (), { \"model_based_pick\" : False } else : self . logger . debug ( \"best_vector: {} , {} , {} , {} \" . format ( best_vector , best , good ( best_vector ), bad ( best_vector ) ) ) return ( self . search_space . from_numerical ( best_vector ), { \"model_based_pick\" : True }, ) except Exception : self . logger . debug ( \"Sample base optimization failed. Falling back to a random sample.\" ) return self . search_space . sample (), { \"model_based_pick\" : False } def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"[summary] Args: evaluation: [description] \"\"\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is None : loss = np . inf else : loss = ( - objective_value if self . objective . greater_is_better else objective_value ) config_vector = self . search_space . to_numerical ( evaluation . configuration ) config_vector = convert_to_statsmodels_kde_representation ( config_vector , self . vartypes ) fidelity = evaluation . settings [ \"fidelity\" ] if fidelity not in self . configs . keys (): self . configs [ fidelity ] = [] self . losses [ fidelity ] = [] self . configs [ fidelity ] . append ( config_vector ) self . losses [ fidelity ] . append ( loss ) if bool ( self . kde_models . keys ()) and max ( self . kde_models . keys ()) > fidelity : return if np . isfinite ( self . losses [ fidelity ]) . sum () <= self . min_samples_in_model - 1 : n_runs_finite_loss = np . isfinite ( self . losses [ fidelity ]) . sum () self . logger . debug ( f \"Only { n_runs_finite_loss } run(s) with a finite loss for fidelity \" + f \" { fidelity } available, need more than { self . min_samples_in_model + 1 } \" + \"-> can't build model!\" ) return train_configs = np . array ( self . configs [ fidelity ]) train_losses = np . array ( self . losses [ fidelity ]) n_good = max ( self . min_samples_in_model , ( self . top_n_percent * train_configs . shape [ 0 ]) // 100 , ) n_bad = max ( self . min_samples_in_model , (( 100 - self . top_n_percent ) * train_configs . shape [ 0 ]) // 100 , ) # Refit KDE for the current fidelity idx = np . argsort ( train_losses ) train_data_good = impute_conditional_data ( train_configs [ idx [: n_good ]], self . vartypes , rng = self . _rng ) train_data_bad = impute_conditional_data ( train_configs [ idx [ n_good : n_good + n_bad ]], self . vartypes , rng = self . _rng ) if train_data_good . shape [ 0 ] <= train_data_good . shape [ 1 ]: return if train_data_bad . shape [ 0 ] <= train_data_bad . shape [ 1 ]: return # more expensive crossvalidation method # bw_estimation = 'cv_ls' # quick rule of thumb bw_estimation = \"normal_reference\" bad_kde = sm . nonparametric . KDEMultivariate ( data = train_data_bad , var_type = self . kde_vartypes , bw = bw_estimation , ) good_kde = sm . nonparametric . KDEMultivariate ( data = train_data_good , var_type = self . kde_vartypes , bw = bw_estimation , ) bad_kde . bw = np . clip ( bad_kde . bw , self . min_bandwidth , None ) good_kde . bw = np . clip ( good_kde . bw , self . min_bandwidth , None ) self . kde_models [ fidelity ] = { \"good\" : good_kde , \"bad\" : bad_kde } # update probs for the categorical parameters for later sampling self . logger . debug ( f \"done building a new model for fidelity { fidelity } based on \" + f \" { n_good } / { n_bad } split \\n Best loss for this fidelity: \" + f \" { np . min ( train_losses ) } \\n \" + ( \"=\" * 40 ) )","title":"Sampler"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler.digest_evaluation","text":"[summary] Parameters: Name Type Description Default evaluation Evaluation [description] required Source code in blackboxopt/optimizers/staged/bohb.py def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"[summary] Args: evaluation: [description] \"\"\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is None : loss = np . inf else : loss = ( - objective_value if self . objective . greater_is_better else objective_value ) config_vector = self . search_space . to_numerical ( evaluation . configuration ) config_vector = convert_to_statsmodels_kde_representation ( config_vector , self . vartypes ) fidelity = evaluation . settings [ \"fidelity\" ] if fidelity not in self . configs . keys (): self . configs [ fidelity ] = [] self . losses [ fidelity ] = [] self . configs [ fidelity ] . append ( config_vector ) self . losses [ fidelity ] . append ( loss ) if bool ( self . kde_models . keys ()) and max ( self . kde_models . keys ()) > fidelity : return if np . isfinite ( self . losses [ fidelity ]) . sum () <= self . min_samples_in_model - 1 : n_runs_finite_loss = np . isfinite ( self . losses [ fidelity ]) . sum () self . logger . debug ( f \"Only { n_runs_finite_loss } run(s) with a finite loss for fidelity \" + f \" { fidelity } available, need more than { self . min_samples_in_model + 1 } \" + \"-> can't build model!\" ) return train_configs = np . array ( self . configs [ fidelity ]) train_losses = np . array ( self . losses [ fidelity ]) n_good = max ( self . min_samples_in_model , ( self . top_n_percent * train_configs . shape [ 0 ]) // 100 , ) n_bad = max ( self . min_samples_in_model , (( 100 - self . top_n_percent ) * train_configs . shape [ 0 ]) // 100 , ) # Refit KDE for the current fidelity idx = np . argsort ( train_losses ) train_data_good = impute_conditional_data ( train_configs [ idx [: n_good ]], self . vartypes , rng = self . _rng ) train_data_bad = impute_conditional_data ( train_configs [ idx [ n_good : n_good + n_bad ]], self . vartypes , rng = self . _rng ) if train_data_good . shape [ 0 ] <= train_data_good . shape [ 1 ]: return if train_data_bad . shape [ 0 ] <= train_data_bad . shape [ 1 ]: return # more expensive crossvalidation method # bw_estimation = 'cv_ls' # quick rule of thumb bw_estimation = \"normal_reference\" bad_kde = sm . nonparametric . KDEMultivariate ( data = train_data_bad , var_type = self . kde_vartypes , bw = bw_estimation , ) good_kde = sm . nonparametric . KDEMultivariate ( data = train_data_good , var_type = self . kde_vartypes , bw = bw_estimation , ) bad_kde . bw = np . clip ( bad_kde . bw , self . min_bandwidth , None ) good_kde . bw = np . clip ( good_kde . bw , self . min_bandwidth , None ) self . kde_models [ fidelity ] = { \"good\" : good_kde , \"bad\" : bad_kde } # update probs for the categorical parameters for later sampling self . logger . debug ( f \"done building a new model for fidelity { fidelity } based on \" + f \" { n_good } / { n_bad } split \\n Best loss for this fidelity: \" + f \" { np . min ( train_losses ) } \\n \" + ( \"=\" * 40 ) )","title":"digest_evaluation()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.Sampler.sample_configuration","text":"[summary] Returns: Type Description Tuple[dict, dict] [description] Source code in blackboxopt/optimizers/staged/bohb.py def sample_configuration ( self ) -> Tuple [ dict , dict ]: \"\"\"[summary] Returns: [description] \"\"\" self . logger . debug ( \"start sampling a new configuration.\" ) # Sample from prior, if no model is available or with given probability if len ( self . kde_models ) == 0 or self . _rng . random () < self . random_fraction : return self . search_space . sample (), { \"model_based_pick\" : False } best = np . inf best_vector = None try : # sample from largest fidelity fidelity = max ( self . kde_models . keys ()) good = self . kde_models [ fidelity ][ \"good\" ] . pdf bad = self . kde_models [ fidelity ][ \"bad\" ] . pdf def minimize_me ( x ): return max ( 1e-32 , bad ( x )) / max ( good ( x ), 1e-32 ) kde_good = self . kde_models [ fidelity ][ \"good\" ] kde_bad = self . kde_models [ fidelity ][ \"bad\" ] for i in range ( self . num_samples ): idx = self . _rng . integers ( 0 , len ( kde_good . data )) datum = kde_good . data [ idx ] vector = sample_around_values ( datum , kde_good . bw , self . vartypes , self . min_bandwidth , self . bw_factor , rng = self . _rng , ) if vector is None : continue # Statsmodels KDE estimators relies on seeding through numpy's global # state. We do this close to the evaluation of the PDF (`good`, `bad`) # to increase robustness for multi threading. # As we seed in a loop, we need to change it each iteration to not get # the same random numbers each time. # We also reset the np.random's global state, in case the user relies # on it in other parts of the code and to not hide other determinism # issues. # TODO: Check github issue if there was progress and the seeding can be # removed: https://github.com/statsmodels/statsmodels/issues/306 cached_rng_state = None if self . seed : cached_rng_state = np . random . get_state () np . random . seed ( self . seed + i ) val = minimize_me ( vector ) if cached_rng_state : np . random . set_state ( cached_rng_state ) if not np . isfinite ( val ): self . logger . warning ( \"sampled vector: %s has EI value %s \" % ( vector , val ) ) self . logger . warning ( \"data in the KDEs: \\n %s \\n %s \" % ( kde_good . data , kde_bad . data ) ) self . logger . warning ( \"bandwidth of the KDEs: \\n %s \\n %s \" % ( kde_good . bw , kde_bad . bw ) ) # right now, this happens because a KDE does not contain all values # for a categorical parameter this cannot be fixed with the # statsmodels KDE, so for now, we are just going to evaluate this # one if the good_kde has a finite value, i.e. there is no config # with that value in the bad kde, so it shouldn't be terrible. if np . isfinite ( good ( vector )) and best_vector is not None : best_vector = vector continue if val < best : best = val best_vector = convert_from_statsmodels_kde_representation ( vector , self . vartypes ) if best_vector is None : self . logger . debug ( f \"Sampling based optimization with { self . num_samples } samples did \" + \"not find any finite/numerical acquisition function value \" + \"-> using random configuration\" ) return self . search_space . sample (), { \"model_based_pick\" : False } else : self . logger . debug ( \"best_vector: {} , {} , {} , {} \" . format ( best_vector , best , good ( best_vector ), bad ( best_vector ) ) ) return ( self . search_space . from_numerical ( best_vector ), { \"model_based_pick\" : True }, ) except Exception : self . logger . debug ( \"Sample base optimization failed. Falling back to a random sample.\" ) return self . search_space . sample (), { \"model_based_pick\" : False }","title":"sample_configuration()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.convert_from_statsmodels_kde_representation","text":"Convert numerical representation for categoricals and ordinals back into the unit hypercube. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations following the statsmodels convention for categorical and ordinal values being integers. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation consistent with a numerical representation in the hypercube. Source code in blackboxopt/optimizers/staged/bohb.py def convert_from_statsmodels_kde_representation ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Convert numerical representation for categoricals and ordinals back into the unit hypercube. Args: array: Numerical representation of the configurations following the statsmodels convention for categorical and ordinal values being integers. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation consistent with a numerical representation in the hypercube. \"\"\" processed_vector = np . copy ( array ) for i in range ( len ( processed_vector )): if vartypes [ i ] != 0 : num_values = abs ( vartypes [ i ]) processed_vector [ i ] = ( processed_vector [ i ] + 0.5 ) / num_values return processed_vector","title":"convert_from_statsmodels_kde_representation()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.convert_to_statsmodels_kde_representation","text":"Convert numerical representation for categoricals and ordinals to integers. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations with categorical and ordinal values mapped into the unit hypercube. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required Returns: Type Description ndarray Numerical representation consistent with the statsmodels package. Source code in blackboxopt/optimizers/staged/bohb.py def convert_to_statsmodels_kde_representation ( array : np . ndarray , vartypes : Union [ list , np . ndarray ] ) -> np . ndarray : \"\"\"Convert numerical representation for categoricals and ordinals to integers. Args: array: Numerical representation of the configurations with categorical and ordinal values mapped into the unit hypercube. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. Returns: Numerical representation consistent with the statsmodels package. \"\"\" processed_vector = np . copy ( array ) for i in range ( len ( processed_vector )): if vartypes [ i ] == 0 : continue num_values = abs ( vartypes [ i ]) processed_vector [ i ] = np . around (( processed_vector [ i ] * num_values ) - 0.5 ) return processed_vector","title":"convert_to_statsmodels_kde_representation()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.impute_conditional_data","text":"Impute NaNs in numerical representation with observed values or prior samples. This method is needed to use the statsmodels KDE, which doesn't handle missing values out of the box. Parameters: Name Type Description Default array ndarray Numerical representation of the configurations which can include NaN values for inactive variables. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. required rng Optional[numpy.random._generator.Generator] A random number generator to make the imputation reproducible. None Returns: Type Description ndarray Numerical representation where all NaNs have been replaced with observed values or prior samples. Source code in blackboxopt/optimizers/staged/bohb.py def impute_conditional_data ( array : np . ndarray , vartypes : Union [ list , np . ndarray ], rng : Optional [ np . random . Generator ] = None , ) -> np . ndarray : \"\"\"Impute NaNs in numerical representation with observed values or prior samples. This method is needed to use the `statsmodels` KDE, which doesn't handle missing values out of the box. Args: array: Numerical representation of the configurations which can include NaN values for inactive variables. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values, and <0 means ordinal with as many values. rng: A random number generator to make the imputation reproducible. Returns: Numerical representation where all NaNs have been replaced with observed values or prior samples. \"\"\" rng = np . random . default_rng ( rng ) return_array = np . empty_like ( array ) for i in range ( array . shape [ 0 ]): datum = np . copy ( array [ i ]) nan_indices = np . argwhere ( np . isnan ( datum )) . flatten () while np . any ( nan_indices ): nan_idx = nan_indices [ 0 ] valid_indices = np . argwhere ( np . isfinite ( array [:, nan_idx ])) . flatten () if len ( valid_indices ) > 0 : # pick one of them at random and overwrite all NaN values row_idx = rng . choice ( valid_indices ) datum [ nan_indices ] = array [ row_idx , nan_indices ] else : # no point in the data has this value activated, so fill it with a valid # but random value t = vartypes [ nan_idx ] if t == 0 : datum [ nan_idx ] = rng . random () elif t > 0 : datum [ nan_idx ] = rng . integers ( t ) elif t < 0 : datum [ nan_idx ] = rng . integers ( - t ) nan_indices = np . argwhere ( np . isnan ( datum )) . flatten () return_array [ i , :] = datum return return_array","title":"impute_conditional_data()"},{"location":"reference/optimizers/staged/bohb/#blackboxopt.optimizers.staged.bohb.sample_around_values","text":"Sample numerical representation close to a given datum. This is specific to the KDE in statsmodels and their kernel for the different variable types. Parameters: Name Type Description Default datum ndarray Numerical representation of a configuration that is used as the 'center' for sampling. required bandwidths ndarray Bandwidth of the corresponding kernels in each dimension. required vartypes Union[list, numpy.ndarray] Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values. required min_bandwidth float Smallest allowed bandwidth. Ensures diversity even if all samples agree on a value in a dimension. required bw_factor float To increase diversity, the bandwidth is actually multiplied by this factor before sampling. required rng Optional[numpy.random._generator.Generator] A random number generator to make the sampling reproducible. None Returns: Type Description Optional[numpy.ndarray] Numerical representation of a configuration close to the provided datum. Source code in blackboxopt/optimizers/staged/bohb.py def sample_around_values ( datum : np . ndarray , bandwidths : np . ndarray , vartypes : Union [ list , np . ndarray ], min_bandwidth : float , bw_factor : float , rng : Optional [ np . random . Generator ] = None , ) -> Optional [ np . ndarray ]: \"\"\"Sample numerical representation close to a given datum. This is specific to the KDE in statsmodels and their kernel for the different variable types. Args: datum: Numerical representation of a configuration that is used as the 'center' for sampling. bandwidths: Bandwidth of the corresponding kernels in each dimension. vartypes: Encoding of the types of the variables: 0 mean continuous, >0 means categorical with as many different values. min_bandwidth: Smallest allowed bandwidth. Ensures diversity even if all samples agree on a value in a dimension. bw_factor: To increase diversity, the bandwidth is actually multiplied by this factor before sampling. rng: A random number generator to make the sampling reproducible. Returns: Numerical representation of a configuration close to the provided datum. \"\"\" rng = np . random . default_rng ( rng ) vector = [] for m , bw , t in zip ( datum , bandwidths , vartypes ): bw = max ( bw , min_bandwidth ) if t == 0 : bw = bw_factor * bw try : v = sps . truncnorm . rvs ( - m / bw , ( 1 - m ) / bw , loc = m , scale = bw , random_state = rng ) except Exception : return None elif t > 0 : v = m if rng . random () < ( 1 - bw ) else rng . integers ( t ) else : bw = min ( 0.9999 , bw ) # bandwidth has to be less the one for this kernel! diffs = np . abs ( np . arange ( - t ) - m ) probs = 0.5 * ( 1 - bw ) * ( bw ** diffs ) idx = diffs == 0 probs [ idx ] = ( idx * ( 1 - bw ))[ idx ] probs /= probs . sum () v = rng . choice ( - t , p = probs ) vector . append ( v ) return np . array ( vector )","title":"sample_around_values()"},{"location":"reference/optimizers/staged/configuration_sampler/","text":"blackboxopt.optimizers.staged.configuration_sampler RandomSearchSampler ( StagedIterationConfigurationSampler ) Source code in blackboxopt/optimizers/staged/configuration_sampler.py class RandomSearchSampler ( StagedIterationConfigurationSampler ): def __init__ ( self , search_space : ParameterSpace ): self . search_space = search_space def sample_configuration ( self ) -> Tuple [ dict , dict ]: return self . search_space . sample (), {} def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"Random Search is stateless and does nothing with finished evaluations.\"\"\" digest_evaluation ( self , evaluation ) Random Search is stateless and does nothing with finished evaluations. Source code in blackboxopt/optimizers/staged/configuration_sampler.py def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"Random Search is stateless and does nothing with finished evaluations.\"\"\" sample_configuration ( self ) Pick the next configuration. Returns: Type Description Tuple[dict, dict] The configuration to be evaluated, Additional information that will be added to the optimizer_info dict. Source code in blackboxopt/optimizers/staged/configuration_sampler.py def sample_configuration ( self ) -> Tuple [ dict , dict ]: return self . search_space . sample (), {}","title":"Configuration sampler"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler","text":"","title":"configuration_sampler"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.RandomSearchSampler","text":"Source code in blackboxopt/optimizers/staged/configuration_sampler.py class RandomSearchSampler ( StagedIterationConfigurationSampler ): def __init__ ( self , search_space : ParameterSpace ): self . search_space = search_space def sample_configuration ( self ) -> Tuple [ dict , dict ]: return self . search_space . sample (), {} def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"Random Search is stateless and does nothing with finished evaluations.\"\"\"","title":"RandomSearchSampler"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.RandomSearchSampler.digest_evaluation","text":"Random Search is stateless and does nothing with finished evaluations. Source code in blackboxopt/optimizers/staged/configuration_sampler.py def digest_evaluation ( self , evaluation : Evaluation ): \"\"\"Random Search is stateless and does nothing with finished evaluations.\"\"\"","title":"digest_evaluation()"},{"location":"reference/optimizers/staged/configuration_sampler/#blackboxopt.optimizers.staged.configuration_sampler.RandomSearchSampler.sample_configuration","text":"Pick the next configuration. Returns: Type Description Tuple[dict, dict] The configuration to be evaluated, Additional information that will be added to the optimizer_info dict. Source code in blackboxopt/optimizers/staged/configuration_sampler.py def sample_configuration ( self ) -> Tuple [ dict , dict ]: return self . search_space . sample (), {}","title":"sample_configuration()"},{"location":"reference/optimizers/staged/hyperband/","text":"blackboxopt.optimizers.staged.hyperband create_hyperband_iteration ( iteration_index , min_fidelity , max_fidelity , eta , config_sampler , objective , logger ) Optimizer specific way to create a new blackboxopt.optimizer.staged.iteration.StagedIteration object Source code in blackboxopt/optimizers/staged/hyperband.py def create_hyperband_iteration ( iteration_index : int , min_fidelity : float , max_fidelity : float , eta : float , config_sampler : StagedIterationConfigurationSampler , objective : Objective , logger : logging . Logger , ) -> StagedIteration : \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.staged.iteration.StagedIteration` object \"\"\" # 's_max + 1' in the paper max_num_stages = 1 + int ( math . floor ( math . log ( max_fidelity / min_fidelity , eta ))) # 's+1' in the paper num_stages = max_num_stages - ( iteration_index % ( max_num_stages )) num_configs_first_stage = int ( math . ceil (( max_num_stages / num_stages ) * eta ** ( num_stages - 1 )) ) num_configs_per_stage = [ int ( num_configs_first_stage // ( eta ** i )) for i in range ( num_stages ) ] fidelities_per_stage = [ max_fidelity / eta ** i for i in range ( num_stages - 1 , - 1 , - 1 ) ] # Hyperband simple draws random configurations, and there is no additional # information that needs to be stored return StagedIteration ( iteration_index , num_configs_per_stage , fidelities_per_stage , config_sampler , greedy_promotion , objective , logger = logger , )","title":"Hyperband"},{"location":"reference/optimizers/staged/hyperband/#blackboxopt.optimizers.staged.hyperband","text":"","title":"hyperband"},{"location":"reference/optimizers/staged/hyperband/#blackboxopt.optimizers.staged.hyperband.create_hyperband_iteration","text":"Optimizer specific way to create a new blackboxopt.optimizer.staged.iteration.StagedIteration object Source code in blackboxopt/optimizers/staged/hyperband.py def create_hyperband_iteration ( iteration_index : int , min_fidelity : float , max_fidelity : float , eta : float , config_sampler : StagedIterationConfigurationSampler , objective : Objective , logger : logging . Logger , ) -> StagedIteration : \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.staged.iteration.StagedIteration` object \"\"\" # 's_max + 1' in the paper max_num_stages = 1 + int ( math . floor ( math . log ( max_fidelity / min_fidelity , eta ))) # 's+1' in the paper num_stages = max_num_stages - ( iteration_index % ( max_num_stages )) num_configs_first_stage = int ( math . ceil (( max_num_stages / num_stages ) * eta ** ( num_stages - 1 )) ) num_configs_per_stage = [ int ( num_configs_first_stage // ( eta ** i )) for i in range ( num_stages ) ] fidelities_per_stage = [ max_fidelity / eta ** i for i in range ( num_stages - 1 , - 1 , - 1 ) ] # Hyperband simple draws random configurations, and there is no additional # information that needs to be stored return StagedIteration ( iteration_index , num_configs_per_stage , fidelities_per_stage , config_sampler , greedy_promotion , objective , logger = logger , )","title":"create_hyperband_iteration()"},{"location":"reference/optimizers/staged/iteration/","text":"blackboxopt.optimizers.staged.iteration Datum dataclass Small container for bookkeeping only. Source code in blackboxopt/optimizers/staged/iteration.py class Datum : \"\"\"Small container for bookkeeping only.\"\"\" config_key : Tuple [ int , int , int ] status : str loss : float = float ( \"NaN\" ) StagedIteration Source code in blackboxopt/optimizers/staged/iteration.py class StagedIteration : def __init__ ( self , iteration : int , num_configs : List [ int ], fidelities : List [ float ], config_sampler : StagedIterationConfigurationSampler , config_promotion_function : Callable , objective : Objective , logger : logging . Logger = None , ): \"\"\"Base class for iterations that compare configurations at different fidelities and race them as in SuccessiveHalving or Hyperband. Args: iteration: Index of this iteration. num_configs: Number of configurations in each stage. fidelities: The fidelity for each stage. Must have the same length as `num_configs'. config_sampler: Configuration Sampler object that suggests a new configuration for evaluation given a fidelity. config_promotion_function: Function that decides which configurations are promoted. Check `blackboxopt.optimizers.utils.staged_iteration.greedy_promotion` for the signature. objective: The objective of the optimization. logger: A standard logger to which some debug output might be written. \"\"\" assert len ( fidelities ) == len ( num_configs ), \"Please specify the number of configuration and the fidelities.\" self . logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger self . iteration = iteration self . fidelities = fidelities self . num_configs = num_configs self . config_sampler = config_sampler self . config_promotion_function = config_promotion_function self . objective = objective self . current_stage = 0 self . evaluation_data : List [ List [ Datum ]] = [[]] self . eval_specs : Dict [ Tuple [ int , int , int ], EvaluationSpecification ] = {} self . pending_evaluations : Dict [ UUID , int ] = {} self . finished = False def generate_evaluation_specification ( self ) -> Optional [ EvaluationSpecification ]: \"\"\"Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: [description] \"\"\" if self . finished : return None # try to find a queued entry first for i , d in enumerate ( self . evaluation_data [ self . current_stage ]): if d . status == \"QUEUED\" : es = copy . deepcopy ( self . eval_specs [ d . config_key ]) es . settings [ \"fidelity\" ] = self . fidelities [ self . current_stage ] d . status = \"RUNNING\" self . pending_evaluations [ es . optimizer_info [ \"id\" ]] = i return es # sample a new configuration if there are empty slots to be filled if ( len ( self . evaluation_data [ self . current_stage ]) < self . num_configs [ self . current_stage ] ): conf_key = ( self . iteration , self . current_stage , len ( self . evaluation_data [ self . current_stage ]), ) conf , opt_info = self . config_sampler . sample_configuration () opt_info . update ({ \"configuration_key\" : conf_key , \"id\" : str ( uuid4 ())}) self . eval_specs [ conf_key ] = EvaluationSpecification ( configuration = conf , settings = {}, optimizer_info = opt_info ) self . evaluation_data [ self . current_stage ] . append ( Datum ( conf_key , \"QUEUED\" )) # To understand recursion, you first must understand recursion :) return self . generate_evaluation_specification () # at this point there are pending evaluations and this iteration has to wait return None def digest_evaluation ( self , evaluation_specificiation_id : UUID , evaluation : Evaluation ): \"\"\"Registers the result of an evaluation. Args: evaluation_specificiation_id: [description] evaluation: [description] \"\"\" self . config_sampler . digest_evaluation ( evaluation ) i = self . pending_evaluations . pop ( evaluation_specificiation_id ) d = self . evaluation_data [ self . current_stage ][ i ] d . status = \"FINISHED\" if not evaluation . all_objectives_none else \"CRASHED\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is not None : d . loss = ( - objective_value if self . objective . greater_is_better else objective_value ) # quick check if all configurations have finished yet if len ( self . evaluation_data [ self . current_stage ]) == self . num_configs [ self . current_stage ] and all ( [ e . status in [ \"FINISHED\" , \"CRASHED\" ] for e in self . evaluation_data [ self . current_stage ] ] ): self . _progress_to_next_stage () def _progress_to_next_stage ( self ): \"\"\"Implements logic to promote configurations to the next stage.\"\"\" # filter out crashed configurations data = [ d for d in self . evaluation_data [ self . current_stage ] if np . isfinite ( d . loss ) ] self . current_stage += 1 if self . current_stage == len ( self . num_configs ): self . finished = True return config_keys = self . config_promotion_function ( data , self . num_configs [ self . current_stage ] ) self . logger . debug ( \"Iteration %i : Advancing configurations %s to stage %i .\" , self . iteration , str ( config_keys ), self . current_stage , ) self . evaluation_data . append ( [ Datum ( config_key , \"QUEUED\" ) for config_key in config_keys ] ) digest_evaluation ( self , evaluation_specificiation_id , evaluation ) Registers the result of an evaluation. Parameters: Name Type Description Default evaluation_specificiation_id UUID [description] required evaluation Evaluation [description] required Source code in blackboxopt/optimizers/staged/iteration.py def digest_evaluation ( self , evaluation_specificiation_id : UUID , evaluation : Evaluation ): \"\"\"Registers the result of an evaluation. Args: evaluation_specificiation_id: [description] evaluation: [description] \"\"\" self . config_sampler . digest_evaluation ( evaluation ) i = self . pending_evaluations . pop ( evaluation_specificiation_id ) d = self . evaluation_data [ self . current_stage ][ i ] d . status = \"FINISHED\" if not evaluation . all_objectives_none else \"CRASHED\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is not None : d . loss = ( - objective_value if self . objective . greater_is_better else objective_value ) # quick check if all configurations have finished yet if len ( self . evaluation_data [ self . current_stage ]) == self . num_configs [ self . current_stage ] and all ( [ e . status in [ \"FINISHED\" , \"CRASHED\" ] for e in self . evaluation_data [ self . current_stage ] ] ): self . _progress_to_next_stage () generate_evaluation_specification ( self ) Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: Type Description Optional[blackboxopt.evaluation.EvaluationSpecification] [description] Source code in blackboxopt/optimizers/staged/iteration.py def generate_evaluation_specification ( self ) -> Optional [ EvaluationSpecification ]: \"\"\"Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: [description] \"\"\" if self . finished : return None # try to find a queued entry first for i , d in enumerate ( self . evaluation_data [ self . current_stage ]): if d . status == \"QUEUED\" : es = copy . deepcopy ( self . eval_specs [ d . config_key ]) es . settings [ \"fidelity\" ] = self . fidelities [ self . current_stage ] d . status = \"RUNNING\" self . pending_evaluations [ es . optimizer_info [ \"id\" ]] = i return es # sample a new configuration if there are empty slots to be filled if ( len ( self . evaluation_data [ self . current_stage ]) < self . num_configs [ self . current_stage ] ): conf_key = ( self . iteration , self . current_stage , len ( self . evaluation_data [ self . current_stage ]), ) conf , opt_info = self . config_sampler . sample_configuration () opt_info . update ({ \"configuration_key\" : conf_key , \"id\" : str ( uuid4 ())}) self . eval_specs [ conf_key ] = EvaluationSpecification ( configuration = conf , settings = {}, optimizer_info = opt_info ) self . evaluation_data [ self . current_stage ] . append ( Datum ( conf_key , \"QUEUED\" )) # To understand recursion, you first must understand recursion :) return self . generate_evaluation_specification () # at this point there are pending evaluations and this iteration has to wait return None","title":"Iteration"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration","text":"","title":"iteration"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.Datum","text":"Small container for bookkeeping only. Source code in blackboxopt/optimizers/staged/iteration.py class Datum : \"\"\"Small container for bookkeeping only.\"\"\" config_key : Tuple [ int , int , int ] status : str loss : float = float ( \"NaN\" )","title":"Datum"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration","text":"Source code in blackboxopt/optimizers/staged/iteration.py class StagedIteration : def __init__ ( self , iteration : int , num_configs : List [ int ], fidelities : List [ float ], config_sampler : StagedIterationConfigurationSampler , config_promotion_function : Callable , objective : Objective , logger : logging . Logger = None , ): \"\"\"Base class for iterations that compare configurations at different fidelities and race them as in SuccessiveHalving or Hyperband. Args: iteration: Index of this iteration. num_configs: Number of configurations in each stage. fidelities: The fidelity for each stage. Must have the same length as `num_configs'. config_sampler: Configuration Sampler object that suggests a new configuration for evaluation given a fidelity. config_promotion_function: Function that decides which configurations are promoted. Check `blackboxopt.optimizers.utils.staged_iteration.greedy_promotion` for the signature. objective: The objective of the optimization. logger: A standard logger to which some debug output might be written. \"\"\" assert len ( fidelities ) == len ( num_configs ), \"Please specify the number of configuration and the fidelities.\" self . logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger self . iteration = iteration self . fidelities = fidelities self . num_configs = num_configs self . config_sampler = config_sampler self . config_promotion_function = config_promotion_function self . objective = objective self . current_stage = 0 self . evaluation_data : List [ List [ Datum ]] = [[]] self . eval_specs : Dict [ Tuple [ int , int , int ], EvaluationSpecification ] = {} self . pending_evaluations : Dict [ UUID , int ] = {} self . finished = False def generate_evaluation_specification ( self ) -> Optional [ EvaluationSpecification ]: \"\"\"Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: [description] \"\"\" if self . finished : return None # try to find a queued entry first for i , d in enumerate ( self . evaluation_data [ self . current_stage ]): if d . status == \"QUEUED\" : es = copy . deepcopy ( self . eval_specs [ d . config_key ]) es . settings [ \"fidelity\" ] = self . fidelities [ self . current_stage ] d . status = \"RUNNING\" self . pending_evaluations [ es . optimizer_info [ \"id\" ]] = i return es # sample a new configuration if there are empty slots to be filled if ( len ( self . evaluation_data [ self . current_stage ]) < self . num_configs [ self . current_stage ] ): conf_key = ( self . iteration , self . current_stage , len ( self . evaluation_data [ self . current_stage ]), ) conf , opt_info = self . config_sampler . sample_configuration () opt_info . update ({ \"configuration_key\" : conf_key , \"id\" : str ( uuid4 ())}) self . eval_specs [ conf_key ] = EvaluationSpecification ( configuration = conf , settings = {}, optimizer_info = opt_info ) self . evaluation_data [ self . current_stage ] . append ( Datum ( conf_key , \"QUEUED\" )) # To understand recursion, you first must understand recursion :) return self . generate_evaluation_specification () # at this point there are pending evaluations and this iteration has to wait return None def digest_evaluation ( self , evaluation_specificiation_id : UUID , evaluation : Evaluation ): \"\"\"Registers the result of an evaluation. Args: evaluation_specificiation_id: [description] evaluation: [description] \"\"\" self . config_sampler . digest_evaluation ( evaluation ) i = self . pending_evaluations . pop ( evaluation_specificiation_id ) d = self . evaluation_data [ self . current_stage ][ i ] d . status = \"FINISHED\" if not evaluation . all_objectives_none else \"CRASHED\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is not None : d . loss = ( - objective_value if self . objective . greater_is_better else objective_value ) # quick check if all configurations have finished yet if len ( self . evaluation_data [ self . current_stage ]) == self . num_configs [ self . current_stage ] and all ( [ e . status in [ \"FINISHED\" , \"CRASHED\" ] for e in self . evaluation_data [ self . current_stage ] ] ): self . _progress_to_next_stage () def _progress_to_next_stage ( self ): \"\"\"Implements logic to promote configurations to the next stage.\"\"\" # filter out crashed configurations data = [ d for d in self . evaluation_data [ self . current_stage ] if np . isfinite ( d . loss ) ] self . current_stage += 1 if self . current_stage == len ( self . num_configs ): self . finished = True return config_keys = self . config_promotion_function ( data , self . num_configs [ self . current_stage ] ) self . logger . debug ( \"Iteration %i : Advancing configurations %s to stage %i .\" , self . iteration , str ( config_keys ), self . current_stage , ) self . evaluation_data . append ( [ Datum ( config_key , \"QUEUED\" ) for config_key in config_keys ] )","title":"StagedIteration"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration.digest_evaluation","text":"Registers the result of an evaluation. Parameters: Name Type Description Default evaluation_specificiation_id UUID [description] required evaluation Evaluation [description] required Source code in blackboxopt/optimizers/staged/iteration.py def digest_evaluation ( self , evaluation_specificiation_id : UUID , evaluation : Evaluation ): \"\"\"Registers the result of an evaluation. Args: evaluation_specificiation_id: [description] evaluation: [description] \"\"\" self . config_sampler . digest_evaluation ( evaluation ) i = self . pending_evaluations . pop ( evaluation_specificiation_id ) d = self . evaluation_data [ self . current_stage ][ i ] d . status = \"FINISHED\" if not evaluation . all_objectives_none else \"CRASHED\" objective_value = evaluation . objectives [ self . objective . name ] if objective_value is not None : d . loss = ( - objective_value if self . objective . greater_is_better else objective_value ) # quick check if all configurations have finished yet if len ( self . evaluation_data [ self . current_stage ]) == self . num_configs [ self . current_stage ] and all ( [ e . status in [ \"FINISHED\" , \"CRASHED\" ] for e in self . evaluation_data [ self . current_stage ] ] ): self . _progress_to_next_stage ()","title":"digest_evaluation()"},{"location":"reference/optimizers/staged/iteration/#blackboxopt.optimizers.staged.iteration.StagedIteration.generate_evaluation_specification","text":"Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: Type Description Optional[blackboxopt.evaluation.EvaluationSpecification] [description] Source code in blackboxopt/optimizers/staged/iteration.py def generate_evaluation_specification ( self ) -> Optional [ EvaluationSpecification ]: \"\"\"Pick the next evaluation specification with a budget i.e. fidelity to run. Returns: [description] \"\"\" if self . finished : return None # try to find a queued entry first for i , d in enumerate ( self . evaluation_data [ self . current_stage ]): if d . status == \"QUEUED\" : es = copy . deepcopy ( self . eval_specs [ d . config_key ]) es . settings [ \"fidelity\" ] = self . fidelities [ self . current_stage ] d . status = \"RUNNING\" self . pending_evaluations [ es . optimizer_info [ \"id\" ]] = i return es # sample a new configuration if there are empty slots to be filled if ( len ( self . evaluation_data [ self . current_stage ]) < self . num_configs [ self . current_stage ] ): conf_key = ( self . iteration , self . current_stage , len ( self . evaluation_data [ self . current_stage ]), ) conf , opt_info = self . config_sampler . sample_configuration () opt_info . update ({ \"configuration_key\" : conf_key , \"id\" : str ( uuid4 ())}) self . eval_specs [ conf_key ] = EvaluationSpecification ( configuration = conf , settings = {}, optimizer_info = opt_info ) self . evaluation_data [ self . current_stage ] . append ( Datum ( conf_key , \"QUEUED\" )) # To understand recursion, you first must understand recursion :) return self . generate_evaluation_specification () # at this point there are pending evaluations and this iteration has to wait return None","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/staged/optimizer/","text":"blackboxopt.optimizers.staged.optimizer StagedIterationOptimizer ( SingleObjectiveOptimizer ) Source code in blackboxopt/optimizers/staged/optimizer.py class StagedIterationOptimizer ( SingleObjectiveOptimizer ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , num_iterations : int , seed : int = None , logger : logging . Logger = None , ): \"\"\"Base class for optimizers using iterations that compare configurations at different fidelities and race them in stages, like Hyperband or BOHB. Args: search_space: [description] objective: [description] num_iterations: The number of iterations that the optimizer will run. seed: [description] logger: [description] \"\"\" super () . __init__ ( search_space = search_space , objective = objective , seed = seed ) self . logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger self . num_iterations = num_iterations self . iterations : List [ StagedIteration ] = [] self . evaluation_uuid_to_iteration : Dict [ str , int ] = {} self . pending_configurations : Dict [ str , EvaluationSpecification ] = {} def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , ) def _report ( self , evaluation : Evaluation ) -> None : evaluation_specification_id = evaluation . optimizer_info . get ( \"id\" ) self . pending_configurations . pop ( str ( evaluation_specification_id )) idx = self . evaluation_uuid_to_iteration . pop ( str ( evaluation_specification_id )) self . iterations [ idx ] . digest_evaluation ( evaluation_specification_id , evaluation ) def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady @abc . abstractmethod def _create_new_iteration ( self , iteration_index ): \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.utils.staged_iteration.StagedIteration` object \"\"\" generate_evaluation_specification ( self ) Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/staged/optimizer.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady report ( self , evaluations ) Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/staged/optimizer.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"Optimizer"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer","text":"","title":"optimizer"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer","text":"Source code in blackboxopt/optimizers/staged/optimizer.py class StagedIterationOptimizer ( SingleObjectiveOptimizer ): def __init__ ( self , search_space : ParameterSpace , objective : Objective , num_iterations : int , seed : int = None , logger : logging . Logger = None , ): \"\"\"Base class for optimizers using iterations that compare configurations at different fidelities and race them in stages, like Hyperband or BOHB. Args: search_space: [description] objective: [description] num_iterations: The number of iterations that the optimizer will run. seed: [description] logger: [description] \"\"\" super () . __init__ ( search_space = search_space , objective = objective , seed = seed ) self . logger = logging . getLogger ( \"blackboxopt\" ) if logger is None else logger self . num_iterations = num_iterations self . iterations : List [ StagedIteration ] = [] self . evaluation_uuid_to_iteration : Dict [ str , int ] = {} self . pending_configurations : Dict [ str , EvaluationSpecification ] = {} def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , ) def _report ( self , evaluation : Evaluation ) -> None : evaluation_specification_id = evaluation . optimizer_info . get ( \"id\" ) self . pending_configurations . pop ( str ( evaluation_specification_id )) idx = self . evaluation_uuid_to_iteration . pop ( str ( evaluation_specification_id )) self . iterations [ idx ] . digest_evaluation ( evaluation_specification_id , evaluation ) def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady @abc . abstractmethod def _create_new_iteration ( self , iteration_index ): \"\"\"Optimizer specific way to create a new `blackboxopt.optimizer.utils.staged_iteration.StagedIteration` object \"\"\"","title":"StagedIterationOptimizer"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer.generate_evaluation_specification","text":"Get next configuration and settings to evaluate. Exceptions: Type Description OptimizationComplete When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady When the optimizer is not ready to propose a new evaluation specification. Source code in blackboxopt/optimizers/staged/optimizer.py def generate_evaluation_specification ( self ) -> EvaluationSpecification : \"\"\"Get next configuration and settings to evaluate. Raises: OptimizationComplete: When the optimization run is finished, e.g. when the budget has been exhausted. OptimizerNotReady: When the optimizer is not ready to propose a new evaluation specification. \"\"\" # check if any of the already active iterations returns a configuration and # simply return that for idx , iteration in enumerate ( self . iterations ): es = iteration . generate_evaluation_specification () if es is not None : self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = idx self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # if that didn't work, check if there another iteration can be started and then # ask it for a configuration if len ( self . iterations ) < self . num_iterations : self . iterations . append ( self . _create_new_iteration ( len ( self . iterations ))) es = self . iterations [ - 1 ] . generate_evaluation_specification () self . evaluation_uuid_to_iteration [ str ( es . optimizer_info [ \"id\" ])] = ( len ( self . iterations ) - 1 ) self . pending_configurations [ str ( es . optimizer_info [ \"id\" ])] = es return es # check if the optimization is already complete or whether the optimizer is # waiting for evaluation results -> raise corresponding error if all ([ iteration . finished for iteration in self . iterations ]): raise OptimizationComplete raise OptimizerNotReady","title":"generate_evaluation_specification()"},{"location":"reference/optimizers/staged/optimizer/#blackboxopt.optimizers.staged.optimizer.StagedIterationOptimizer.report","text":"Report one or multiple evaluations to the optimizer. All valid evaluations are processed. Faulty evaluations are not processed, instead an EvaluationsError is raised, which includes the problematic evaluations with their respective Exceptions in the evaluations_with_errors attribute. Parameters: Name Type Description Default evaluations Union[blackboxopt.evaluation.Evaluation, Iterable[blackboxopt.evaluation.Evaluation]] A single evaluated evaluation specifications, or an iterable of many. required Exceptions: Type Description EvaluationsError Raised when an evaluation could not be processed. Source code in blackboxopt/optimizers/staged/optimizer.py def report ( self , evaluations : Union [ Evaluation , Iterable [ Evaluation ]]) -> None : _evals = [ evaluations ] if isinstance ( evaluations , Evaluation ) else evaluations call_functions_with_evaluations_and_collect_errors ( [ super () . report , _validate_optimizer_info_id , self . _report ], _evals , )","title":"report()"},{"location":"reference/optimizers/staged/utils/","text":"blackboxopt.optimizers.staged.utils best_evaluation_at_highest_fidelity ( evaluations , objective ) From given list of evaluations, get the best in terms of minimal loss at the highest fidelity. Parameters: Name Type Description Default evaluations List[blackboxopt.evaluation.Evaluation] [description] required objective Objective [description] required Returns: Type Description Optional[blackboxopt.evaluation.Evaluation] [description] Source code in blackboxopt/optimizers/staged/utils.py def best_evaluation_at_highest_fidelity ( evaluations : List [ Evaluation ], objective : Objective , ) -> Optional [ Evaluation ]: \"\"\"From given list of evaluations, get the best in terms of minimal loss at the highest fidelity. Args: evaluations: [description] objective: [description] Returns: [description] \"\"\" if not evaluations : return None successes = [ evaluation for evaluation in evaluations if evaluation . objectives [ objective . name ] is not None ] if not successes : return None successful_fidelities = [ evaluation . settings [ \"fidelity\" ] for evaluation in successes ] if not successful_fidelities : return None max_successful_fidelities = max ( successful_fidelities ) successful_max_fidelity_evaluations = [ evaluation for evaluation in successes if evaluation . settings [ \"fidelity\" ] == max_successful_fidelities ] if not successful_max_fidelity_evaluations : return None sort_function = max if objective . greater_is_better else min best_evaluation = sort_function ( successful_max_fidelity_evaluations , key = lambda e : e . objectives [ objective . name ] ) return best_evaluation greedy_promotion ( data , num_configs ) Promotes the best configurations to the next stage solely relying on the current loss. Parameters: Name Type Description Default data List[blackboxopt.optimizers.staged.iteration.Datum] List with all successful evaluations for this stage. All failed configurations have already been removed. required num_configs int Maximum number of configurations to be promoted. required Returns: Type Description list List of the config_keys to be evaluated on the next higher fidelity. These must only include config_keys found in data und must also be of at most length num_configs . If fewer ids are returned, the remaining configurations for the next stage will be sampled using the config_sample_function of the staged_iteration. Source code in blackboxopt/optimizers/staged/utils.py def greedy_promotion ( data : List [ Datum ], num_configs : int ) -> list : \"\"\"Promotes the best configurations to the next stage solely relying on the current loss. Args: data: List with all successful evaluations for this stage. All failed configurations have already been removed. num_configs: Maximum number of configurations to be promoted. Returns: List of the config_keys to be evaluated on the next higher fidelity. These must only include config_keys found in `data` und must also be of at most length `num_configs`. If fewer ids are returned, the remaining configurations for the next stage will be sampled using the `config_sample_function` of the staged_iteration. \"\"\" losses = [ d . loss for d in data ] ranks = np . argsort ( np . argsort ( losses )) n = min ( num_configs , len ( data )) return [ datum . config_key for rank , datum in zip ( ranks , data ) if rank < n ]","title":"Utils"},{"location":"reference/optimizers/staged/utils/#blackboxopt.optimizers.staged.utils","text":"","title":"utils"},{"location":"reference/optimizers/staged/utils/#blackboxopt.optimizers.staged.utils.best_evaluation_at_highest_fidelity","text":"From given list of evaluations, get the best in terms of minimal loss at the highest fidelity. Parameters: Name Type Description Default evaluations List[blackboxopt.evaluation.Evaluation] [description] required objective Objective [description] required Returns: Type Description Optional[blackboxopt.evaluation.Evaluation] [description] Source code in blackboxopt/optimizers/staged/utils.py def best_evaluation_at_highest_fidelity ( evaluations : List [ Evaluation ], objective : Objective , ) -> Optional [ Evaluation ]: \"\"\"From given list of evaluations, get the best in terms of minimal loss at the highest fidelity. Args: evaluations: [description] objective: [description] Returns: [description] \"\"\" if not evaluations : return None successes = [ evaluation for evaluation in evaluations if evaluation . objectives [ objective . name ] is not None ] if not successes : return None successful_fidelities = [ evaluation . settings [ \"fidelity\" ] for evaluation in successes ] if not successful_fidelities : return None max_successful_fidelities = max ( successful_fidelities ) successful_max_fidelity_evaluations = [ evaluation for evaluation in successes if evaluation . settings [ \"fidelity\" ] == max_successful_fidelities ] if not successful_max_fidelity_evaluations : return None sort_function = max if objective . greater_is_better else min best_evaluation = sort_function ( successful_max_fidelity_evaluations , key = lambda e : e . objectives [ objective . name ] ) return best_evaluation","title":"best_evaluation_at_highest_fidelity()"},{"location":"reference/optimizers/staged/utils/#blackboxopt.optimizers.staged.utils.greedy_promotion","text":"Promotes the best configurations to the next stage solely relying on the current loss. Parameters: Name Type Description Default data List[blackboxopt.optimizers.staged.iteration.Datum] List with all successful evaluations for this stage. All failed configurations have already been removed. required num_configs int Maximum number of configurations to be promoted. required Returns: Type Description list List of the config_keys to be evaluated on the next higher fidelity. These must only include config_keys found in data und must also be of at most length num_configs . If fewer ids are returned, the remaining configurations for the next stage will be sampled using the config_sample_function of the staged_iteration. Source code in blackboxopt/optimizers/staged/utils.py def greedy_promotion ( data : List [ Datum ], num_configs : int ) -> list : \"\"\"Promotes the best configurations to the next stage solely relying on the current loss. Args: data: List with all successful evaluations for this stage. All failed configurations have already been removed. num_configs: Maximum number of configurations to be promoted. Returns: List of the config_keys to be evaluated on the next higher fidelity. These must only include config_keys found in `data` und must also be of at most length `num_configs`. If fewer ids are returned, the remaining configurations for the next stage will be sampled using the `config_sample_function` of the staged_iteration. \"\"\" losses = [ d . loss for d in data ] ranks = np . argsort ( np . argsort ( losses )) n = min ( num_configs , len ( data )) return [ datum . config_key for rank , datum in zip ( ranks , data ) if rank < n ]","title":"greedy_promotion()"},{"location":"reference/visualizations/utils/","text":"blackboxopt.visualizations.utils get_incumbent_objective_over_time_single_fidelity ( objective , objective_values , times , fidelities , target_fidelity ) Filter for results with given target fidelity and generate incumbent trace. Source code in blackboxopt/visualizations/utils.py def get_incumbent_objective_over_time_single_fidelity ( objective : Objective , objective_values : np . ndarray , times : np . ndarray , fidelities : np . ndarray , target_fidelity : float , ): \"\"\"Filter for results with given target fidelity and generate incumbent trace.\"\"\" # filter out fidelity and take min/max of objective_values idx = np . logical_and ( fidelities == target_fidelity , np . isfinite ( objective_values )) _times = times [ idx ] if objective . greater_is_better : _objective_values = np . maximum . accumulate ( objective_values [ idx ]) else : _objective_values = np . minimum . accumulate ( objective_values [ idx ]) # get unique objective values and sort their indices (to be in chronological order) _ , idx = np . unique ( _objective_values , return_index = True ) idx . sort () # find objective_values _objective_values = _objective_values [ idx ] _times = _times [ idx ] # add steps where a new incumbent was found _times = np . repeat ( _times , 2 )[ 1 :] _objective_values = np . repeat ( _objective_values , 2 )[: - 1 ] # append best value for largest time to extend the lines _times = np . concatenate ([ _times , np . nanmax ( times , keepdims = True )]) _objective_values = np . concatenate ([ _objective_values , _objective_values [ - 1 :]]) return _times , _objective_values patch_plotly_io_to_html ( method ) Patch plotly.io.to_html with additional javascript to improve usability. Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets fixed. Injects <script> -tag with content from to_html_patch.js at the end of the HTML output. But only, if the chart title starts with \"[BBO]\" (to minimize side effects, if the user uses plotly.io for something else). plotly.io.to_html is also internally used for figure.show() and figure.to_html() , so this is covered, too. Parameters: Name Type Description Default method Callable Original plotly.io.to_html method. required Returns: Type Description Callable Patched method. Source code in blackboxopt/visualizations/utils.py def patch_plotly_io_to_html ( method : Callable ) -> Callable : \"\"\"Patch `plotly.io.to_html` with additional javascript to improve usability. Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets fixed. Injects `<script>`-tag with content from `to_html_patch.js` at the end of the HTML output. But only, if the chart title starts with \"[BBO]\" (to minimize side effects, if the user uses `plotly.io` for something else). `plotly.io.to_html` is also internally used for `figure.show()` and `figure.to_html()`, so this is covered, too. Args: method: Original `plotly.io.to_html` method. Returns: Patched method. \"\"\" @wraps ( method ) def wrapped ( * args , ** kwargs ): html = method ( * args , ** kwargs ) # Test if title text contains \"[BBO]\" if html . find ( '\"title\":{\"text\":\"[BBO]' ) < 0 : return html js = importlib . resources . read_text ( blackboxopt . visualizations , \"to_html_patch.js\" ) html_to_inject = f \"<script> { js } </script>\" insert_idx = html . rfind ( \"</body>\" ) if insert_idx >= 0 : # Full html page got rendered, inject <script> before <\\body> html = html [: insert_idx ] + html_to_inject + html [ insert_idx :] else : # Only chart part got rendered: append <script> at the end html = html + html_to_inject return html return wrapped","title":"Utils"},{"location":"reference/visualizations/utils/#blackboxopt.visualizations.utils","text":"","title":"utils"},{"location":"reference/visualizations/utils/#blackboxopt.visualizations.utils.get_incumbent_objective_over_time_single_fidelity","text":"Filter for results with given target fidelity and generate incumbent trace. Source code in blackboxopt/visualizations/utils.py def get_incumbent_objective_over_time_single_fidelity ( objective : Objective , objective_values : np . ndarray , times : np . ndarray , fidelities : np . ndarray , target_fidelity : float , ): \"\"\"Filter for results with given target fidelity and generate incumbent trace.\"\"\" # filter out fidelity and take min/max of objective_values idx = np . logical_and ( fidelities == target_fidelity , np . isfinite ( objective_values )) _times = times [ idx ] if objective . greater_is_better : _objective_values = np . maximum . accumulate ( objective_values [ idx ]) else : _objective_values = np . minimum . accumulate ( objective_values [ idx ]) # get unique objective values and sort their indices (to be in chronological order) _ , idx = np . unique ( _objective_values , return_index = True ) idx . sort () # find objective_values _objective_values = _objective_values [ idx ] _times = _times [ idx ] # add steps where a new incumbent was found _times = np . repeat ( _times , 2 )[ 1 :] _objective_values = np . repeat ( _objective_values , 2 )[: - 1 ] # append best value for largest time to extend the lines _times = np . concatenate ([ _times , np . nanmax ( times , keepdims = True )]) _objective_values = np . concatenate ([ _objective_values , _objective_values [ - 1 :]]) return _times , _objective_values","title":"get_incumbent_objective_over_time_single_fidelity()"},{"location":"reference/visualizations/utils/#blackboxopt.visualizations.utils.patch_plotly_io_to_html","text":"Patch plotly.io.to_html with additional javascript to improve usability. Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets fixed. Injects <script> -tag with content from to_html_patch.js at the end of the HTML output. But only, if the chart title starts with \"[BBO]\" (to minimize side effects, if the user uses plotly.io for something else). plotly.io.to_html is also internally used for figure.show() and figure.to_html() , so this is covered, too. Parameters: Name Type Description Default method Callable Original plotly.io.to_html method. required Returns: Type Description Callable Patched method. Source code in blackboxopt/visualizations/utils.py def patch_plotly_io_to_html ( method : Callable ) -> Callable : \"\"\"Patch `plotly.io.to_html` with additional javascript to improve usability. Might become obsolete, when https://github.com/plotly/plotly.js/issues/998 gets fixed. Injects `<script>`-tag with content from `to_html_patch.js` at the end of the HTML output. But only, if the chart title starts with \"[BBO]\" (to minimize side effects, if the user uses `plotly.io` for something else). `plotly.io.to_html` is also internally used for `figure.show()` and `figure.to_html()`, so this is covered, too. Args: method: Original `plotly.io.to_html` method. Returns: Patched method. \"\"\" @wraps ( method ) def wrapped ( * args , ** kwargs ): html = method ( * args , ** kwargs ) # Test if title text contains \"[BBO]\" if html . find ( '\"title\":{\"text\":\"[BBO]' ) < 0 : return html js = importlib . resources . read_text ( blackboxopt . visualizations , \"to_html_patch.js\" ) html_to_inject = f \"<script> { js } </script>\" insert_idx = html . rfind ( \"</body>\" ) if insert_idx >= 0 : # Full html page got rendered, inject <script> before <\\body> html = html [: insert_idx ] + html_to_inject + html [ insert_idx :] else : # Only chart part got rendered: append <script> at the end html = html + html_to_inject return html return wrapped","title":"patch_plotly_io_to_html()"},{"location":"reference/visualizations/visualizer/","text":"blackboxopt.visualizations.visualizer create_hover_information ( sections ) Create a hovertemplate which is used to render hover hints in plotly charts. The data for the chart hovertext has to be provided as custom_data attribute to the chart and can be e.g. a list of column names. One oddness is, that in the template the columns can't be referenced by name, but only by index. That's why it is important to have the same ordering in the template as in the custom_data and the reason why this is done together in one function. Parameters: Name Type Description Default sections dict Sections to render. The kyeys will show up as the section titles, values are expected to be a list of column names to be rendered under the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] } required Returns: Type Description Tuple[str, List] (plotly hover template, data column names) Source code in blackboxopt/visualizations/visualizer.py def create_hover_information ( sections : dict ) -> Tuple [ str , List ]: \"\"\" Create a [hovertemplate](https://plotly.com/python/reference/pie/#pie-hovertemplate) which is used to render hover hints in plotly charts. The data for the chart hovertext has to be provided as `custom_data` attribute to the chart and can be e.g. a list of column names. One oddness is, that in the template the columns can't be referenced by name, but only by index. That's why it is important to have the same ordering in the template as in the `custom_data` and the reason why this is done together in one function. Args: sections: Sections to render. The kyeys will show up as the section titles, values are expected to be a list of column names to be rendered under the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] } Returns: (plotly hover template, data column names) \"\"\" template = \"\" idx = 0 for section , columns in sections . items (): template += f \"<br><b> { section . replace ( '_' , ' ' ) . title () } </b><br>\" for column in columns : template += f \" { column } : % {{ customdata[ { idx } ] }} <br>\" idx += 1 template += \"<extra></extra>\" data_columns : list = sum ( sections . values (), []) return template , data_columns evaluations_to_df ( evaluations ) Convert evaluations into multi index dataframe. The evaluations will be casted to dictionaries which will be normalized. The keys of the dicts will be used as secondary column index. Evaluations with one or more missing objective-value will be dropped. Examples: Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...) Will be transformed into: | objectives | stacktrace | ... | <- \"group\" index | loss_1 | loss_2 | stacktrace | ... | <- \"field\" index | ------ | ------ | ---------- | --- | | 1.0 | -0.0 | None | ... | Source code in blackboxopt/visualizations/visualizer.py def evaluations_to_df ( evaluations : List [ Evaluation ]) -> pd . DataFrame : \"\"\"Convert evaluations into multi index dataframe. The evaluations will be casted to dictionaries which will be normalized. The keys of the dicts will be used as secondary column index. Evaluations with one or more missing objective-value will be dropped. Example: ``` Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...) ``` Will be transformed into: | objectives | stacktrace | ... | <- \"group\" index | loss_1 | loss_2 | stacktrace | ... | <- \"field\" index | ------ | ------ | ---------- | --- | | 1.0 | -0.0 | None | ... | \"\"\" if not evaluations or len ( evaluations ) == 0 : raise NoSuccessfulEvaluationsError # Filter out e.g. EvaluationSpecifications which might be passed into evaluations = [ e for e in evaluations if isinstance ( e , Evaluation )] # Transform to dicts, filter out evaluations with missing objectives evaluation_dicts = [ e . __dict__ for e in evaluations if not e . any_objective_none ] if len ( evaluation_dicts ) == 0 : raise NoSuccessfulEvaluationsError df = pd . DataFrame ( evaluation_dicts ) # Flatten json/dict columns into single multi-index dataframe dfs_expanded = [] for column in df . columns : # Normalize json columns keep original column for non-json columns try : df_temp = pd . json_normalize ( df [ column ], errors = \"ignore\" , max_level = 0 ) except AttributeError : df_temp = df [[ column ]] # Use keys of dicts as second level of column index df_temp . columns = pd . MultiIndex . from_product ( [[ column ], df_temp . columns ], names = [ \"group\" , \"field\" ] ) # Drop empty columns df_temp = df_temp . dropna ( axis = 1 , how = \"all\" ) dfs_expanded . append ( df_temp ) df = pd . concat ( dfs_expanded , join = \"outer\" , axis = 1 ) # Parse datetime columns date_columns = [ c for c in df . columns if \"unixtime\" in str ( c )] df [ date_columns ] = df [ date_columns ] . apply ( pd . to_datetime , unit = \"s\" ) # Calculate duration in seconds df [ \"duration\" , \"duration\" ] = ( df [ \"finished_unixtime\" , \"finished_unixtime\" ] - df [ \"created_unixtime\" , \"created_unixtime\" ] ) return df hypervolume_over_iterations ( evaluations_per_optimizer , objectives , reference_point , percentiles = None , hex_colors = None ) Visualize the hypervolume over iterations. In case multiple studies per optimizer are provided, a central tendency as well as variability is visualized. Parameters: Name Type Description Default evaluations_per_optimizer Dict[str, List[List[blackboxopt.evaluation.Evaluation]]] For each key i.e. optimizer, a list of studies which each contain a list of evaluations for the respective study corresponding to the number of iterations. required objectives Sequence[blackboxopt.base.Objective] The objectives to which the reported objective values correspond. required reference_point List[float] The hypervolume reference point. required percentiles Optional[Tuple[float, float, float]] When provided (e.g. (25, 50, 75) ) the median is used as the measure of central tendency, while the area between the 25 and 75 percentiles is shaded. In case no percentiles are given, the mean is used as the central tendency and an area indicating the standard error of the mean is shaded. None hex_colors Optional[List[str]] A list of hex color code strings. Defaults to plotly express' Dark24 None Returns: Type Description Plotly figure with hypervolume over iterations and a trace per optimizer. Source code in blackboxopt/visualizations/visualizer.py def hypervolume_over_iterations ( evaluations_per_optimizer : Dict [ str , List [ List [ Evaluation ]]], objectives : Sequence [ Objective ], reference_point : List [ float ], percentiles : Optional [ Tuple [ float , float , float ]] = None , hex_colors : Optional [ List [ str ]] = None , ): \"\"\"Visualize the hypervolume over iterations. In case multiple studies per optimizer are provided, a central tendency as well as variability is visualized. Args: evaluations_per_optimizer: For each key i.e. optimizer, a list of studies which each contain a list of evaluations for the respective study corresponding to the number of iterations. objectives: The objectives to which the reported objective values correspond. reference_point: The hypervolume reference point. percentiles: When provided (e.g. `(25, 50, 75)`) the median is used as the measure of central tendency, while the area between the 25 and 75 percentiles is shaded. In case no percentiles are given, the mean is used as the central tendency and an area indicating the standard error of the mean is shaded. hex_colors: A list of hex color code strings. Defaults to plotly express' Dark24 Returns: Plotly figure with hypervolume over iterations and a trace per optimizer. \"\"\" if hex_colors is None : hex_colors = px . colors . qualitative . Dark24 hex_color_iterator = iter ( hex_colors ) plotly_data = [] for optimizer , studies in evaluations_per_optimizer . items (): hv_per_study = [] for evaluations in studies : iteration_steps = len ( evaluations ) hvs = [ compute_hypervolume ( evaluations [: ( step + 1 )], objectives , reference_point ) for step in range ( iteration_steps ) ] hv_per_study . append ( hvs ) if percentiles is not None : lower = np . percentile ( hv_per_study , percentiles [ 0 ], axis = 0 ) central = np . percentile ( hv_per_study , percentiles [ 1 ], axis = 0 ) upper = np . percentile ( hv_per_study , percentiles [ 2 ], axis = 0 ) else : central = np . mean ( hv_per_study , axis = 0 ) sem = sps . sem ( hv_per_study , axis = 0 ) lower = central - sem upper = central + sem x_plotted = np . arange ( len ( central )) r , g , b = plotly . colors . hex_to_rgb ( next ( hex_color_iterator )) color_line = f \"rgb( { r } , { g } , { b } )\" color_fill = f \"rgba( { r } , { g } , { b } , 0.3)\" plotly_data . extend ( [ go . Scatter ( name = optimizer , x = x_plotted , y = central , mode = \"lines\" , legendgroup = optimizer , showlegend = True , line = dict ( color = color_line , simplify = True ), ), go . Scatter ( x = x_plotted , y = lower , mode = \"lines\" , marker = dict ( color = color_line ), line = dict ( width = 0 , simplify = True ), legendgroup = optimizer , showlegend = False , hoverinfo = \"skip\" , ), go . Scatter ( x = x_plotted , y = upper , mode = \"lines\" , marker = dict ( color = color_line ), line = dict ( width = 0 , simplify = True ), legendgroup = optimizer , showlegend = False , hoverinfo = \"skip\" , fillcolor = color_fill , fill = \"tonexty\" , ), ] ) fig = go . Figure ( plotly_data ) return fig parallel_coordinate_plot_parameters ( evaluations , columns = None , color_by = None ) Create an interactive parallel coordinate plot. Useful to investigate relationships in a higher dimensional search space and the optimization's objective(s). Parameters: Name Type Description Default evaluations List[blackboxopt.evaluation.Evaluation] Evaluations to plot. required columns Optional[List[str]] Names of columns to show. Can contain parameter names, objective names and settings keys. If None , all parameters, objectives and settings are displayed. None color_by Optional[str] Parameter name, objective name or settings key. The corresponding column will be shown at the very right, it's value will be used for the color scale. If None , all lines have the same color. None Returns: Type Description Plotly figure Raised In case evaluations does not contain at least one successful evaluation (an evaluation with objective value != None ). Source code in blackboxopt/visualizations/visualizer.py def parallel_coordinate_plot_parameters ( evaluations : List [ Evaluation ], columns : Optional [ List [ str ]] = None , color_by : Optional [ str ] = None , ): \"\"\"Create an interactive parallel coordinate plot. Useful to investigate relationships in a higher dimensional search space and the optimization's objective(s). Args: evaluations: Evaluations to plot. columns: Names of columns to show. Can contain parameter names, objective names and settings keys. If `None`, all parameters, objectives and settings are displayed. color_by: Parameter name, objective name or settings key. The corresponding column will be shown at the very right, it's value will be used for the color scale. If `None`, all lines have the same color. Returns: Plotly figure Raised: NoSuccessfulEvaluationsError: In case `evaluations` does not contain at least one successful evaluation (an evaluation with objective value != `None`). \"\"\" if not evaluations : raise NoSuccessfulEvaluationsError # Prepare dataframe for visualization df = evaluations_to_df ( evaluations ) # Drop unused columns and indices if \"settings\" in df . columns : df = df [[ \"configuration\" , \"settings\" , \"objectives\" ]] settings_cols = df [ \"settings\" ] . columns . to_list () else : df = df [[ \"configuration\" , \"objectives\" ]] settings_cols = [] objective_cols = df [ \"objectives\" ] . columns . to_list () df = df . droplevel ( 0 , axis = 1 ) # If no columns are specified, use all: if not columns : columns = df . columns . to_list () if color_by and color_by not in columns : raise ValueError ( f \"Unknown column name in color_by=' { color_by } '. Please make sure, that this\" + \"column name is correct and one of the visible columns.\" ) ambigious_columns = [ k for k , v in Counter ( df [ columns ] . columns ) . items () if v > 1 ] if ambigious_columns : raise ValueError ( \"All columns to plot must have a unique name, but those are ambigious: \" + f \" { ambigious_columns } . Either rename parameters/settings/objective to \" + \"be unique or provide only the unambigious ones as `columns` argument.\" ) # Prepare a coordinate (vertical line) for every column coordinates = [] colored_coordinate = {} for column in columns : coordinate : Dict [ str , Any ] = {} if column in objective_cols : coordinate [ \"label\" ] = f \"<b>Objective: { column } </b>\" elif column in settings_cols : coordinate [ \"label\" ] = f \"Setting: { column } \" else : coordinate [ \"label\" ] = column parameter_type = df [ column ] . dtype . name if parameter_type . startswith ( \"float\" ) or parameter_type . startswith ( \"int\" ): # Handling floats and integers the same, because unfortunately it's hard to # use integers only for ticks and still be robust regarding a large range # of values. coordinate [ \"values\" ] = df [ column ] elif parameter_type in [ \"object\" , \"bool\" ]: # Encode categorical values to integers. Unfortunately, ordinal parameters # loose there ordering, as there is no information about the order in the # evaluations. # The string conversion is necessary for unhashable entries, e.g. of # type List, which can't be casted to categories. df [ column ] = df [ column ] . astype ( str ) . astype ( \"category\" ) categories = df [ column ] . cat . categories . to_list () encoded_categories = list ( range ( len ( categories ))) df [ column ] . cat . categories = encoded_categories # Use integer encodings for scale and category values as tick labels coordinate [ \"ticktext\" ] = categories coordinate [ \"tickvals\" ] = encoded_categories coordinate [ \"values\" ] = df [ column ] . astype ( \"str\" ) else : warnings . warn ( f \"Ignoring column with unknown type: { column } < { parameter_type } >\" ) continue if column == color_by : colored_coordinate = coordinate else : coordinates . append ( coordinate ) # Append colored coordinate to the end (right) if colored_coordinate : coordinates . append ( colored_coordinate ) # Plot return go . Figure ( data = go . Parcoords ( line = dict ( # Color lines by objective value color = df [ color_by ] if color_by else None , colorscale = px . colors . diverging . Tealrose , showscale = True , # Use colorbar as kind of colored extension to the axis colorbar = dict ( thickness = 16 , x = 1 , xpad = 0 , ypad = 1 , tickmode = \"array\" , tickvals = [] ), ), dimensions = coordinates , ), layout = dict ( title = \"[BBO] Parallel coordinates plot\" ), )","title":"Visualizer"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer","text":"","title":"visualizer"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.create_hover_information","text":"Create a hovertemplate which is used to render hover hints in plotly charts. The data for the chart hovertext has to be provided as custom_data attribute to the chart and can be e.g. a list of column names. One oddness is, that in the template the columns can't be referenced by name, but only by index. That's why it is important to have the same ordering in the template as in the custom_data and the reason why this is done together in one function. Parameters: Name Type Description Default sections dict Sections to render. The kyeys will show up as the section titles, values are expected to be a list of column names to be rendered under the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] } required Returns: Type Description Tuple[str, List] (plotly hover template, data column names) Source code in blackboxopt/visualizations/visualizer.py def create_hover_information ( sections : dict ) -> Tuple [ str , List ]: \"\"\" Create a [hovertemplate](https://plotly.com/python/reference/pie/#pie-hovertemplate) which is used to render hover hints in plotly charts. The data for the chart hovertext has to be provided as `custom_data` attribute to the chart and can be e.g. a list of column names. One oddness is, that in the template the columns can't be referenced by name, but only by index. That's why it is important to have the same ordering in the template as in the `custom_data` and the reason why this is done together in one function. Args: sections: Sections to render. The kyeys will show up as the section titles, values are expected to be a list of column names to be rendered under the section. E.g.: { \"info\": [\"Objective #1\", \"Objective #2\", \"fidelity\"] } Returns: (plotly hover template, data column names) \"\"\" template = \"\" idx = 0 for section , columns in sections . items (): template += f \"<br><b> { section . replace ( '_' , ' ' ) . title () } </b><br>\" for column in columns : template += f \" { column } : % {{ customdata[ { idx } ] }} <br>\" idx += 1 template += \"<extra></extra>\" data_columns : list = sum ( sections . values (), []) return template , data_columns","title":"create_hover_information()"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.evaluations_to_df","text":"Convert evaluations into multi index dataframe. The evaluations will be casted to dictionaries which will be normalized. The keys of the dicts will be used as secondary column index. Evaluations with one or more missing objective-value will be dropped. Examples: Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...) Will be transformed into: | objectives | stacktrace | ... | <- \"group\" index | loss_1 | loss_2 | stacktrace | ... | <- \"field\" index | ------ | ------ | ---------- | --- | | 1.0 | -0.0 | None | ... | Source code in blackboxopt/visualizations/visualizer.py def evaluations_to_df ( evaluations : List [ Evaluation ]) -> pd . DataFrame : \"\"\"Convert evaluations into multi index dataframe. The evaluations will be casted to dictionaries which will be normalized. The keys of the dicts will be used as secondary column index. Evaluations with one or more missing objective-value will be dropped. Example: ``` Evaluation(objectives={'loss_1': 1.0, 'loss_2': -0.0}, stacktrace=None, ...) ``` Will be transformed into: | objectives | stacktrace | ... | <- \"group\" index | loss_1 | loss_2 | stacktrace | ... | <- \"field\" index | ------ | ------ | ---------- | --- | | 1.0 | -0.0 | None | ... | \"\"\" if not evaluations or len ( evaluations ) == 0 : raise NoSuccessfulEvaluationsError # Filter out e.g. EvaluationSpecifications which might be passed into evaluations = [ e for e in evaluations if isinstance ( e , Evaluation )] # Transform to dicts, filter out evaluations with missing objectives evaluation_dicts = [ e . __dict__ for e in evaluations if not e . any_objective_none ] if len ( evaluation_dicts ) == 0 : raise NoSuccessfulEvaluationsError df = pd . DataFrame ( evaluation_dicts ) # Flatten json/dict columns into single multi-index dataframe dfs_expanded = [] for column in df . columns : # Normalize json columns keep original column for non-json columns try : df_temp = pd . json_normalize ( df [ column ], errors = \"ignore\" , max_level = 0 ) except AttributeError : df_temp = df [[ column ]] # Use keys of dicts as second level of column index df_temp . columns = pd . MultiIndex . from_product ( [[ column ], df_temp . columns ], names = [ \"group\" , \"field\" ] ) # Drop empty columns df_temp = df_temp . dropna ( axis = 1 , how = \"all\" ) dfs_expanded . append ( df_temp ) df = pd . concat ( dfs_expanded , join = \"outer\" , axis = 1 ) # Parse datetime columns date_columns = [ c for c in df . columns if \"unixtime\" in str ( c )] df [ date_columns ] = df [ date_columns ] . apply ( pd . to_datetime , unit = \"s\" ) # Calculate duration in seconds df [ \"duration\" , \"duration\" ] = ( df [ \"finished_unixtime\" , \"finished_unixtime\" ] - df [ \"created_unixtime\" , \"created_unixtime\" ] ) return df","title":"evaluations_to_df()"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.hypervolume_over_iterations","text":"Visualize the hypervolume over iterations. In case multiple studies per optimizer are provided, a central tendency as well as variability is visualized. Parameters: Name Type Description Default evaluations_per_optimizer Dict[str, List[List[blackboxopt.evaluation.Evaluation]]] For each key i.e. optimizer, a list of studies which each contain a list of evaluations for the respective study corresponding to the number of iterations. required objectives Sequence[blackboxopt.base.Objective] The objectives to which the reported objective values correspond. required reference_point List[float] The hypervolume reference point. required percentiles Optional[Tuple[float, float, float]] When provided (e.g. (25, 50, 75) ) the median is used as the measure of central tendency, while the area between the 25 and 75 percentiles is shaded. In case no percentiles are given, the mean is used as the central tendency and an area indicating the standard error of the mean is shaded. None hex_colors Optional[List[str]] A list of hex color code strings. Defaults to plotly express' Dark24 None Returns: Type Description Plotly figure with hypervolume over iterations and a trace per optimizer. Source code in blackboxopt/visualizations/visualizer.py def hypervolume_over_iterations ( evaluations_per_optimizer : Dict [ str , List [ List [ Evaluation ]]], objectives : Sequence [ Objective ], reference_point : List [ float ], percentiles : Optional [ Tuple [ float , float , float ]] = None , hex_colors : Optional [ List [ str ]] = None , ): \"\"\"Visualize the hypervolume over iterations. In case multiple studies per optimizer are provided, a central tendency as well as variability is visualized. Args: evaluations_per_optimizer: For each key i.e. optimizer, a list of studies which each contain a list of evaluations for the respective study corresponding to the number of iterations. objectives: The objectives to which the reported objective values correspond. reference_point: The hypervolume reference point. percentiles: When provided (e.g. `(25, 50, 75)`) the median is used as the measure of central tendency, while the area between the 25 and 75 percentiles is shaded. In case no percentiles are given, the mean is used as the central tendency and an area indicating the standard error of the mean is shaded. hex_colors: A list of hex color code strings. Defaults to plotly express' Dark24 Returns: Plotly figure with hypervolume over iterations and a trace per optimizer. \"\"\" if hex_colors is None : hex_colors = px . colors . qualitative . Dark24 hex_color_iterator = iter ( hex_colors ) plotly_data = [] for optimizer , studies in evaluations_per_optimizer . items (): hv_per_study = [] for evaluations in studies : iteration_steps = len ( evaluations ) hvs = [ compute_hypervolume ( evaluations [: ( step + 1 )], objectives , reference_point ) for step in range ( iteration_steps ) ] hv_per_study . append ( hvs ) if percentiles is not None : lower = np . percentile ( hv_per_study , percentiles [ 0 ], axis = 0 ) central = np . percentile ( hv_per_study , percentiles [ 1 ], axis = 0 ) upper = np . percentile ( hv_per_study , percentiles [ 2 ], axis = 0 ) else : central = np . mean ( hv_per_study , axis = 0 ) sem = sps . sem ( hv_per_study , axis = 0 ) lower = central - sem upper = central + sem x_plotted = np . arange ( len ( central )) r , g , b = plotly . colors . hex_to_rgb ( next ( hex_color_iterator )) color_line = f \"rgb( { r } , { g } , { b } )\" color_fill = f \"rgba( { r } , { g } , { b } , 0.3)\" plotly_data . extend ( [ go . Scatter ( name = optimizer , x = x_plotted , y = central , mode = \"lines\" , legendgroup = optimizer , showlegend = True , line = dict ( color = color_line , simplify = True ), ), go . Scatter ( x = x_plotted , y = lower , mode = \"lines\" , marker = dict ( color = color_line ), line = dict ( width = 0 , simplify = True ), legendgroup = optimizer , showlegend = False , hoverinfo = \"skip\" , ), go . Scatter ( x = x_plotted , y = upper , mode = \"lines\" , marker = dict ( color = color_line ), line = dict ( width = 0 , simplify = True ), legendgroup = optimizer , showlegend = False , hoverinfo = \"skip\" , fillcolor = color_fill , fill = \"tonexty\" , ), ] ) fig = go . Figure ( plotly_data ) return fig","title":"hypervolume_over_iterations()"},{"location":"reference/visualizations/visualizer/#blackboxopt.visualizations.visualizer.parallel_coordinate_plot_parameters","text":"Create an interactive parallel coordinate plot. Useful to investigate relationships in a higher dimensional search space and the optimization's objective(s). Parameters: Name Type Description Default evaluations List[blackboxopt.evaluation.Evaluation] Evaluations to plot. required columns Optional[List[str]] Names of columns to show. Can contain parameter names, objective names and settings keys. If None , all parameters, objectives and settings are displayed. None color_by Optional[str] Parameter name, objective name or settings key. The corresponding column will be shown at the very right, it's value will be used for the color scale. If None , all lines have the same color. None Returns: Type Description Plotly figure Raised In case evaluations does not contain at least one successful evaluation (an evaluation with objective value != None ). Source code in blackboxopt/visualizations/visualizer.py def parallel_coordinate_plot_parameters ( evaluations : List [ Evaluation ], columns : Optional [ List [ str ]] = None , color_by : Optional [ str ] = None , ): \"\"\"Create an interactive parallel coordinate plot. Useful to investigate relationships in a higher dimensional search space and the optimization's objective(s). Args: evaluations: Evaluations to plot. columns: Names of columns to show. Can contain parameter names, objective names and settings keys. If `None`, all parameters, objectives and settings are displayed. color_by: Parameter name, objective name or settings key. The corresponding column will be shown at the very right, it's value will be used for the color scale. If `None`, all lines have the same color. Returns: Plotly figure Raised: NoSuccessfulEvaluationsError: In case `evaluations` does not contain at least one successful evaluation (an evaluation with objective value != `None`). \"\"\" if not evaluations : raise NoSuccessfulEvaluationsError # Prepare dataframe for visualization df = evaluations_to_df ( evaluations ) # Drop unused columns and indices if \"settings\" in df . columns : df = df [[ \"configuration\" , \"settings\" , \"objectives\" ]] settings_cols = df [ \"settings\" ] . columns . to_list () else : df = df [[ \"configuration\" , \"objectives\" ]] settings_cols = [] objective_cols = df [ \"objectives\" ] . columns . to_list () df = df . droplevel ( 0 , axis = 1 ) # If no columns are specified, use all: if not columns : columns = df . columns . to_list () if color_by and color_by not in columns : raise ValueError ( f \"Unknown column name in color_by=' { color_by } '. Please make sure, that this\" + \"column name is correct and one of the visible columns.\" ) ambigious_columns = [ k for k , v in Counter ( df [ columns ] . columns ) . items () if v > 1 ] if ambigious_columns : raise ValueError ( \"All columns to plot must have a unique name, but those are ambigious: \" + f \" { ambigious_columns } . Either rename parameters/settings/objective to \" + \"be unique or provide only the unambigious ones as `columns` argument.\" ) # Prepare a coordinate (vertical line) for every column coordinates = [] colored_coordinate = {} for column in columns : coordinate : Dict [ str , Any ] = {} if column in objective_cols : coordinate [ \"label\" ] = f \"<b>Objective: { column } </b>\" elif column in settings_cols : coordinate [ \"label\" ] = f \"Setting: { column } \" else : coordinate [ \"label\" ] = column parameter_type = df [ column ] . dtype . name if parameter_type . startswith ( \"float\" ) or parameter_type . startswith ( \"int\" ): # Handling floats and integers the same, because unfortunately it's hard to # use integers only for ticks and still be robust regarding a large range # of values. coordinate [ \"values\" ] = df [ column ] elif parameter_type in [ \"object\" , \"bool\" ]: # Encode categorical values to integers. Unfortunately, ordinal parameters # loose there ordering, as there is no information about the order in the # evaluations. # The string conversion is necessary for unhashable entries, e.g. of # type List, which can't be casted to categories. df [ column ] = df [ column ] . astype ( str ) . astype ( \"category\" ) categories = df [ column ] . cat . categories . to_list () encoded_categories = list ( range ( len ( categories ))) df [ column ] . cat . categories = encoded_categories # Use integer encodings for scale and category values as tick labels coordinate [ \"ticktext\" ] = categories coordinate [ \"tickvals\" ] = encoded_categories coordinate [ \"values\" ] = df [ column ] . astype ( \"str\" ) else : warnings . warn ( f \"Ignoring column with unknown type: { column } < { parameter_type } >\" ) continue if column == color_by : colored_coordinate = coordinate else : coordinates . append ( coordinate ) # Append colored coordinate to the end (right) if colored_coordinate : coordinates . append ( colored_coordinate ) # Plot return go . Figure ( data = go . Parcoords ( line = dict ( # Color lines by objective value color = df [ color_by ] if color_by else None , colorscale = px . colors . diverging . Tealrose , showscale = True , # Use colorbar as kind of colored extension to the axis colorbar = dict ( thickness = 16 , x = 1 , xpad = 0 , ypad = 1 , tickmode = \"array\" , tickvals = [] ), ), dimensions = coordinates , ), layout = dict ( title = \"[BBO] Parallel coordinates plot\" ), )","title":"parallel_coordinate_plot_parameters()"}]}